{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "filename = \"sound.wav\"\n",
    "data,sample_rate = librosa.load(filename)\n",
    "ipd.Audio(filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(func):\n",
    "    error=np.abs(data-func())\n",
    "    return np.mean(error)*1500\n",
    "\n",
    "def func(cf) :\n",
    "    time = np.linspace(0, 0.5, len(data))\n",
    "    sound = np.sum(cf[0]*np.sin(np.array( [i*time for i in cf[1:]] )), axis=0)\n",
    "    \n",
    "    return sound\n",
    "\n",
    "def test_1():\n",
    "    return func(init_cf)\n",
    "\n",
    "def test_2():\n",
    "    return func(gen_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_list():\n",
    "    gen_list = [max(data)/4, 0,0,0,0]\n",
    "    for i in range (1, len(gen_list)):\n",
    "        gen_list[i] = random.randint(1, 10000)\n",
    "    \n",
    "    return gen_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_cf = [max(data)/4, 800, 1000, 1200, 1400]\n",
    "minimum_cf = [max(data)/4, 800, 1000, 1200, 1400]\n",
    "minimum_loss = 10000\n",
    "\n",
    "while minimum_loss > 5:\n",
    "\n",
    "    for _ in range(10000):\n",
    "        init_cf[4] = init_cf[4] + 1\n",
    "        if minimum_loss > mse(test_1):\n",
    "            print(\"change\")\n",
    "            print(\"before : \", minimum_loss)\n",
    "            print(\"after : \", mse(test_1))\n",
    "            print(\"{} -> {}\".format(minimum_cf[1:], init_cf[1:]))\n",
    "            print(\"\")\n",
    "            minimum_loss = mse(test_1)\n",
    "            minimum_cf = init_cf\n",
    "\n",
    "        for _ in range (10000):\n",
    "            init_cf[3] = init_cf[3] + 1\n",
    "            if minimum_loss > mse(test_1):\n",
    "                print(\"change\")\n",
    "                print(\"before : \", minimum_loss)\n",
    "                print(\"after : \", mse(test_1))\n",
    "                print(\"{} -> {}\".format(minimum_cf[1:], init_cf[1:]))\n",
    "                print(\"\")                        \n",
    "                minimum_loss = mse(test_1)\n",
    "                minimum_cf = init_cf\n",
    "            \n",
    "            for _ in range (10000):\n",
    "                    init_cf[2] = init_cf[2] + 1\n",
    "                    if minimum_loss > mse(test_1):\n",
    "                        print(\"change\")\n",
    "                        print(\"before : \", minimum_loss)\n",
    "                        print(\"after : \", mse(test_1))\n",
    "                        print(\"{} -> {}\".format(minimum_cf[1:], init_cf[1:]))\n",
    "                        print(\"\")                        \n",
    "                        minimum_loss = mse(test_1)\n",
    "                        minimum_cf = init_cf\n",
    "                    \n",
    "                    for _ in range (10000):\n",
    "                        init_cf[1] = init_cf[1] + 1\n",
    "                        if minimum_loss > mse(test_1):\n",
    "                            print(\"change\")\n",
    "                            print(\"before : \", minimum_loss)\n",
    "                            print(\"after : \", mse(test_1))\n",
    "                            print(\"{} -> {}\".format(minimum_cf[1:], init_cf[1:]))\n",
    "                            print(\"\")                        \n",
    "                            minimum_loss = mse(test_1)\n",
    "                            minimum_cf = init_cf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "check_list = list()\n",
    "minimum_loss_cf = [0,0,0,0,0]\n",
    "minimum_loss = 10000\n",
    "\n",
    "while minimum_loss > 5:\n",
    "    gen_list = generate_list()\n",
    "\n",
    "    while gen_list in check_list:\n",
    "        gen_list = generate_list()\n",
    "        print('regenerate')\n",
    "    \n",
    "    check_list.append(gen_list)\n",
    "    \n",
    "    if minimum_loss > mse(test_2):\n",
    "        print(\"Old Frequency: \", minimum_loss_cf[1:])\n",
    "        print(\"Old Loss: \", minimum_loss)\n",
    "        minimum_loss = mse(test_2)\n",
    "        minimum_loss_cf = gen_list \n",
    "        print(\"Update Frequency: \", minimum_loss_cf[1:])\n",
    "        print(\"Update Loss: \",minimum_loss) \n",
    "        print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# install\n",
    "## numpy\n",
    "## matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(train_raw_img, train_label), (test_raw_img, test_label) = load_mnist(flatten=False, normalize=False)\n",
    "print(train_raw_img.shape)\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_raw_img[i][0], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_label, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset split according to the number\n",
    "\n",
    "new_train_img = [[] for _ in range(10)]\n",
    "new_train_label = [[] for _ in range(10)]\n",
    "\n",
    "for i in range(len(train_label)) :\n",
    "    new_train_img[train_label[i]].append(train_raw_img[i])\n",
    "    new_train_label[train_label[i]].append(train_label[i])\n",
    "\n",
    "# print(len(new_train_img[0])) # 0에 해당하는 image 개수\n",
    "# print(new_train_img[0][0].shape) # 0에 해당하는 image중 첫번째 image의 shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample(idx) :\n",
    "    sample_img = []\n",
    "    sample_label = []\n",
    "    \n",
    "    # data sampling \n",
    "    for i in range(10) :\n",
    "        if i == idx :\n",
    "            sample_img += new_train_img[i][:1000]\n",
    "            sample_label += (new_train_label[i][:1000])\n",
    "        else :\n",
    "            sample_img += new_train_img[i][:111]\n",
    "            sample_label += (new_train_label[i][:111])\n",
    "\n",
    "    sample_img = np.array(sample_img)\n",
    "    sample_label = np.array(sample_label)\n",
    "    \n",
    "    # normalization (set value 0 ~ 1)\n",
    "    sample_img = sample_img.astype('float')/255\n",
    "    \n",
    "    # target number는 1, 아니면 0\n",
    "    sample_label = np.where(sample_label==idx, 1 ,0)\n",
    "    \n",
    "    # reshape\n",
    "    sample_img = sample_img.reshape(len(sample_img.squeeze()), -1)\n",
    "    sample_label = sample_label.reshape(len(sample_label.squeeze()), -1)\n",
    "    \n",
    "    return sample_img, sample_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = make_sample(idx = 0) # idx = target number\n",
    "train_X = np.insert(train_X, 0, 1, axis=1) # bias 추가\n",
    "\n",
    "# print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy loss\n",
    "def CrossEntropyLoss(w, X, y) :\n",
    "    delta = 1e-7\n",
    "    \n",
    "    preds = 1 / (1+np.exp(-X.dot(w)))\n",
    "    loss = np.sum(-y*np.log(preds+delta)-(1-y)*np.log(1-preds+delta))/len(preds)\n",
    "        \n",
    "    return loss , preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval (accuracy)\n",
    "\n",
    "def eval(idx, w) :\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        idx : target_number\n",
    "        w : parameter\n",
    "    \"\"\"\n",
    "    test_X = test_raw_img.astype('float')/255    \n",
    "    test_X = test_X.reshape(len(test_X.squeeze()), -1)\n",
    "    test_X = np.insert(test_X, 0, 1, axis=1) # + bias\n",
    "\n",
    "    test_y = np.where(test_label==idx, 1 ,0)\n",
    "    test_y = test_y.reshape(len(test_y.squeeze()), -1)\n",
    "    \n",
    "    preds = 1/(1+np.exp(-test_X.dot(w)))\n",
    "    result = np.where(preds>0.5, 1, 0)\n",
    "    \n",
    "    acc = np.sum(np.where(result==test_y, True, False))/len(preds)\n",
    "    \n",
    "    return acc\n",
    "    # print('accuracy : ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "def train(X, y, index) :\n",
    "\n",
    "    w = np.random.randn(len(X[0]), 1)\n",
    "    lr = 0.1 # learning rate(수정 가능)\n",
    "    step = 0\n",
    "    acc = 0\n",
    "    prev_loss = float('inf')\n",
    "    J_history = list()\n",
    "    ACC_history = list()\n",
    "    \n",
    "    while (acc <= 0.85) :\n",
    "        step += 1\n",
    "        correct = 0\n",
    "        \n",
    "        acc = eval(index, w)\n",
    "        \n",
    "        loss, preds = CrossEntropyLoss(w, X, y)\n",
    "        \n",
    "        diff = preds - y\n",
    "        gradient = X.T.dot(diff) / X.shape[0]\n",
    "        w -= lr * gradient\n",
    "        \n",
    "        if abs(loss - prev_loss) < 1e-4:\n",
    "            break\n",
    "        \n",
    "        prev_loss = loss\n",
    "        J_history.append(loss)\n",
    "        ACC_history.append(acc)\n",
    "        print(\"total step : %d \" % step)\n",
    "        print(\"error : %f, accuarcy : %f\" % (loss, acc))\n",
    "        \n",
    "    return w, J_history, ACC_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weight\n",
    "w,J_history, ACC_history = train(train_X, train_y, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cost function vs. iterations to check for convergence\n",
    "plt.plot(J_history, label='Cost Function')\n",
    "plt.plot(ACC_history, label='Accuracy')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost Function')\n",
    "plt.title('Gradient Descent Convergence')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi class single label classification (using logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_0X, train_0y = make_sample(idx = 0) # idx = target number\n",
    "train_0X = np.insert(train_0X, 0, 1, axis=1) # bias 추가\n",
    "w0,J0_history, ACC0_history = train(train_X, train_y,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1X, train_1y = make_sample(idx = 1) # idx = target number\n",
    "train_1X = np.insert(train_1X, 0, 1, axis=1) # bias 추가\n",
    "w1,J1_history, ACC1_history = train(train_1X, train_1y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2X, train_2y = make_sample(idx = 2) # idx = target number\n",
    "train_2X = np.insert(train_2X, 0, 1, axis=1) # bias 추가\n",
    "w2,J2_history, ACC2_history  = train(train_2X, train_2y,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_3X, train_3y = make_sample(idx = 3) # idx = target number\n",
    "train_3X = np.insert(train_3X, 0, 1, axis=1) # bias 추가\n",
    "w3,J3_history, ACC3_history = train(train_3X, train_3y,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_4X, train_4y = make_sample(idx = 4) # idx = target number\n",
    "train_4X = np.insert(train_4X, 0, 1, axis=1) # bias 추가\n",
    "w4,J4_history, ACC4_history = train(train_4X, train_4y,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_5X, train_5y = make_sample(idx = 5) # idx = target number\n",
    "train_5X = np.insert(train_5X, 0, 1, axis=1) # bias 추가\n",
    "w5,J5_history, ACC5_history = train(train_5X, train_5y,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_6X, train_6y = make_sample(idx = 6) # idx = target number\n",
    "train_6X = np.insert(train_6X, 0, 1, axis=1) # bias 추가\n",
    "w6,J6_history, ACC6_history = train(train_6X, train_6y,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_7X, train_7y = make_sample(idx = 7) # idx = target number\n",
    "train_7X = np.insert(train_7X, 0, 1, axis=1) # bias 추가\n",
    "w7,J7_history, ACC7_history = train(train_7X, train_7y,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_8X, train_8y = make_sample(idx = 8) # idx = target number\n",
    "train_8X = np.insert(train_8X, 0, 1, axis=1) # bias 추가\n",
    "w8,J8_history, ACC8_history = train(train_8X, train_8y,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_9X, train_9y = make_sample(idx = 9) # idx = target number\n",
    "train_9X = np.insert(train_9X, 0, 1, axis=1) # bias 추가\n",
    "w9,J9_history, ACC9_history = train(train_9X, train_9y,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_cls_classifier(X,w0,w1,w2,w3,w4,w5,w6,w7,w8,w9):\n",
    "    \n",
    "    res_0 = 1 / (1+np.exp(-X.dot(w0)))\n",
    "    res_1 = 1 / (1+np.exp(-X.dot(w1)))\n",
    "    res_2 = 1 / (1+np.exp(-X.dot(w2)))\n",
    "    res_3 = 1 / (1+np.exp(-X.dot(w3)))\n",
    "    res_4 = 1 / (1+np.exp(-X.dot(w4)))\n",
    "    res_5 = 1 / (1+np.exp(-X.dot(w5)))\n",
    "    res_6 = 1 / (1+np.exp(-X.dot(w6)))\n",
    "    res_7 = 1 / (1+np.exp(-X.dot(w7)))\n",
    "    res_8 = 1 / (1+np.exp(-X.dot(w8)))\n",
    "    res_9 = 1 / (1+np.exp(-X.dot(w9)))\n",
    "    \n",
    "    res = np.concatenate((res_0,\n",
    "                        res_1,\n",
    "                        res_2,\n",
    "                        res_3,\n",
    "                        res_4,\n",
    "                        res_5,\n",
    "                        res_6,\n",
    "                        res_7,\n",
    "                        res_8,\n",
    "                        res_9), axis = 1)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "import pandas as pd\n",
    "\n",
    "test_X = test_raw_img.astype('float')/255    \n",
    "test_X = test_X.reshape(len(test_X.squeeze()), -1)\n",
    "test_X = np.insert(test_X, 0, 1, axis=1) # + bias\n",
    "print('shape of test_X : ', test_X.shape)\n",
    "print('shape of test_label : ', test_label.shape)\n",
    "\n",
    "probs = mult_cls_classifier(test_X,w0,w1,w2,w3,w4,w5,w6,w7,w8,w9)\n",
    "# make prediction using argmax\n",
    "max_pred = np.argmax(probs, axis = 1)\n",
    "\n",
    "acc = np.sum(np.where(test_label==max_pred, True, False))/len(test_X)\n",
    "print('accuracy : ', acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Regularization with Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy loss\n",
    "def CrossEntropyLoss(w, X, y, alpha) :\n",
    "    delta = 1e-7\n",
    "    \n",
    "    preds = 1 / (1+np.exp(-X.dot(w)))\n",
    "    reg_term = 0.5 * alpha * np.sum(w**2)\n",
    "    loss = np.sum(-y*np.log(preds+delta)-(1-y)*np.log(1-preds+delta))/len(preds) + reg_term\n",
    "        \n",
    "    return loss , preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "def train(X, y, index,alpha) :\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        X : train_X\n",
    "        y : train_y\n",
    "\n",
    "    Returns:\n",
    "        w : weight\n",
    "    \"\"\"\n",
    "    w = np.random.randn(len(X[0]), 1)\n",
    "    lr = 0.1 # learning rate(수정 가능)\n",
    "    step = 0\n",
    "    acc = 0\n",
    "    prev_loss = float('inf')    \n",
    "    weight_capacity = list()\n",
    "    \n",
    "    while (acc <= 0.85) :\n",
    "        step += 1\n",
    "        correct = 0\n",
    "        \n",
    "        acc = eval(index, w)\n",
    "        \n",
    "        loss, preds = CrossEntropyLoss(w, X, y, alpha)\n",
    "        \n",
    "        diff = preds - y\n",
    "        gradient = X.T.dot(diff) / X.shape[0]\n",
    "        weight_capacity.append(np.linalg.norm(w[0][0]))\n",
    "        w -= lr * gradient\n",
    "        \n",
    "        if abs(loss - prev_loss) < 1e-4:\n",
    "            break\n",
    "        \n",
    "        prev_loss = loss\n",
    "        \n",
    "        # print(\"total step : %d \" % step)\n",
    "        # print(\"error : %f, accuarcy : %f\" % (loss, acc))\n",
    "        \n",
    "    return w, weight_capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "weights = np.zeros((5, len(lambdas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l in enumerate(lambdas):\n",
    "    w,_ = train(train_7X, train_7y, 7, l)\n",
    "    weights[:, i] = np.sum(np.array(w[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "for i in range(5):\n",
    "    ax.plot(lambdas, weights[i], label=f'Feature {i+1}')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('lambda')\n",
    "ax.set_ylabel('weight')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# install\n",
    "## numpy\n",
    "## matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data load & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(train_raw_img, train_label), (test_raw_img, test_label) = load_mnist(flatten=False, normalize=False)\n",
    "print(train_raw_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing (train & inference)\n",
    "\n",
    "train_img = train_raw_img.reshape(len(train_raw_img.squeeze()), -1)\n",
    "train_label = train_label.reshape(len(train_label), -1)\n",
    "\n",
    "test_img = test_raw_img.reshape(len(test_raw_img.squeeze()), -1)\n",
    "test_label = test_label.reshape(len(test_label), -1)\n",
    "\n",
    "print(train_img.shape)\n",
    "print(train_label.shape)\n",
    "print(test_img.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization (set value 0 ~ 1)\n",
    "\n",
    "train_img = train_img.astype('float')\n",
    "train_img = train_img/255\n",
    "\n",
    "test_img = test_img.astype('float')\n",
    "test_img = test_img/255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear :\n",
    "    def __init__(self, input_size=1, hidden_size=1) :\n",
    "        self.W = np.random.randn(input_size, hidden_size)\n",
    "        self.b = np.zeros(hidden_size)\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db =  None\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout, lr) :\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis = 0)\n",
    "        self.W -= lr*self.dW\n",
    "        self.b -= lr*self.db\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu :\n",
    "    def __init__(self) :\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        self.mask = (x < 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout) :\n",
    "        dout[self.mask] = 0\n",
    "        return dout                                                                                                                                                                                                                                                                                                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax_with_crossEntropy :\n",
    "    def __init__(self) :\n",
    "        self.delta = 1e-7\n",
    "        # softmax\n",
    "        self.softmax_x = None\n",
    "        self.softmax_out = None\n",
    "        # crossEntropy\n",
    "        self.pred = None\n",
    "        self.target = None\n",
    "        \n",
    "    def softmax_forward(self, x) :\n",
    "        self.softmax_x = x\n",
    "        # subtracting the maximum value for numerical stability\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        self.softmax_out = exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "        return self.softmax_out\n",
    "\n",
    "    def crossEntropy_forward(self, pred, target) :\n",
    "        self.pred = pred\n",
    "        self.target = target\n",
    "        # avoid log(0) by adding delta\n",
    "        loss = -np.sum(target * np.log(pred + self.delta))\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def backward(self):\n",
    "        batch_size = self.pred.shape[0]\n",
    "        dx = (self.pred - self.target) / batch_size\n",
    "        return dx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot label 만드는 함수\n",
    "\n",
    "def make_one_hot(labels) :\n",
    "    a = []\n",
    "    for label in labels :\n",
    "        one_hot = np.zeros(10)\n",
    "        one_hot[label] = 1\n",
    "        a.append(one_hot)\n",
    "    a = np.array(a)\n",
    "    return a\n",
    "\n",
    "# one_hot_labels = make_one_hot(train_label)\n",
    "# print(train_label[0])\n",
    "# print(one_hot_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train version 1\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def train_MLP(config) :\n",
    "    lr, num_epoch = config['learning_rate'], config['num_epoch']\n",
    "    print_loss_interval = 1\n",
    "    \n",
    "    layer1 = Linear(784, 100)\n",
    "    relu = Relu()\n",
    "    layer2 = Linear(100, 10)\n",
    "    softmax_with_CE = softmax_with_crossEntropy()\n",
    "    \n",
    "    iter_lst = list()\n",
    "    loss_lst = list()\n",
    "    Non_weight_lsit = list()\n",
    "    \n",
    "    for iter in range(num_epoch) :\n",
    "        # forward\n",
    "        x = layer1.forward(train_img)\n",
    "        x = relu.forward(x)\n",
    "        x = layer2.forward(x)\n",
    "        preds = softmax_with_CE.softmax_forward(x)\n",
    "        \n",
    "        # loss\n",
    "        one_hot_labels = make_one_hot(train_label)\n",
    "        losses = softmax_with_CE.crossEntropy_forward(preds, one_hot_labels)\n",
    "        loss = losses.sum()/len(preds)\n",
    "        \n",
    "        if iter % print_loss_interval == 0:\n",
    "            print(\"[epoch %d / %d] average loss : %f\" % (iter, num_epoch, loss))\n",
    "            \n",
    "        iter_lst.append(iter)\n",
    "        loss_lst.append(loss)\n",
    "        \n",
    "        # backward\n",
    "        dL = softmax_with_CE.backward()\n",
    "        Non_weight_lsit.append(np.sum(dL))\n",
    "        dL = layer2.backward(dL, lr)\n",
    "        dL = relu.backward(dL)\n",
    "        dL = layer1.backward(dL, lr)\n",
    "                     \n",
    "    model = OrderedDict()\n",
    "    model['layer1'] = layer1\n",
    "    model['relu'] = relu\n",
    "    model['layer2'] = layer2\n",
    "    model['softmax_with_CE'] = softmax_with_CE\n",
    "    \n",
    "    return model,iter_lst, loss_lst,Non_weight_lsit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100\n",
    "          }\n",
    "\n",
    "model,iter_lst, loss_lst,Non_weight_lsit = train_MLP(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(iter_lst, loss_lst)\n",
    "plt.title(\"Loss vs. Iteration\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, train_version = True) :\n",
    "    if train_version :\n",
    "        x = train_img\n",
    "        labels = train_label.squeeze()\n",
    "        print('In train dataset ... ')\n",
    "    else : \n",
    "        x = test_img\n",
    "        labels = test_label.squeeze()\n",
    "        print('\\nIn test dataset ... ')\n",
    "    \n",
    "    for layer in model.values() :\n",
    "        if isinstance(layer, softmax_with_crossEntropy) :\n",
    "            x = layer.softmax_forward(x)\n",
    "        else :\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "    preds = x.argmax(axis=1)\n",
    "    acc = np.sum(np.where(preds==labels, True, False))/len(labels)\n",
    "    return acc\n",
    "\n",
    "print('\\t Accuracy :', eval(model, train_version=True))\n",
    "print('\\t Accuracy :', eval(model, train_version=False))\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size=1, hidden_size=1):\n",
    "        self.W = np.random.randn(input_size, hidden_size)\n",
    "        self.b = np.zeros(hidden_size)\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x # 입력값 저장\n",
    "        out = np.dot(x, self.W) + self.b # 선형 연산 수행\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout, lr, lambda_reg=0.01):\n",
    "        dx = np.dot(dout, self.W.T) # 입력값에 대한 미분값 계산\n",
    "        self.dW = np.dot(self.x.T, dout) + lambda_reg * self.W # 가중치 행렬에 대한 미분값 계산\n",
    "        self.db = np.sum(dout, axis=0) # bias에 대한 미분값 계산\n",
    "        self.W -= lr * self.dW # 가중치 행렬 업데이트\n",
    "        self.b -= lr * self.db  # bias 업데이트\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Classifier:\n",
    "    def __init__(self):\n",
    "        self.delta = 1e-7\n",
    "        # softmax\n",
    "        self.softmax_x = None\n",
    "        self.softmax_out = None\n",
    "        # crossEntropy\n",
    "        self.pred = None\n",
    "        self.target = None\n",
    "        self.alpha = 0.001\n",
    "\n",
    "    def softmax_forward(self, x):\n",
    "        \"\"\"\n",
    "        Softmax forward propagation\n",
    "        \n",
    "        Parameters:\n",
    "        - x (numpy array): Input data\n",
    "        \n",
    "        Returns:\n",
    "        - softmax_out (numpy array): Output of softmax activation function\n",
    "        \"\"\"\n",
    "        self.softmax_x = x\n",
    "        # subtracting the maximum value for numerical stability\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        self.softmax_out = exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "        return self.softmax_out\n",
    "\n",
    "    def crossEntropy_forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        Cross-entropy forward propagation\n",
    "        \n",
    "        Parameters:\n",
    "        - pred (numpy array): Predicted output\n",
    "        - target (numpy array): Target output\n",
    "        \n",
    "        Returns:\n",
    "        - loss (float): Cross-entropy loss\n",
    "        \"\"\"\n",
    "        self.pred = pred\n",
    "        self.target = target\n",
    "        # avoid log(0) by adding delta\n",
    "        reg_term = 0.5 * self.alpha * np.sum(pred ** 2)\n",
    "        loss = -np.sum(target * np.log(pred + self.delta)) / len(pred) + reg_term\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Backward propagation for cross-entropy and softmax\n",
    "        \n",
    "        Returns:\n",
    "        - dx (numpy array): Gradient of loss with respect to the input\n",
    "        \"\"\"\n",
    "        batch_size = self.pred.shape[0]\n",
    "        dx = (self.pred - self.target) / batch_size\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train version 1\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def train_MLP(config) :\n",
    "    lr, num_epoch,model = config['learning_rate'], config['num_epoch'], config['model']\n",
    "    print_loss_interval = 1\n",
    "    \n",
    "    layer1 = Linear(784, 100)\n",
    "    relu = Relu()\n",
    "    layer2 = Linear(100, 10)\n",
    "    softmax_with_CE = model\n",
    "    \n",
    "    loss_lst = list()\n",
    "    iter_lst = list()\n",
    "    weight_list = list()\n",
    "    \n",
    "    for iter in range(num_epoch) :\n",
    "        # forward\n",
    "        x = layer1.forward(train_img)\n",
    "        x = relu.forward(x)\n",
    "        x = layer2.forward(x)\n",
    "        preds = softmax_with_CE.softmax_forward(x)\n",
    "        \n",
    "        # loss\n",
    "        one_hot_labels = make_one_hot(train_label)\n",
    "        losses = softmax_with_CE.crossEntropy_forward(preds, one_hot_labels)\n",
    "        loss = losses.sum()/len(preds)\n",
    "        \n",
    "        if iter % print_loss_interval == 0:\n",
    "            print(\"[epoch %d / %d] average loss : %f\" % (iter, num_epoch, loss))\n",
    "        \n",
    "        loss_lst.append(loss)\n",
    "        iter_lst.append(iter)\n",
    "        \n",
    "        # backward\n",
    "        dL = softmax_with_CE.backward()\n",
    "        weight_list.append(np.sum(dL))\n",
    "        dL = layer2.backward(dL, lr)\n",
    "        dL = relu.backward(dL)\n",
    "        dL = layer1.backward(dL,lr)\n",
    "\n",
    "        \n",
    "    model = OrderedDict()\n",
    "    model['layer1'] = layer1\n",
    "    model['relu'] = relu\n",
    "    model['layer2'] = layer2\n",
    "    model['softmax_with_CE'] = softmax_with_CE\n",
    "    \n",
    "    return model, loss_lst, iter_lst, weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { 'model' : MLP_Classifier(),\n",
    "            'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100\n",
    "          }\n",
    "\n",
    "model, loss_lst, iter_lst,weight_list = train_MLP(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(iter_lst, loss_lst)\n",
    "plt.title(\"Loss vs. Iteration\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Non_weight_lsit)\n",
    "plt.title(\"Not applied in Regluarization\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(weight_list)\n",
    "plt.title(\"L2 Regularization\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, train_version = True) :\n",
    "    if train_version :\n",
    "        x = train_img\n",
    "        labels = train_label.squeeze()\n",
    "        print('In train dataset ... ')\n",
    "    else : \n",
    "        x = test_img\n",
    "        labels = test_label.squeeze()\n",
    "        print('\\nIn test dataset ... ')\n",
    "    \n",
    "    for layer in model.values() :\n",
    "        if isinstance(layer, MLP_Classifier) :\n",
    "            x = layer.softmax_forward(x)\n",
    "        else :\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "    preds = x.argmax(axis=1)\n",
    "    acc = np.sum(np.where(preds==labels, True, False))/len(labels)\n",
    "    return acc\n",
    "\n",
    "print('\\t Accuracy :', eval(model, train_version=True))\n",
    "print('\\t Accuracy :', eval(model, train_version=False))\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# install\n",
    "## numpy\n",
    "## matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data load & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(train_raw_img, train_label), (test_raw_img, test_label) = load_mnist(flatten=False, normalize=False)\n",
    "print(train_raw_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing (train & inference)\n",
    "\n",
    "train_img = train_raw_img.reshape(len(train_raw_img.squeeze()), -1)\n",
    "train_label = train_label.reshape(len(train_label), -1)\n",
    "\n",
    "test_img = test_raw_img.reshape(len(test_raw_img.squeeze()), -1)\n",
    "test_label = test_label.reshape(len(test_label), -1)\n",
    "\n",
    "print(train_img.shape)\n",
    "print(train_label.shape)\n",
    "print(test_img.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization (set value 0 ~ 1)\n",
    "\n",
    "train_img = train_img.astype('float')\n",
    "train_img = train_img/255\n",
    "\n",
    "test_img = test_img.astype('float')\n",
    "test_img = test_img/255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Linear:\n",
    "    def __init__(self, input_size=1, hidden_size=1):\n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.randn(input_size, hidden_size)\n",
    "        self.b = np.zeros(hidden_size)\n",
    "        \n",
    "        # Store input size and hidden size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Calculate linear transformation\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout, lr = 0.01, lamb=0):\n",
    "        # Calculate gradients for weights and biases\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.W -= lr * (dW + lamb*self.W)\n",
    "        self.b -= lr * db\n",
    "        \n",
    "        # Calculate gradients for input\n",
    "        # print(dout.shape, self.W.T.shape)\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        \n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation functions (ReLU, LeakyReLU, Sigmoid, Tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dx = dout\n",
    "        dx[self.mask] = 0\n",
    "        return dx\n",
    "    \n",
    "\n",
    "class LeakyReLU:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] *= self.alpha\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] *= self.alpha\n",
    "        dx = dout\n",
    "        return dx\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        return dx\n",
    "\n",
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = np.tanh(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out ** 2)\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_with_CrossEntropy :\n",
    "    def __init__(self) :\n",
    "        # softmax\n",
    "        self.softmax_x = None \n",
    "        self.softmax_out = None\n",
    "        # crossEntropy\n",
    "        self.pred = None\n",
    "        self.target = None\n",
    "        self.delta = 1e-7\n",
    "        \n",
    "    def softmax_forward(self, x) :\n",
    "        self.softmax_x = x\n",
    "        tmp = np.max(self.softmax_x, axis=1).reshape(-1, 1)\n",
    "        self.softmax_out = np.exp(self.softmax_x-tmp)/np.sum(np.exp(self.softmax_x-tmp), axis=1).reshape(-1,1)\n",
    "        return self.softmax_out\n",
    "    \n",
    "    def crossEntropy_forward(self, pred, target) :\n",
    "        self.pred = pred\n",
    "        self.target = target\n",
    "        loss = -np.sum(self.target*np.log(self.pred+self.delta), axis=1)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self) :\n",
    "        dout = (self.pred-self.target)/len(self.pred)\n",
    "        return dout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot label 만드는 함수\n",
    "\n",
    "def make_one_hot(labels) :\n",
    "    a = []\n",
    "    for label in labels :\n",
    "        one_hot = np.zeros(10)\n",
    "        one_hot[label] = 1\n",
    "        a.append(one_hot)\n",
    "    a = np.array(a)\n",
    "    return a\n",
    "\n",
    "# one_hot_labels = make_one_hot(train_label)\n",
    "# print(train_label[0])\n",
    "# print(one_hot_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, train_version = True) :\n",
    "    if train_version :\n",
    "        x = train_img\n",
    "        labels = train_label.squeeze()\n",
    "        print('In train dataset ... ')\n",
    "    else : \n",
    "        x = test_img\n",
    "        labels = test_label.squeeze()\n",
    "        print('\\nIn test dataset ... ')\n",
    "    \n",
    "    for layer in model.values() :\n",
    "        if isinstance(layer, Softmax_with_CrossEntropy) :\n",
    "            x = layer.softmax_forward(x)\n",
    "        else :\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "    preds = x.argmax(axis=1)\n",
    "    acc = np.sum(np.where(preds==labels, True, False))/len(labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def train_MLP(config) :\n",
    "    lr, num_epoch = config['learning_rate'], config['num_epoch']\n",
    "    print_loss_interval = 20\n",
    "    \n",
    "    layer1 = Linear(784, 100)\n",
    "    activation_function = config['activation_function']\n",
    "    layer2 = Linear(100, 10)\n",
    "    softmax_with_CE = Softmax_with_CrossEntropy()\n",
    "    \n",
    "    for iter in range(num_epoch) :\n",
    "        # forward\n",
    "        x = layer1.forward(train_img)\n",
    "        x = activation_function.forward(x)\n",
    "        x = layer2.forward(x)\n",
    "        preds = softmax_with_CE.softmax_forward(x)\n",
    "        \n",
    "        # loss\n",
    "        one_hot_labels = make_one_hot(train_label)\n",
    "        losses = softmax_with_CE.crossEntropy_forward(preds, one_hot_labels)\n",
    "        loss = losses.sum()/len(preds)\n",
    "        \n",
    "        # backward\n",
    "        dL = softmax_with_CE.backward()\n",
    "        dL = layer2.backward(dL, lr)\n",
    "        dL = activation_function.backward(dL)\n",
    "        dL = layer1.backward(dL, lr)\n",
    "        \n",
    "        if iter == 0 or (iter+1) % print_loss_interval == 0:\n",
    "            print(\"[epoch %d / %d] average loss : %f\" % (iter+1, num_epoch, loss))\n",
    "            \n",
    "    model = OrderedDict()\n",
    "    model['layer1'] = layer1\n",
    "    model['activation_function'] = activation_function\n",
    "    model['layer2'] = layer2\n",
    "    model['softmax_with_CE'] = softmax_with_CE\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100,\n",
    "            'activation_function' : ReLU()\n",
    "          }\n",
    "\n",
    "config2 = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100,\n",
    "            'activation_function' : LeakyReLU()\n",
    "          }\n",
    "\n",
    "config3 = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100,\n",
    "            'activation_function' : Sigmoid()\n",
    "          }\n",
    "\n",
    "config4 = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100,\n",
    "            'activation_function' : Tanh()\n",
    "          }\n",
    "\n",
    "print('---ReLU---')\n",
    "model1 = train_MLP(config1)\n",
    "\n",
    "print('\\n---Leaky_Relu---')\n",
    "model2 = train_MLP(config2)\n",
    "\n",
    "print('\\n---Sigmoid---')\n",
    "model3 = train_MLP(config3)\n",
    "\n",
    "print('\\n---TanH---')\n",
    "model4 = train_MLP(config4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "\n",
    "print('\\t Accuracy :', eval(model1, train_version=False))\n",
    "print('\\t Accuracy :', eval(model2, train_version=False))\n",
    "print('\\t Accuracy :', eval(model3, train_version=False))\n",
    "print('\\t Accuracy :', eval(model4, train_version=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_with_CrossEntropy :\n",
    "    def __init__(self) :\n",
    "        # softmax\n",
    "        self.softmax_x = None \n",
    "        self.softmax_out = None\n",
    "        # crossEntropy\n",
    "        self.pred = None\n",
    "        self.target = None\n",
    "        self.delta = 1e-7\n",
    "        \n",
    "    def softmax_forward(self, x) :\n",
    "        self.softmax_x = x\n",
    "        tmp = np.max(self.softmax_x, axis=1).reshape(-1, 1)\n",
    "        self.softmax_out = np.exp(self.softmax_x-tmp)/np.sum(np.exp(self.softmax_x-tmp), axis=1).reshape(-1,1)\n",
    "        return self.softmax_out\n",
    "    \n",
    "    def crossEntropy_forward(self, pred, target) :\n",
    "        self.pred = pred\n",
    "        self.target = target\n",
    "        loss = -np.sum(self.target*np.log(self.pred+self.delta), axis=1)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self) :\n",
    "        dout = (self.pred-self.target)/len(self.pred)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def train_MLP_v2(config,train_img = train_img, train_label = train_label) :\n",
    "    lr, num_epoch, batch_size = config['learning_rate'], config['num_epoch'], config['batch_size']\n",
    "    print_loss_interval = 20\n",
    "    \n",
    "    # Initialize the layers of the MLP\n",
    "    layer1 = Linear(784, 100) # First linear layer with 784 input features and 100 output features\n",
    "    activation_function = config['activation_function'] # Activation function object, such as ReLU\n",
    "    layer2 = Linear(100, 10) # Second linear layer with 100 input features and 10 output features\n",
    "    softmax_with_CE = Softmax_with_CrossEntropy() # Softmax with Cross-Entropy loss function object\n",
    "    \n",
    "    for iter in range(num_epoch):\n",
    "        # Forward and backward propagation for each mini-batch\n",
    "        num_data = len(train_img)\n",
    "        num_batch = num_data // batch_size\n",
    "        if num_data % batch_size != 0:\n",
    "            num_batch += 1\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch_idx in range(num_batch):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(num_data, (batch_idx + 1) * batch_size)\n",
    "\n",
    "            batch_img = train_img[start_idx:end_idx]\n",
    "            batch_label = train_label[start_idx:end_idx]\n",
    "\n",
    "            x = layer1.forward(batch_img) # Forward propagation through the first linear layer\n",
    "            x = activation_function.forward(x) # Forward propagation through the activation function\n",
    "            x = layer2.forward(x) # Forward propagation through the second linear layer\n",
    "            preds = softmax_with_CE.softmax_forward(x) # Calculate the softmax probabilities\n",
    "\n",
    "            # Calculate the loss and the gradient of the loss w.r.t the predictions\n",
    "            one_hot_labels = make_one_hot(batch_label)\n",
    "            losses = softmax_with_CE.crossEntropy_forward(preds, one_hot_labels)\n",
    "            batch_loss = losses.sum() / len(preds)\n",
    "            epoch_loss += batch_loss\n",
    "            dL = softmax_with_CE.backward() # Backward propagation through the loss function\n",
    "            dL = layer2.backward(dL, lr) # Backward propagation through the second linear layer\n",
    "            dL = activation_function.backward(dL) # Backward propagation through the activation function\n",
    "            dL = layer1.backward(dL, lr) # Backward propagation through the first linear layer\n",
    "\n",
    "        avg_loss = epoch_loss / num_batch\n",
    "\n",
    "        # Print the average loss at the current epoch\n",
    "        if iter == 0 or (iter + 1) % print_loss_interval == 0:\n",
    "            print(\"[epoch %d / %d] average loss : %f\" % (iter + 1, num_epoch, avg_loss))\n",
    "\n",
    "    # Save the trained model as an ordered dictionary\n",
    "    model = OrderedDict()\n",
    "    model['layer1'] = layer1\n",
    "    model['activation_function'] = activation_function\n",
    "    model['layer2'] = layer2\n",
    "    model['softmax_with_CE'] = softmax_with_CE\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---batch---')\n",
    "config_batch = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 20,\n",
    "            'activation_function' : ReLU(),\n",
    "            'batch_size' : len(train_img) \n",
    "          }\n",
    "model_batch = train_MLP_v2(config_batch)\n",
    "\n",
    "\n",
    "print('\\n---mini_batch---')\n",
    "config_mini_batch = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 20,\n",
    "            'activation_function' : ReLU(),\n",
    "            'batch_size' : 2500\n",
    "          }\n",
    "model_mini_batch = train_MLP_v2(config_mini_batch)\n",
    "\n",
    "print('\\n---stochastic---')\n",
    "config_stochastic = { 'learning_rate' : 0.001,\n",
    "            'num_epoch' : 20,\n",
    "            'activation_function' : ReLU(),\n",
    "            'batch_size' : 1\n",
    "          }\n",
    "model_stochastic = train_MLP_v2(config_stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t Accuracy :', eval(model_batch, train_version=False))\n",
    "print('\\t Accuracy :', eval(model_mini_batch, train_version=False))\n",
    "print('\\t Accuracy :', eval(model_stochastic, train_version=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Multi Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Linear_v2:\n",
    "    def __init__(self, input_size=1, hidden_size=1):\n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.randn(input_size, hidden_size)\n",
    "        self.b = np.zeros(hidden_size)\n",
    "        \n",
    "        # Store input size and hidden size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Calculate linear transformation\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout, lr, lamb=0):\n",
    "        # Calculate gradients for weights and biases    \n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dW += lamb * self.W\n",
    "        # Update weights and biases\n",
    "        self.W -= lr * dW\n",
    "        self.b -= lr * db\n",
    "        \n",
    "        # Calculate gradients for input\n",
    "        # print(dout.shape, self.W.T.shape)\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "    \n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_with_MSVM:\n",
    "    def __init__(self):\n",
    "        # softmax\n",
    "        self.softmax_out = None\n",
    "        self.softmax_x = None\n",
    "        # MSVM\n",
    "        self.pred = None\n",
    "        self.target = None\n",
    "        self.reg = None\n",
    "\n",
    "    def softmax_forward(self, x):\n",
    "        self.softmax_x = x\n",
    "        # subtracting the maximum value for numerical stability\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        self.softmax_out = exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "        return self.softmax_out\n",
    "    \n",
    "    def sigmoid_forward(self, x):\n",
    "        self.sigmoid_x = x\n",
    "        self.sigmoid_out = 1 / (1 + np.exp(-x))\n",
    "        return self.sigmoid_out\n",
    "        \n",
    "\n",
    "    def forward(self, x, target, reg=0.1):\n",
    "        self.target = target\n",
    "        self.MSVM_out = self.softmax_forward(x)\n",
    "        scores = np.dot(x, self.MSVM_out.T)\n",
    "        correct_scores = scores[np.arange(x.shape[0]), target.flatten()]\n",
    "        margins = np.maximum(0, scores - correct_scores.reshape(-1, 1) + 1)\n",
    "        margins[np.arange(x.shape[0]), target.flatten()] = 0\n",
    "        loss = np.sum(margins)\n",
    "        loss /= x.shape[0]\n",
    "        loss += 0.5 * reg * np.sum(self.MSVM_out * self.MSVM_out)\n",
    "        self.pred = (scores >= np.max(scores, axis=1).reshape(-1, 1)).astype(float)\n",
    "        self.margin = margins\n",
    "        return loss\n",
    "\n",
    "    def backward(self, lr, reg):\n",
    "        self.reg = reg\n",
    "        batch_size = self.pred.shape[0]\n",
    "        dL_dsoftmax = self.MSVM_out - self.target\n",
    "\n",
    "        dsoftmax_dMSVMx = (self.softmax_out * (1 - self.softmax_out))\n",
    "        dL_dMSVMx = dL_dsoftmax * dsoftmax_dMSVMx\n",
    "\n",
    "        zero_mask = (dL_dsoftmax == 0) | (self.MSVM_out == 0)\n",
    "        dL_dMSVMx = np.where(zero_mask, 0, -dL_dMSVMx)\n",
    "        dMSVMx_dw = np.dot(dL_dMSVMx.T, self.pred) / batch_size\n",
    "\n",
    "        # L2 regularization\n",
    "        dMSVMx_dw += self.reg * self.MSVM_out\n",
    "\n",
    "        self.MSVM_out -= np.clip(lr * dMSVMx_dw.T, -1e8, 1e8)\n",
    "        return dL_dMSVMx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def clip_gradient(grad, clip_value):\n",
    "    return np.clip(grad, -clip_value, clip_value)\n",
    "\n",
    "def train_MSVM(config, train_img=train_img, train_label=train_label):\n",
    "    lr, num_epoch, batch_size, reg, grad_clip = config['learning_rate'], config['num_epoch'], config['batch_size'], config['reg'], config['grad_clip']\n",
    "    print_loss_interval = 20\n",
    "    \n",
    "    layer1 = Linear_v2(784, 100)\n",
    "    activation_function = config['activation_function']\n",
    "    layer2 = Linear_v2(100, 10)\n",
    "    softmax_with_MSVM = Softmax_with_MSVM()\n",
    "    \n",
    "    for iter in range(num_epoch):\n",
    "        # forward\n",
    "        num_data = len(train_img)\n",
    "        num_batch = num_data // batch_size\n",
    "        if num_data % batch_size != 0:\n",
    "            num_batch += 1\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch_idx in range(num_batch):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(num_data, (batch_idx + 1) * batch_size)\n",
    "\n",
    "            batch_img = train_img[start_idx:end_idx]\n",
    "            batch_label = train_label[start_idx:end_idx]\n",
    "\n",
    "            x = layer1.forward(batch_img)\n",
    "            x = activation_function.forward(x)\n",
    "            x = layer2.forward(x)\n",
    "            preds = softmax_with_MSVM.softmax_forward(x)\n",
    "            \n",
    "            loss = softmax_with_MSVM.forward(preds, batch_label, config['reg'])\n",
    "            batch_loss = loss.sum() / len(preds)\n",
    "            epoch_loss += batch_loss\n",
    "\n",
    "            # backward\n",
    "            dL = softmax_with_MSVM.backward(lr, reg)\n",
    "            dL = clip_gradient(dL, grad_clip)\n",
    "            dL = layer2.backward(dL, lr)\n",
    "            dL = clip_gradient(dL, grad_clip)\n",
    "            dL = activation_function.backward(dL)\n",
    "            dL = clip_gradient(dL, grad_clip)\n",
    "            dL = layer1.backward(dL, lr)\n",
    "            dL = clip_gradient(dL, grad_clip)\n",
    "\n",
    "        avg_loss = epoch_loss / num_batch\n",
    "        print(avg_loss)\n",
    "\n",
    "        if iter == 0 or (iter + 1) % print_loss_interval == 0:\n",
    "            print(\"[epoch %d / %d] average loss : %f\" % (iter + 1, num_epoch, avg_loss))\n",
    "\n",
    "    model = OrderedDict()\n",
    "    model['layer1'] = layer1\n",
    "    model['activation_function'] = activation_function\n",
    "    model['layer2'] = layer2\n",
    "    model['softmax_with_MSVM'] = softmax_with_MSVM\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { 'learning_rate' : 0.0001,\n",
    "            'num_epoch' : 20,\n",
    "            'activation_function' : ReLU(),\n",
    "            'batch_size' : 10,\n",
    "            'reg' : 0.1,\n",
    "            'grad_clip' : 5\n",
    "          }\n",
    "\n",
    "\n",
    "model = train_MSVM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, train_version = True) :\n",
    "    if train_version :\n",
    "        x = train_img\n",
    "        labels = train_label.squeeze()\n",
    "        print('In train dataset ... ')\n",
    "    else : \n",
    "        x = test_img\n",
    "        labels = test_label.squeeze()\n",
    "        print('\\nIn test dataset ... ')\n",
    "    \n",
    "    for layer in model.values() :\n",
    "        if isinstance(layer, Softmax_with_MSVM) :\n",
    "            x = layer.softmax_forward(x)\n",
    "        else :\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "    preds = x.argmax(axis=1)\n",
    "    acc = np.sum(np.where(preds==labels, True, False))/len(labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t Accuracy :', eval(model, train_version=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (3-layer 이상 MLP 구현)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def get_mask(self):\n",
    "        return self.mask.shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout, x):\n",
    "        if self.mask.shape != x.shape:\n",
    "            self.mask = np.zeros_like(x, dtype = np.bool)\n",
    "        dx = dout\n",
    "        dx[self.mask] = 0\n",
    "        return dx \n",
    "\n",
    "    \n",
    "class LeakyReLU:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] *= self.alpha\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout, x):\n",
    "        if self.mask is None or self.mask.shape != x.shape:\n",
    "            self.mask = np.zeros_like(x, dtype=np.bool)\n",
    "        dx = dout\n",
    "        dx[self.mask] *= self.alpha\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        return dx\n",
    "\n",
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = np.tanh(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out ** 2)\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_with_Mul_CrossEntropy:\n",
    "    def __init__(self):\n",
    "        self.softmax_x = None \n",
    "        self.softmax_out = None\n",
    "        self.pred = None\n",
    "        self.target = None\n",
    "\n",
    "    def softmax_forward(self, x):\n",
    "        self.softmax_x = x\n",
    "        tmp = np.max(self.softmax_x, axis=1).reshape(-1, 1)\n",
    "        self.softmax_out = np.exp(self.softmax_x - tmp) / np.sum(np.exp(self.softmax_x - tmp), axis=1).reshape(-1, 1)\n",
    "        return self.softmax_out\n",
    "    \n",
    "    def crossEntropy_forward(self, pred, target):\n",
    "        delta = 1e-7\n",
    "        self.pred = pred\n",
    "        self.target = target\n",
    "        loss = -np.sum(self.target * np.log(self.pred + delta), axis=1)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dL_next):\n",
    "        batch_size = len(self.pred)\n",
    "        dsoftmax = (self.pred - self.target) / batch_size\n",
    "        dL = dsoftmax * dL_next\n",
    "        return dL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def train_Multi_MLP(config):\n",
    "    lr, num_epoch, batch_size = config['learning_rate'], config['num_epoch'], config['batch_size']\n",
    "    print_loss_interval = 1\n",
    "    \n",
    "    layer1 = Linear(784, 256)\n",
    "    activation_function1 = config['activation_function']\n",
    "    layer2 = Linear(256, 128)\n",
    "    activation_function2 = config['activation_function']\n",
    "    layer3 = Linear(128, 64)\n",
    "    activation_function3 = config['activation_function']\n",
    "    layer4 = Linear(64, 32)\n",
    "    activation_function4 = config['activation_function']\n",
    "    layer5 = Linear(32, 10)\n",
    "    softmax_with_CE = Softmax_with_Mul_CrossEntropy()\n",
    "    \n",
    "    for iter in range(num_epoch):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for i in range(0, len(train_img), batch_size):\n",
    "            # forward\n",
    "            batch_img = train_img[i:i+batch_size]\n",
    "            batch_label = train_label[i:i+batch_size]\n",
    "            \n",
    "            x = layer1.forward(batch_img)\n",
    "            x_1 = activation_function1.forward(x)\n",
    "            x = layer2.forward(x_1)\n",
    "            x_2 = activation_function2.forward(x)\n",
    "            x = layer3.forward(x_2)\n",
    "            x_3 = activation_function3.forward(x)\n",
    "            x = layer4.forward(x_3)\n",
    "            x_4 = activation_function4.forward(x)\n",
    "            x = layer5.forward(x_4)\n",
    "            preds = softmax_with_CE.softmax_forward(x)\n",
    "\n",
    "            # loss\n",
    "            one_hot_labels = make_one_hot(batch_label)\n",
    "            losses = softmax_with_CE.crossEntropy_forward(preds, one_hot_labels)\n",
    "            batch_loss = losses.sum() / len(preds)\n",
    "            total_loss += batch_loss\n",
    "            num_batches += 1\n",
    "\n",
    "            # backward\n",
    "            dL = softmax_with_CE.backward(one_hot_labels)\n",
    "            dL = layer5.backward(dL)\n",
    "            dL = activation_function4.backward(dL,x_4)\n",
    "            dL = layer4.backward(dL)        \n",
    "            dL = activation_function3.backward(dL,x_3)\n",
    "            dL = layer3.backward(dL)\n",
    "            dL = activation_function2.backward(dL,x_2)\n",
    "            dL = layer2.backward(dL)\n",
    "            dL = activation_function1.backward(dL,x_1)\n",
    "            dL = layer1.backward(dL)\n",
    "\n",
    "        # average batch loss\n",
    "        avg_loss = total_loss / num_batches\n",
    "        \n",
    "        if iter == 0 or (iter+1) % print_loss_interval == 0:\n",
    "            print(\"[epoch %d / %d] average loss : %f\" % (iter+1, num_epoch, avg_loss))\n",
    "            \n",
    "    model = OrderedDict()\n",
    "    model['layer1'] = layer1\n",
    "    model['activation_function1'] = activation_function1\n",
    "    model['layer2'] = layer2\n",
    "    model['activation_function2'] = activation_function2\n",
    "    model['layer3'] = layer3\n",
    "    model['activation_function3'] = activation_function3\n",
    "    model['layer4'] = layer4\n",
    "    model['activation_function4'] = activation_function4\n",
    "    model['layer5'] = layer5\n",
    "    model['softmax_with_CE'] = softmax_with_CE\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { 'learning_rate' : 0.00001,\n",
    "            'num_epoch' : 3,\n",
    "            'activation_function' : LeakyReLU(),\n",
    "            'batch_size' : 2500,\n",
    "            'reg' : 0.1\n",
    "          }\n",
    "\n",
    "model = train_Multi_MLP(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
