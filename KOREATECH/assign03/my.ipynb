{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b4dZz_X9FGRV"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# install\n",
        "## numpy\n",
        "## matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6MaNVxpFGRX"
      },
      "source": [
        "### data load & preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4g8-8OctFGRY",
        "outputId": "93022e62-a890-495a-c2b0-cefa9e110295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 1, 28, 28)\n",
            "(60000,)\n"
          ]
        }
      ],
      "source": [
        "from dataset.mnist import load_mnist\n",
        "\n",
        "(train_raw_img, train_label), (test_raw_img, test_label) = load_mnist(flatten=False, normalize=False)\n",
        "print(train_raw_img.shape)\n",
        "print(train_label.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x1000 with 25 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_raw_img[i][0], cmap=plt.cm.binary)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uQ8U02CCFGRZ",
        "outputId": "1d7515c9-e956-4bc8-dcce-a7a3a856bc34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
              " array([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(train_label, return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xZnGA_YUFGRa",
        "outputId": "4dd40e63-d080-4e5e-93ef-1d857d6f8b9d"
      },
      "outputs": [],
      "source": [
        "# train_dataset split according to the number\n",
        "\n",
        "new_train_img = [[] for _ in range(10)]\n",
        "new_train_label = [[] for _ in range(10)]\n",
        "\n",
        "for i in range(len(train_label)) :\n",
        "    new_train_img[train_label[i]].append(train_raw_img[i])\n",
        "    new_train_label[train_label[i]].append(train_label[i])\n",
        "\n",
        "# print(len(new_train_img[0])) # 0에 해당하는 image 개수\n",
        "# print(new_train_img[0][0].shape) # 0에 해당하는 image중 첫번째 image의 shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7tJ7XHTFGRb"
      },
      "source": [
        "### 1. Create a classifier that distinguishes between zero and non-zero (using logistic regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MYkumpg9FGRb"
      },
      "outputs": [],
      "source": [
        "def make_sample(idx) :\n",
        "    sample_img = []\n",
        "    sample_label = []\n",
        "    \n",
        "    # data sampling \n",
        "    for i in range(10) :\n",
        "        if i == idx :\n",
        "            sample_img += new_train_img[i][:1000]\n",
        "            sample_label += (new_train_label[i][:1000])\n",
        "        else :\n",
        "            sample_img += new_train_img[i][:111]\n",
        "            sample_label += (new_train_label[i][:111])\n",
        "\n",
        "    sample_img = np.array(sample_img)\n",
        "    sample_label = np.array(sample_label)\n",
        "    \n",
        "    # normalization (set value 0 ~ 1)\n",
        "    sample_img = sample_img.astype('float')/255\n",
        "    \n",
        "    # target number는 1, 아니면 0\n",
        "    sample_label = np.where(sample_label==idx, 1 ,0)\n",
        "    \n",
        "    # reshape\n",
        "    sample_img = sample_img.reshape(len(sample_img.squeeze()), -1)\n",
        "    sample_label = sample_label.reshape(len(sample_label.squeeze()), -1)\n",
        "    \n",
        "    return sample_img, sample_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1IhWbvTIFGRb"
      },
      "outputs": [],
      "source": [
        "train_X, train_y = make_sample(idx = 0) # idx = target number\n",
        "train_X = np.insert(train_X, 0, 1, axis=1) # bias 추가\n",
        "\n",
        "# print(train_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e8O2C8MLFGRc",
        "outputId": "28f55cf1-0bce-4c5b-8356-62b331f5aa90"
      },
      "outputs": [],
      "source": [
        "# cross entropy loss\n",
        "def CrossEntropyLoss(w, X, y) :\n",
        "    delta = 1e-7\n",
        "    \n",
        "    preds = 1 / (1+np.exp(-X.dot(w)))\n",
        "    loss = np.sum(-y*np.log(preds+delta)-(1-y)*np.log(1-preds+delta))/len(preds)\n",
        "        \n",
        "    return loss , preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# eval (accuracy)\n",
        "\n",
        "def eval(idx, w) :\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        idx : target_number\n",
        "        w : parameter\n",
        "    \"\"\"\n",
        "    test_X = test_raw_img.astype('float')/255    \n",
        "    test_X = test_X.reshape(len(test_X.squeeze()), -1)\n",
        "    test_X = np.insert(test_X, 0, 1, axis=1) # + bias\n",
        "\n",
        "    test_y = np.where(test_label==idx, 1 ,0)\n",
        "    test_y = test_y.reshape(len(test_y.squeeze()), -1)\n",
        "    \n",
        "    preds = 1/(1+np.exp(-test_X.dot(w)))\n",
        "    result = np.where(preds>0.5, 1, 0)\n",
        "    \n",
        "    acc = np.sum(np.where(result==test_y, True, False))/len(preds)\n",
        "    \n",
        "    return acc\n",
        "    # print('accuracy : ', acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qeVJQU2RFGRc"
      },
      "outputs": [],
      "source": [
        "# train\n",
        "\n",
        "# (추가)change parameter to take a evaluation for each different digit. \n",
        "\n",
        "def train(X, y, index) :\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        X : train_X\n",
        "        y : train_y\n",
        "\n",
        "    Returns:\n",
        "        w : weight\n",
        "        \n",
        "    (추가)index : \n",
        "        (추가)index = to take evaluation for each digit \n",
        "        (추가)the range of it is 0 ~ 9\n",
        "    \"\"\"\n",
        "    w = np.random.randn(len(X[0]), 1)\n",
        "    lr = 0.1 # learning rate(수정 가능)\n",
        "    step = 0\n",
        "    acc = 0\n",
        "    prev_loss = float('inf')\n",
        "    J_history = list()\n",
        "    ACC_history = list()\n",
        "    \n",
        "    while (acc <= 0.85) :\n",
        "        step += 1\n",
        "        correct = 0\n",
        "        \n",
        "        acc = eval(index, w)\n",
        "        \n",
        "        loss, preds = CrossEntropyLoss(w, X, y)\n",
        "        \n",
        "        diff = preds - y\n",
        "        gradient = X.T.dot(diff) / X.shape[0]\n",
        "        w -= lr * gradient\n",
        "        \n",
        "        if abs(loss - prev_loss) < 1e-4:\n",
        "            break\n",
        "        \n",
        "        prev_loss = loss\n",
        "        J_history.append(loss)\n",
        "        ACC_history.append(acc)\n",
        "        print(\"total step : %d \" % step)\n",
        "        print(\"error : %f, accuarcy : %f\" % (loss, acc))\n",
        "        \n",
        "    return w, J_history, ACC_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nWXi1kuJFGRc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total step : 1 \n",
            "error : 3.563889, accuarcy : 0.250700\n",
            "total step : 2 \n",
            "error : 3.316959, accuarcy : 0.277700\n",
            "total step : 3 \n",
            "error : 3.086968, accuarcy : 0.304100\n",
            "total step : 4 \n",
            "error : 2.873176, accuarcy : 0.332600\n",
            "total step : 5 \n",
            "error : 2.674862, accuarcy : 0.359300\n",
            "total step : 6 \n",
            "error : 2.491320, accuarcy : 0.386600\n",
            "total step : 7 \n",
            "error : 2.321867, accuarcy : 0.414800\n",
            "total step : 8 \n",
            "error : 2.165850, accuarcy : 0.443300\n",
            "total step : 9 \n",
            "error : 2.022621, accuarcy : 0.468200\n",
            "total step : 10 \n",
            "error : 1.891493, accuarcy : 0.489800\n",
            "total step : 11 \n",
            "error : 1.771724, accuarcy : 0.512200\n",
            "total step : 12 \n",
            "error : 1.662534, accuarcy : 0.535600\n",
            "total step : 13 \n",
            "error : 1.563124, accuarcy : 0.553500\n",
            "total step : 14 \n",
            "error : 1.472699, accuarcy : 0.570000\n",
            "total step : 15 \n",
            "error : 1.390488, accuarcy : 0.588200\n",
            "total step : 16 \n",
            "error : 1.315759, accuarcy : 0.603500\n",
            "total step : 17 \n",
            "error : 1.247830, accuarcy : 0.616800\n",
            "total step : 18 \n",
            "error : 1.186065, accuarcy : 0.632600\n",
            "total step : 19 \n",
            "error : 1.129876, accuarcy : 0.645500\n",
            "total step : 20 \n",
            "error : 1.078718, accuarcy : 0.658700\n",
            "total step : 21 \n",
            "error : 1.032086, accuarcy : 0.671700\n",
            "total step : 22 \n",
            "error : 0.989520, accuarcy : 0.682800\n",
            "total step : 23 \n",
            "error : 0.950598, accuarcy : 0.694800\n",
            "total step : 24 \n",
            "error : 0.914942, accuarcy : 0.705700\n",
            "total step : 25 \n",
            "error : 0.882212, accuarcy : 0.713500\n",
            "total step : 26 \n",
            "error : 0.852104, accuarcy : 0.721300\n",
            "total step : 27 \n",
            "error : 0.824350, accuarcy : 0.728900\n",
            "total step : 28 \n",
            "error : 0.798710, accuarcy : 0.735700\n",
            "total step : 29 \n",
            "error : 0.774973, accuarcy : 0.743500\n",
            "total step : 30 \n",
            "error : 0.752950, accuarcy : 0.749900\n",
            "total step : 31 \n",
            "error : 0.732478, accuarcy : 0.755400\n",
            "total step : 32 \n",
            "error : 0.713408, accuarcy : 0.759900\n",
            "total step : 33 \n",
            "error : 0.695611, accuarcy : 0.765700\n",
            "total step : 34 \n",
            "error : 0.678972, accuarcy : 0.770100\n",
            "total step : 35 \n",
            "error : 0.663388, accuarcy : 0.774700\n",
            "total step : 36 \n",
            "error : 0.648767, accuarcy : 0.779200\n",
            "total step : 37 \n",
            "error : 0.635026, accuarcy : 0.783300\n",
            "total step : 38 \n",
            "error : 0.622094, accuarcy : 0.786900\n",
            "total step : 39 \n",
            "error : 0.609902, accuarcy : 0.790300\n",
            "total step : 40 \n",
            "error : 0.598392, accuarcy : 0.794200\n",
            "total step : 41 \n",
            "error : 0.587509, accuarcy : 0.797200\n",
            "total step : 42 \n",
            "error : 0.577205, accuarcy : 0.800300\n",
            "total step : 43 \n",
            "error : 0.567435, accuarcy : 0.805200\n",
            "total step : 44 \n",
            "error : 0.558158, accuarcy : 0.807300\n",
            "total step : 45 \n",
            "error : 0.549339, accuarcy : 0.811600\n",
            "total step : 46 \n",
            "error : 0.540944, accuarcy : 0.814600\n",
            "total step : 47 \n",
            "error : 0.532942, accuarcy : 0.816900\n",
            "total step : 48 \n",
            "error : 0.525306, accuarcy : 0.819900\n",
            "total step : 49 \n",
            "error : 0.518010, accuarcy : 0.822100\n",
            "total step : 50 \n",
            "error : 0.511032, accuarcy : 0.824600\n",
            "total step : 51 \n",
            "error : 0.504349, accuarcy : 0.826500\n",
            "total step : 52 \n",
            "error : 0.497942, accuarcy : 0.828100\n",
            "total step : 53 \n",
            "error : 0.491795, accuarcy : 0.829900\n",
            "total step : 54 \n",
            "error : 0.485890, accuarcy : 0.831600\n",
            "total step : 55 \n",
            "error : 0.480212, accuarcy : 0.833800\n",
            "total step : 56 \n",
            "error : 0.474747, accuarcy : 0.835200\n",
            "total step : 57 \n",
            "error : 0.469484, accuarcy : 0.835900\n",
            "total step : 58 \n",
            "error : 0.464410, accuarcy : 0.837300\n",
            "total step : 59 \n",
            "error : 0.459514, accuarcy : 0.839000\n",
            "total step : 60 \n",
            "error : 0.454786, accuarcy : 0.841000\n",
            "total step : 61 \n",
            "error : 0.450218, accuarcy : 0.842300\n",
            "total step : 62 \n",
            "error : 0.445800, accuarcy : 0.843700\n",
            "total step : 63 \n",
            "error : 0.441524, accuarcy : 0.844500\n",
            "total step : 64 \n",
            "error : 0.437384, accuarcy : 0.845400\n",
            "total step : 65 \n",
            "error : 0.433372, accuarcy : 0.847000\n",
            "total step : 66 \n",
            "error : 0.429482, accuarcy : 0.848500\n",
            "total step : 67 \n",
            "error : 0.425707, accuarcy : 0.849600\n",
            "total step : 68 \n",
            "error : 0.422043, accuarcy : 0.850300\n"
          ]
        }
      ],
      "source": [
        "# save weight\n",
        "w,J_history, ACC_history = train(train_X, train_y, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the cost function vs. iterations to check for convergence\n",
        "plt.plot(J_history, label='Cost Function')\n",
        "plt.plot(ACC_history, label='Accuracy')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost Function')\n",
        "plt.title('Gradient Descent Convergence')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itKbnbmNFGRd"
      },
      "source": [
        "### 2. multi class single label classification (using logistic regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ExZROSKHFGRd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total step : 1 \n",
            "error : 6.365101, accuarcy : 0.324700\n",
            "total step : 2 \n",
            "error : 5.709550, accuarcy : 0.298700\n",
            "total step : 3 \n",
            "error : 5.124185, accuarcy : 0.280700\n",
            "total step : 4 \n",
            "error : 4.638520, accuarcy : 0.272700\n",
            "total step : 5 \n",
            "error : 4.250882, accuarcy : 0.269900\n",
            "total step : 6 \n",
            "error : 3.939100, accuarcy : 0.271200\n",
            "total step : 7 \n",
            "error : 3.677872, accuarcy : 0.278200\n",
            "total step : 8 \n",
            "error : 3.449442, accuarcy : 0.288300\n",
            "total step : 9 \n",
            "error : 3.243240, accuarcy : 0.301200\n",
            "total step : 10 \n",
            "error : 3.053246, accuarcy : 0.314600\n",
            "total step : 11 \n",
            "error : 2.876193, accuarcy : 0.330700\n",
            "total step : 12 \n",
            "error : 2.710348, accuarcy : 0.347800\n",
            "total step : 13 \n",
            "error : 2.554761, accuarcy : 0.364600\n",
            "total step : 14 \n",
            "error : 2.408853, accuarcy : 0.383300\n",
            "total step : 15 \n",
            "error : 2.272215, accuarcy : 0.402600\n",
            "total step : 16 \n",
            "error : 2.144511, accuarcy : 0.419600\n",
            "total step : 17 \n",
            "error : 2.025417, accuarcy : 0.436700\n",
            "total step : 18 \n",
            "error : 1.914587, accuarcy : 0.455100\n",
            "total step : 19 \n",
            "error : 1.811645, accuarcy : 0.471500\n",
            "total step : 20 \n",
            "error : 1.716198, accuarcy : 0.488700\n",
            "total step : 21 \n",
            "error : 1.627842, accuarcy : 0.508800\n",
            "total step : 22 \n",
            "error : 1.546168, accuarcy : 0.525500\n",
            "total step : 23 \n",
            "error : 1.470761, accuarcy : 0.539900\n",
            "total step : 24 \n",
            "error : 1.401205, accuarcy : 0.557200\n",
            "total step : 25 \n",
            "error : 1.337078, accuarcy : 0.572700\n",
            "total step : 26 \n",
            "error : 1.277959, accuarcy : 0.586800\n",
            "total step : 27 \n",
            "error : 1.223433, accuarcy : 0.601900\n",
            "total step : 28 \n",
            "error : 1.173101, accuarcy : 0.615000\n",
            "total step : 29 \n",
            "error : 1.126583, accuarcy : 0.627300\n",
            "total step : 30 \n",
            "error : 1.083532, accuarcy : 0.639000\n",
            "total step : 31 \n",
            "error : 1.043628, accuarcy : 0.651400\n",
            "total step : 32 \n",
            "error : 1.006582, accuarcy : 0.661600\n",
            "total step : 33 \n",
            "error : 0.972134, accuarcy : 0.672400\n",
            "total step : 34 \n",
            "error : 0.940052, accuarcy : 0.682600\n",
            "total step : 35 \n",
            "error : 0.910125, accuarcy : 0.693200\n",
            "total step : 36 \n",
            "error : 0.882165, accuarcy : 0.701600\n",
            "total step : 37 \n",
            "error : 0.856001, accuarcy : 0.709900\n",
            "total step : 38 \n",
            "error : 0.831481, accuarcy : 0.717700\n",
            "total step : 39 \n",
            "error : 0.808467, accuarcy : 0.727400\n",
            "total step : 40 \n",
            "error : 0.786832, accuarcy : 0.735300\n",
            "total step : 41 \n",
            "error : 0.766465, accuarcy : 0.742600\n",
            "total step : 42 \n",
            "error : 0.747265, accuarcy : 0.747800\n",
            "total step : 43 \n",
            "error : 0.729138, accuarcy : 0.753300\n",
            "total step : 44 \n",
            "error : 0.712004, accuarcy : 0.760500\n",
            "total step : 45 \n",
            "error : 0.695786, accuarcy : 0.767400\n",
            "total step : 46 \n",
            "error : 0.680417, accuarcy : 0.773100\n",
            "total step : 47 \n",
            "error : 0.665835, accuarcy : 0.777900\n",
            "total step : 48 \n",
            "error : 0.651986, accuarcy : 0.783800\n",
            "total step : 49 \n",
            "error : 0.638817, accuarcy : 0.788500\n",
            "total step : 50 \n",
            "error : 0.626283, accuarcy : 0.793500\n",
            "total step : 51 \n",
            "error : 0.614341, accuarcy : 0.797700\n",
            "total step : 52 \n",
            "error : 0.602951, accuarcy : 0.802700\n",
            "total step : 53 \n",
            "error : 0.592079, accuarcy : 0.807200\n",
            "total step : 54 \n",
            "error : 0.581691, accuarcy : 0.810700\n",
            "total step : 55 \n",
            "error : 0.571756, accuarcy : 0.814100\n",
            "total step : 56 \n",
            "error : 0.562246, accuarcy : 0.817000\n",
            "total step : 57 \n",
            "error : 0.553135, accuarcy : 0.821000\n",
            "total step : 58 \n",
            "error : 0.544399, accuarcy : 0.824300\n",
            "total step : 59 \n",
            "error : 0.536015, accuarcy : 0.827600\n",
            "total step : 60 \n",
            "error : 0.527963, accuarcy : 0.830000\n",
            "total step : 61 \n",
            "error : 0.520224, accuarcy : 0.832600\n",
            "total step : 62 \n",
            "error : 0.512779, accuarcy : 0.834700\n",
            "total step : 63 \n",
            "error : 0.505612, accuarcy : 0.836500\n",
            "total step : 64 \n",
            "error : 0.498708, accuarcy : 0.840000\n",
            "total step : 65 \n",
            "error : 0.492053, accuarcy : 0.841900\n",
            "total step : 66 \n",
            "error : 0.485632, accuarcy : 0.844100\n",
            "total step : 67 \n",
            "error : 0.479434, accuarcy : 0.845600\n",
            "total step : 68 \n",
            "error : 0.473448, accuarcy : 0.848100\n",
            "total step : 69 \n",
            "error : 0.467662, accuarcy : 0.849600\n",
            "total step : 70 \n",
            "error : 0.462067, accuarcy : 0.852400\n"
          ]
        }
      ],
      "source": [
        "train_0X, train_0y = make_sample(idx = 0) # idx = target number\n",
        "train_0X = np.insert(train_0X, 0, 1, axis=1) # bias 추가\n",
        "w0,J0_history, ACC0_history = train(train_X, train_y,0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total step : 1 \n",
            "error : 4.089703, accuarcy : 0.827500\n",
            "total step : 2 \n",
            "error : 3.466404, accuarcy : 0.810400\n",
            "total step : 3 \n",
            "error : 2.896703, accuarcy : 0.792500\n",
            "total step : 4 \n",
            "error : 2.414390, accuarcy : 0.774200\n",
            "total step : 5 \n",
            "error : 2.040623, accuarcy : 0.766800\n",
            "total step : 6 \n",
            "error : 1.770120, accuarcy : 0.761100\n",
            "total step : 7 \n",
            "error : 1.579804, accuarcy : 0.759100\n",
            "total step : 8 \n",
            "error : 1.443130, accuarcy : 0.758600\n",
            "total step : 9 \n",
            "error : 1.339689, accuarcy : 0.758200\n",
            "total step : 10 \n",
            "error : 1.256674, accuarcy : 0.760200\n",
            "total step : 11 \n",
            "error : 1.186723, accuarcy : 0.764400\n",
            "total step : 12 \n",
            "error : 1.125694, accuarcy : 0.767900\n",
            "total step : 13 \n",
            "error : 1.071225, accuarcy : 0.772600\n",
            "total step : 14 \n",
            "error : 1.021911, accuarcy : 0.777800\n",
            "total step : 15 \n",
            "error : 0.976870, accuarcy : 0.783800\n",
            "total step : 16 \n",
            "error : 0.935508, accuarcy : 0.790000\n",
            "total step : 17 \n",
            "error : 0.897394, accuarcy : 0.795000\n",
            "total step : 18 \n",
            "error : 0.862192, accuarcy : 0.800900\n",
            "total step : 19 \n",
            "error : 0.829623, accuarcy : 0.806700\n",
            "total step : 20 \n",
            "error : 0.799446, accuarcy : 0.811500\n",
            "total step : 21 \n",
            "error : 0.771443, accuarcy : 0.816100\n",
            "total step : 22 \n",
            "error : 0.745421, accuarcy : 0.820500\n",
            "total step : 23 \n",
            "error : 0.721202, accuarcy : 0.824800\n",
            "total step : 24 \n",
            "error : 0.698624, accuarcy : 0.828300\n",
            "total step : 25 \n",
            "error : 0.677541, accuarcy : 0.833100\n",
            "total step : 26 \n",
            "error : 0.657822, accuarcy : 0.836200\n",
            "total step : 27 \n",
            "error : 0.639347, accuarcy : 0.839200\n",
            "total step : 28 \n",
            "error : 0.622008, accuarcy : 0.842500\n",
            "total step : 29 \n",
            "error : 0.605709, accuarcy : 0.844900\n",
            "total step : 30 \n",
            "error : 0.590365, accuarcy : 0.849000\n",
            "total step : 31 \n",
            "error : 0.575899, accuarcy : 0.851500\n"
          ]
        }
      ],
      "source": [
        "train_1X, train_1y = make_sample(idx = 1) # idx = target number\n",
        "train_1X = np.insert(train_1X, 0, 1, axis=1) # bias 추가\n",
        "w1,J1_history, ACC1_history = train(train_1X, train_1y,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total step : 1 \n",
            "error : 2.819543, accuarcy : 0.388500\n",
            "total step : 2 \n",
            "error : 2.545350, accuarcy : 0.436700\n",
            "total step : 3 \n",
            "error : 2.335373, accuarcy : 0.478500\n",
            "total step : 4 \n",
            "error : 2.178272, accuarcy : 0.517100\n",
            "total step : 5 \n",
            "error : 2.061633, accuarcy : 0.543300\n",
            "total step : 6 \n",
            "error : 1.974090, accuarcy : 0.568500\n",
            "total step : 7 \n",
            "error : 1.906440, accuarcy : 0.588600\n",
            "total step : 8 \n",
            "error : 1.851876, accuarcy : 0.606700\n",
            "total step : 9 \n",
            "error : 1.805872, accuarcy : 0.620300\n",
            "total step : 10 \n",
            "error : 1.765580, accuarcy : 0.630000\n",
            "total step : 11 \n",
            "error : 1.729213, accuarcy : 0.640400\n",
            "total step : 12 \n",
            "error : 1.695641, accuarcy : 0.649700\n",
            "total step : 13 \n",
            "error : 1.664142, accuarcy : 0.657400\n",
            "total step : 14 \n",
            "error : 1.634250, accuarcy : 0.664600\n",
            "total step : 15 \n",
            "error : 1.605660, accuarcy : 0.669600\n",
            "total step : 16 \n",
            "error : 1.578173, accuarcy : 0.675900\n",
            "total step : 17 \n",
            "error : 1.551654, accuarcy : 0.681400\n",
            "total step : 18 \n",
            "error : 1.526010, accuarcy : 0.686100\n",
            "total step : 19 \n",
            "error : 1.501174, accuarcy : 0.691400\n",
            "total step : 20 \n",
            "error : 1.477097, accuarcy : 0.696400\n",
            "total step : 21 \n",
            "error : 1.453741, accuarcy : 0.700200\n",
            "total step : 22 \n",
            "error : 1.431074, accuarcy : 0.702800\n",
            "total step : 23 \n",
            "error : 1.409070, accuarcy : 0.705700\n",
            "total step : 24 \n",
            "error : 1.387705, accuarcy : 0.709800\n",
            "total step : 25 \n",
            "error : 1.366956, accuarcy : 0.714100\n",
            "total step : 26 \n",
            "error : 1.346804, accuarcy : 0.716100\n",
            "total step : 27 \n",
            "error : 1.327228, accuarcy : 0.719200\n",
            "total step : 28 \n",
            "error : 1.308209, accuarcy : 0.721500\n",
            "total step : 29 \n",
            "error : 1.289730, accuarcy : 0.723500\n",
            "total step : 30 \n",
            "error : 1.271772, accuarcy : 0.725800\n",
            "total step : 31 \n",
            "error : 1.254318, accuarcy : 0.728400\n",
            "total step : 32 \n",
            "error : 1.237352, accuarcy : 0.731100\n",
            "total step : 33 \n",
            "error : 1.220857, accuarcy : 0.734000\n",
            "total step : 34 \n",
            "error : 1.204818, accuarcy : 0.735500\n",
            "total step : 35 \n",
            "error : 1.189220, accuarcy : 0.737600\n",
            "total step : 36 \n",
            "error : 1.174048, accuarcy : 0.739600\n",
            "total step : 37 \n",
            "error : 1.159290, accuarcy : 0.741300\n",
            "total step : 38 \n",
            "error : 1.144932, accuarcy : 0.743400\n",
            "total step : 39 \n",
            "error : 1.130962, accuarcy : 0.745500\n",
            "total step : 40 \n",
            "error : 1.117366, accuarcy : 0.747600\n",
            "total step : 41 \n",
            "error : 1.104133, accuarcy : 0.749600\n",
            "total step : 42 \n",
            "error : 1.091253, accuarcy : 0.751200\n",
            "total step : 43 \n",
            "error : 1.078713, accuarcy : 0.753800\n",
            "total step : 44 \n",
            "error : 1.066504, accuarcy : 0.756000\n",
            "total step : 45 \n",
            "error : 1.054615, accuarcy : 0.758000\n",
            "total step : 46 \n",
            "error : 1.043035, accuarcy : 0.760400\n",
            "total step : 47 \n",
            "error : 1.031756, accuarcy : 0.761900\n",
            "total step : 48 \n",
            "error : 1.020768, accuarcy : 0.763400\n",
            "total step : 49 \n",
            "error : 1.010062, accuarcy : 0.764800\n",
            "total step : 50 \n",
            "error : 0.999629, accuarcy : 0.766200\n",
            "total step : 51 \n",
            "error : 0.989460, accuarcy : 0.767000\n",
            "total step : 52 \n",
            "error : 0.979548, accuarcy : 0.768700\n",
            "total step : 53 \n",
            "error : 0.969884, accuarcy : 0.770700\n",
            "total step : 54 \n",
            "error : 0.960461, accuarcy : 0.772800\n",
            "total step : 55 \n",
            "error : 0.951270, accuarcy : 0.774200\n",
            "total step : 56 \n",
            "error : 0.942306, accuarcy : 0.775900\n",
            "total step : 57 \n",
            "error : 0.933560, accuarcy : 0.777200\n",
            "total step : 58 \n",
            "error : 0.925025, accuarcy : 0.778400\n",
            "total step : 59 \n",
            "error : 0.916696, accuarcy : 0.779600\n",
            "total step : 60 \n",
            "error : 0.908566, accuarcy : 0.781000\n",
            "total step : 61 \n",
            "error : 0.900628, accuarcy : 0.782100\n",
            "total step : 62 \n",
            "error : 0.892877, accuarcy : 0.783300\n",
            "total step : 63 \n",
            "error : 0.885306, accuarcy : 0.783700\n",
            "total step : 64 \n",
            "error : 0.877911, accuarcy : 0.784600\n",
            "total step : 65 \n",
            "error : 0.870686, accuarcy : 0.785200\n",
            "total step : 66 \n",
            "error : 0.863625, accuarcy : 0.785800\n",
            "total step : 67 \n",
            "error : 0.856724, accuarcy : 0.786900\n",
            "total step : 68 \n",
            "error : 0.849979, accuarcy : 0.788600\n",
            "total step : 69 \n",
            "error : 0.843383, accuarcy : 0.790000\n",
            "total step : 70 \n",
            "error : 0.836934, accuarcy : 0.791300\n",
            "total step : 71 \n",
            "error : 0.830626, accuarcy : 0.792500\n",
            "total step : 72 \n",
            "error : 0.824456, accuarcy : 0.793900\n",
            "total step : 73 \n",
            "error : 0.818419, accuarcy : 0.795000\n",
            "total step : 74 \n",
            "error : 0.812512, accuarcy : 0.796000\n",
            "total step : 75 \n",
            "error : 0.806730, accuarcy : 0.796800\n",
            "total step : 76 \n",
            "error : 0.801072, accuarcy : 0.797400\n",
            "total step : 77 \n",
            "error : 0.795532, accuarcy : 0.798200\n",
            "total step : 78 \n",
            "error : 0.790107, accuarcy : 0.798900\n",
            "total step : 79 \n",
            "error : 0.784795, accuarcy : 0.799500\n",
            "total step : 80 \n",
            "error : 0.779593, accuarcy : 0.800200\n",
            "total step : 81 \n",
            "error : 0.774496, accuarcy : 0.801800\n",
            "total step : 82 \n",
            "error : 0.769503, accuarcy : 0.802400\n",
            "total step : 83 \n",
            "error : 0.764611, accuarcy : 0.803100\n",
            "total step : 84 \n",
            "error : 0.759816, accuarcy : 0.803700\n",
            "total step : 85 \n",
            "error : 0.755117, accuarcy : 0.805300\n",
            "total step : 86 \n",
            "error : 0.750510, accuarcy : 0.806200\n",
            "total step : 87 \n",
            "error : 0.745993, accuarcy : 0.807700\n",
            "total step : 88 \n",
            "error : 0.741564, accuarcy : 0.808200\n",
            "total step : 89 \n",
            "error : 0.737221, accuarcy : 0.809200\n",
            "total step : 90 \n",
            "error : 0.732961, accuarcy : 0.809800\n",
            "total step : 91 \n",
            "error : 0.728782, accuarcy : 0.810600\n",
            "total step : 92 \n",
            "error : 0.724682, accuarcy : 0.811500\n",
            "total step : 93 \n",
            "error : 0.720659, accuarcy : 0.812400\n",
            "total step : 94 \n",
            "error : 0.716710, accuarcy : 0.813600\n",
            "total step : 95 \n",
            "error : 0.712835, accuarcy : 0.814100\n",
            "total step : 96 \n",
            "error : 0.709031, accuarcy : 0.814600\n",
            "total step : 97 \n",
            "error : 0.705297, accuarcy : 0.815000\n",
            "total step : 98 \n",
            "error : 0.701630, accuarcy : 0.815600\n",
            "total step : 99 \n",
            "error : 0.698029, accuarcy : 0.816600\n",
            "total step : 100 \n",
            "error : 0.694493, accuarcy : 0.817600\n",
            "total step : 101 \n",
            "error : 0.691019, accuarcy : 0.817500\n",
            "total step : 102 \n",
            "error : 0.687607, accuarcy : 0.818500\n",
            "total step : 103 \n",
            "error : 0.684254, accuarcy : 0.819300\n",
            "total step : 104 \n",
            "error : 0.680960, accuarcy : 0.820000\n",
            "total step : 105 \n",
            "error : 0.677723, accuarcy : 0.820700\n",
            "total step : 106 \n",
            "error : 0.674541, accuarcy : 0.821000\n",
            "total step : 107 \n",
            "error : 0.671413, accuarcy : 0.821800\n",
            "total step : 108 \n",
            "error : 0.668339, accuarcy : 0.822100\n",
            "total step : 109 \n",
            "error : 0.665316, accuarcy : 0.822400\n",
            "total step : 110 \n",
            "error : 0.662344, accuarcy : 0.822600\n",
            "total step : 111 \n",
            "error : 0.659420, accuarcy : 0.823200\n",
            "total step : 112 \n",
            "error : 0.656546, accuarcy : 0.824200\n",
            "total step : 113 \n",
            "error : 0.653718, accuarcy : 0.825200\n",
            "total step : 114 \n",
            "error : 0.650936, accuarcy : 0.825600\n",
            "total step : 115 \n",
            "error : 0.648199, accuarcy : 0.826200\n",
            "total step : 116 \n",
            "error : 0.645506, accuarcy : 0.827000\n",
            "total step : 117 \n",
            "error : 0.642856, accuarcy : 0.827800\n",
            "total step : 118 \n",
            "error : 0.640248, accuarcy : 0.828300\n",
            "total step : 119 \n",
            "error : 0.637682, accuarcy : 0.828700\n",
            "total step : 120 \n",
            "error : 0.635155, accuarcy : 0.829400\n",
            "total step : 121 \n",
            "error : 0.632667, accuarcy : 0.830200\n",
            "total step : 122 \n",
            "error : 0.630218, accuarcy : 0.830500\n",
            "total step : 123 \n",
            "error : 0.627807, accuarcy : 0.830700\n",
            "total step : 124 \n",
            "error : 0.625432, accuarcy : 0.831100\n",
            "total step : 125 \n",
            "error : 0.623093, accuarcy : 0.831800\n",
            "total step : 126 \n",
            "error : 0.620790, accuarcy : 0.832000\n",
            "total step : 127 \n",
            "error : 0.618521, accuarcy : 0.832700\n",
            "total step : 128 \n",
            "error : 0.616285, accuarcy : 0.832700\n",
            "total step : 129 \n",
            "error : 0.614082, accuarcy : 0.833100\n",
            "total step : 130 \n",
            "error : 0.611912, accuarcy : 0.833600\n",
            "total step : 131 \n",
            "error : 0.609773, accuarcy : 0.833800\n",
            "total step : 132 \n",
            "error : 0.607666, accuarcy : 0.833600\n",
            "total step : 133 \n",
            "error : 0.605588, accuarcy : 0.834400\n",
            "total step : 134 \n",
            "error : 0.603540, accuarcy : 0.835000\n",
            "total step : 135 \n",
            "error : 0.601521, accuarcy : 0.835300\n",
            "total step : 136 \n",
            "error : 0.599531, accuarcy : 0.835600\n",
            "total step : 137 \n",
            "error : 0.597569, accuarcy : 0.836000\n",
            "total step : 138 \n",
            "error : 0.595633, accuarcy : 0.836100\n",
            "total step : 139 \n",
            "error : 0.593725, accuarcy : 0.836300\n",
            "total step : 140 \n",
            "error : 0.591843, accuarcy : 0.836500\n",
            "total step : 141 \n",
            "error : 0.589986, accuarcy : 0.837500\n",
            "total step : 142 \n",
            "error : 0.588155, accuarcy : 0.837700\n",
            "total step : 143 \n",
            "error : 0.586348, accuarcy : 0.838000\n",
            "total step : 144 \n",
            "error : 0.584566, accuarcy : 0.838300\n",
            "total step : 145 \n",
            "error : 0.582807, accuarcy : 0.838800\n",
            "total step : 146 \n",
            "error : 0.581071, accuarcy : 0.839800\n",
            "total step : 147 \n",
            "error : 0.579358, accuarcy : 0.840000\n",
            "total step : 148 \n",
            "error : 0.577668, accuarcy : 0.840100\n",
            "total step : 149 \n",
            "error : 0.575999, accuarcy : 0.840400\n",
            "total step : 150 \n",
            "error : 0.574352, accuarcy : 0.840900\n",
            "total step : 151 \n",
            "error : 0.572726, accuarcy : 0.841200\n",
            "total step : 152 \n",
            "error : 0.571121, accuarcy : 0.841700\n",
            "total step : 153 \n",
            "error : 0.569536, accuarcy : 0.841800\n",
            "total step : 154 \n",
            "error : 0.567971, accuarcy : 0.842200\n",
            "total step : 155 \n",
            "error : 0.566425, accuarcy : 0.842700\n",
            "total step : 156 \n",
            "error : 0.564899, accuarcy : 0.843000\n",
            "total step : 157 \n",
            "error : 0.563392, accuarcy : 0.843000\n",
            "total step : 158 \n",
            "error : 0.561903, accuarcy : 0.843300\n",
            "total step : 159 \n",
            "error : 0.560432, accuarcy : 0.843900\n",
            "total step : 160 \n",
            "error : 0.558979, accuarcy : 0.843900\n",
            "total step : 161 \n",
            "error : 0.557543, accuarcy : 0.844200\n",
            "total step : 162 \n",
            "error : 0.556124, accuarcy : 0.844300\n",
            "total step : 163 \n",
            "error : 0.554723, accuarcy : 0.844300\n",
            "total step : 164 \n",
            "error : 0.553338, accuarcy : 0.844600\n",
            "total step : 165 \n",
            "error : 0.551969, accuarcy : 0.845000\n",
            "total step : 166 \n",
            "error : 0.550616, accuarcy : 0.845800\n",
            "total step : 167 \n",
            "error : 0.549279, accuarcy : 0.846100\n",
            "total step : 168 \n",
            "error : 0.547957, accuarcy : 0.846100\n",
            "total step : 169 \n",
            "error : 0.546650, accuarcy : 0.846200\n",
            "total step : 170 \n",
            "error : 0.545358, accuarcy : 0.846200\n",
            "total step : 171 \n",
            "error : 0.544081, accuarcy : 0.846400\n",
            "total step : 172 \n",
            "error : 0.542818, accuarcy : 0.846800\n",
            "total step : 173 \n",
            "error : 0.541569, accuarcy : 0.847200\n",
            "total step : 174 \n",
            "error : 0.540334, accuarcy : 0.847600\n",
            "total step : 175 \n",
            "error : 0.539112, accuarcy : 0.847700\n",
            "total step : 176 \n",
            "error : 0.537904, accuarcy : 0.848200\n",
            "total step : 177 \n",
            "error : 0.536709, accuarcy : 0.848700\n",
            "total step : 178 \n",
            "error : 0.535527, accuarcy : 0.848800\n",
            "total step : 179 \n",
            "error : 0.534358, accuarcy : 0.849000\n",
            "total step : 180 \n",
            "error : 0.533201, accuarcy : 0.849500\n",
            "total step : 181 \n",
            "error : 0.532056, accuarcy : 0.849600\n",
            "total step : 182 \n",
            "error : 0.530923, accuarcy : 0.850300\n"
          ]
        }
      ],
      "source": [
        "train_2X, train_2y = make_sample(idx = 2) # idx = target number\n",
        "train_2X = np.insert(train_2X, 0, 1, axis=1) # bias 추가\n",
        "w2,J2_history, ACC2_history  = train(train_2X, train_2y,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total step : 1 \n",
            "error : 3.687951, accuarcy : 0.466900\n",
            "total step : 2 \n",
            "error : 3.529909, accuarcy : 0.455100\n",
            "total step : 3 \n",
            "error : 3.389804, accuarcy : 0.448500\n",
            "total step : 4 \n",
            "error : 3.263298, accuarcy : 0.446700\n",
            "total step : 5 \n",
            "error : 3.147662, accuarcy : 0.446600\n",
            "total step : 6 \n",
            "error : 3.040900, accuarcy : 0.448000\n",
            "total step : 7 \n",
            "error : 2.941471, accuarcy : 0.450200\n",
            "total step : 8 \n",
            "error : 2.848236, accuarcy : 0.454100\n",
            "total step : 9 \n",
            "error : 2.760389, accuarcy : 0.457800\n",
            "total step : 10 \n",
            "error : 2.677365, accuarcy : 0.463400\n",
            "total step : 11 \n",
            "error : 2.598753, accuarcy : 0.467800\n",
            "total step : 12 \n",
            "error : 2.524230, accuarcy : 0.474000\n",
            "total step : 13 \n",
            "error : 2.453524, accuarcy : 0.479800\n",
            "total step : 14 \n",
            "error : 2.386391, accuarcy : 0.484600\n",
            "total step : 15 \n",
            "error : 2.322606, accuarcy : 0.490100\n",
            "total step : 16 \n",
            "error : 2.261956, accuarcy : 0.498100\n",
            "total step : 17 \n",
            "error : 2.204243, accuarcy : 0.504900\n",
            "total step : 18 \n",
            "error : 2.149278, accuarcy : 0.511500\n",
            "total step : 19 \n",
            "error : 2.096887, accuarcy : 0.517900\n",
            "total step : 20 \n",
            "error : 2.046909, accuarcy : 0.524100\n",
            "total step : 21 \n",
            "error : 1.999195, accuarcy : 0.531500\n",
            "total step : 22 \n",
            "error : 1.953609, accuarcy : 0.537300\n",
            "total step : 23 \n",
            "error : 1.910026, accuarcy : 0.543800\n",
            "total step : 24 \n",
            "error : 1.868334, accuarcy : 0.551100\n",
            "total step : 25 \n",
            "error : 1.828428, accuarcy : 0.558700\n",
            "total step : 26 \n",
            "error : 1.790215, accuarcy : 0.563900\n",
            "total step : 27 \n",
            "error : 1.753609, accuarcy : 0.568800\n",
            "total step : 28 \n",
            "error : 1.718532, accuarcy : 0.573400\n",
            "total step : 29 \n",
            "error : 1.684913, accuarcy : 0.581200\n",
            "total step : 30 \n",
            "error : 1.652684, accuarcy : 0.587300\n",
            "total step : 31 \n",
            "error : 1.621783, accuarcy : 0.593300\n",
            "total step : 32 \n",
            "error : 1.592152, accuarcy : 0.598700\n",
            "total step : 33 \n",
            "error : 1.563735, accuarcy : 0.603200\n",
            "total step : 34 \n",
            "error : 1.536478, accuarcy : 0.609200\n",
            "total step : 35 \n",
            "error : 1.510330, accuarcy : 0.615400\n",
            "total step : 36 \n",
            "error : 1.485242, accuarcy : 0.620300\n",
            "total step : 37 \n",
            "error : 1.461166, accuarcy : 0.625400\n",
            "total step : 38 \n",
            "error : 1.438057, accuarcy : 0.630600\n",
            "total step : 39 \n",
            "error : 1.415871, accuarcy : 0.635300\n",
            "total step : 40 \n",
            "error : 1.394566, accuarcy : 0.639800\n",
            "total step : 41 \n",
            "error : 1.374102, accuarcy : 0.644500\n",
            "total step : 42 \n",
            "error : 1.354439, accuarcy : 0.649000\n",
            "total step : 43 \n",
            "error : 1.335541, accuarcy : 0.651900\n",
            "total step : 44 \n",
            "error : 1.317372, accuarcy : 0.655800\n",
            "total step : 45 \n",
            "error : 1.299896, accuarcy : 0.658900\n",
            "total step : 46 \n",
            "error : 1.283082, accuarcy : 0.662700\n",
            "total step : 47 \n",
            "error : 1.266897, accuarcy : 0.665500\n",
            "total step : 48 \n",
            "error : 1.251311, accuarcy : 0.668900\n",
            "total step : 49 \n",
            "error : 1.236294, accuarcy : 0.671500\n",
            "total step : 50 \n",
            "error : 1.221819, accuarcy : 0.674900\n",
            "total step : 51 \n",
            "error : 1.207859, accuarcy : 0.677800\n",
            "total step : 52 \n",
            "error : 1.194388, accuarcy : 0.681200\n",
            "total step : 53 \n",
            "error : 1.181381, accuarcy : 0.684600\n",
            "total step : 54 \n",
            "error : 1.168816, accuarcy : 0.687500\n",
            "total step : 55 \n",
            "error : 1.156669, accuarcy : 0.691400\n",
            "total step : 56 \n",
            "error : 1.144920, accuarcy : 0.693500\n",
            "total step : 57 \n",
            "error : 1.133549, accuarcy : 0.696400\n",
            "total step : 58 \n",
            "error : 1.122538, accuarcy : 0.699400\n",
            "total step : 59 \n",
            "error : 1.111868, accuarcy : 0.702400\n",
            "total step : 60 \n",
            "error : 1.101523, accuarcy : 0.705600\n",
            "total step : 61 \n",
            "error : 1.091488, accuarcy : 0.707600\n",
            "total step : 62 \n",
            "error : 1.081748, accuarcy : 0.709800\n",
            "total step : 63 \n",
            "error : 1.072289, accuarcy : 0.712200\n",
            "total step : 64 \n",
            "error : 1.063098, accuarcy : 0.714400\n",
            "total step : 65 \n",
            "error : 1.054163, accuarcy : 0.717700\n",
            "total step : 66 \n",
            "error : 1.045473, accuarcy : 0.719700\n",
            "total step : 67 \n",
            "error : 1.037017, accuarcy : 0.721700\n",
            "total step : 68 \n",
            "error : 1.028785, accuarcy : 0.724300\n",
            "total step : 69 \n",
            "error : 1.020768, accuarcy : 0.726400\n",
            "total step : 70 \n",
            "error : 1.012956, accuarcy : 0.728700\n",
            "total step : 71 \n",
            "error : 1.005342, accuarcy : 0.730300\n",
            "total step : 72 \n",
            "error : 0.997916, accuarcy : 0.731800\n",
            "total step : 73 \n",
            "error : 0.990673, accuarcy : 0.733600\n",
            "total step : 74 \n",
            "error : 0.983604, accuarcy : 0.734600\n",
            "total step : 75 \n",
            "error : 0.976704, accuarcy : 0.736500\n",
            "total step : 76 \n",
            "error : 0.969966, accuarcy : 0.738300\n",
            "total step : 77 \n",
            "error : 0.963383, accuarcy : 0.739400\n",
            "total step : 78 \n",
            "error : 0.956951, accuarcy : 0.741000\n",
            "total step : 79 \n",
            "error : 0.950664, accuarcy : 0.742800\n",
            "total step : 80 \n",
            "error : 0.944517, accuarcy : 0.744200\n",
            "total step : 81 \n",
            "error : 0.938505, accuarcy : 0.746200\n",
            "total step : 82 \n",
            "error : 0.932623, accuarcy : 0.747600\n",
            "total step : 83 \n",
            "error : 0.926868, accuarcy : 0.749200\n",
            "total step : 84 \n",
            "error : 0.921234, accuarcy : 0.750600\n",
            "total step : 85 \n",
            "error : 0.915719, accuarcy : 0.751400\n",
            "total step : 86 \n",
            "error : 0.910318, accuarcy : 0.752600\n",
            "total step : 87 \n",
            "error : 0.905028, accuarcy : 0.753900\n",
            "total step : 88 \n",
            "error : 0.899845, accuarcy : 0.754800\n",
            "total step : 89 \n",
            "error : 0.894766, accuarcy : 0.756500\n",
            "total step : 90 \n",
            "error : 0.889788, accuarcy : 0.757500\n",
            "total step : 91 \n",
            "error : 0.884907, accuarcy : 0.758300\n",
            "total step : 92 \n",
            "error : 0.880121, accuarcy : 0.759700\n",
            "total step : 93 \n",
            "error : 0.875427, accuarcy : 0.761000\n",
            "total step : 94 \n",
            "error : 0.870822, accuarcy : 0.762200\n",
            "total step : 95 \n",
            "error : 0.866304, accuarcy : 0.763600\n",
            "total step : 96 \n",
            "error : 0.861870, accuarcy : 0.764900\n",
            "total step : 97 \n",
            "error : 0.857518, accuarcy : 0.765600\n",
            "total step : 98 \n",
            "error : 0.853245, accuarcy : 0.766800\n",
            "total step : 99 \n",
            "error : 0.849049, accuarcy : 0.768100\n",
            "total step : 100 \n",
            "error : 0.844927, accuarcy : 0.769100\n",
            "total step : 101 \n",
            "error : 0.840879, accuarcy : 0.769700\n",
            "total step : 102 \n",
            "error : 0.836901, accuarcy : 0.770700\n",
            "total step : 103 \n",
            "error : 0.832993, accuarcy : 0.772300\n",
            "total step : 104 \n",
            "error : 0.829151, accuarcy : 0.772900\n",
            "total step : 105 \n",
            "error : 0.825374, accuarcy : 0.773700\n",
            "total step : 106 \n",
            "error : 0.821660, accuarcy : 0.775000\n",
            "total step : 107 \n",
            "error : 0.818009, accuarcy : 0.775600\n",
            "total step : 108 \n",
            "error : 0.814417, accuarcy : 0.777000\n",
            "total step : 109 \n",
            "error : 0.810883, accuarcy : 0.777900\n",
            "total step : 110 \n",
            "error : 0.807407, accuarcy : 0.778100\n",
            "total step : 111 \n",
            "error : 0.803986, accuarcy : 0.779200\n",
            "total step : 112 \n",
            "error : 0.800619, accuarcy : 0.779700\n",
            "total step : 113 \n",
            "error : 0.797304, accuarcy : 0.780900\n",
            "total step : 114 \n",
            "error : 0.794041, accuarcy : 0.781700\n",
            "total step : 115 \n",
            "error : 0.790827, accuarcy : 0.782300\n",
            "total step : 116 \n",
            "error : 0.787663, accuarcy : 0.783100\n",
            "total step : 117 \n",
            "error : 0.784546, accuarcy : 0.783900\n",
            "total step : 118 \n",
            "error : 0.781475, accuarcy : 0.784700\n",
            "total step : 119 \n",
            "error : 0.778449, accuarcy : 0.785500\n",
            "total step : 120 \n",
            "error : 0.775467, accuarcy : 0.786000\n",
            "total step : 121 \n",
            "error : 0.772529, accuarcy : 0.786600\n",
            "total step : 122 \n",
            "error : 0.769632, accuarcy : 0.787400\n",
            "total step : 123 \n",
            "error : 0.766776, accuarcy : 0.788400\n",
            "total step : 124 \n",
            "error : 0.763960, accuarcy : 0.788800\n",
            "total step : 125 \n",
            "error : 0.761183, accuarcy : 0.789400\n",
            "total step : 126 \n",
            "error : 0.758444, accuarcy : 0.790200\n",
            "total step : 127 \n",
            "error : 0.755742, accuarcy : 0.791600\n",
            "total step : 128 \n",
            "error : 0.753076, accuarcy : 0.792000\n",
            "total step : 129 \n",
            "error : 0.750446, accuarcy : 0.792700\n",
            "total step : 130 \n",
            "error : 0.747850, accuarcy : 0.793200\n",
            "total step : 131 \n",
            "error : 0.745288, accuarcy : 0.794000\n",
            "total step : 132 \n",
            "error : 0.742759, accuarcy : 0.794300\n",
            "total step : 133 \n",
            "error : 0.740262, accuarcy : 0.795100\n",
            "total step : 134 \n",
            "error : 0.737797, accuarcy : 0.795600\n",
            "total step : 135 \n",
            "error : 0.735362, accuarcy : 0.795900\n",
            "total step : 136 \n",
            "error : 0.732957, accuarcy : 0.796200\n",
            "total step : 137 \n",
            "error : 0.730582, accuarcy : 0.796700\n",
            "total step : 138 \n",
            "error : 0.728235, accuarcy : 0.797200\n",
            "total step : 139 \n",
            "error : 0.725917, accuarcy : 0.798000\n",
            "total step : 140 \n",
            "error : 0.723626, accuarcy : 0.798400\n",
            "total step : 141 \n",
            "error : 0.721361, accuarcy : 0.798900\n",
            "total step : 142 \n",
            "error : 0.719123, accuarcy : 0.799300\n",
            "total step : 143 \n",
            "error : 0.716910, accuarcy : 0.800000\n",
            "total step : 144 \n",
            "error : 0.714723, accuarcy : 0.801200\n",
            "total step : 145 \n",
            "error : 0.712560, accuarcy : 0.801600\n",
            "total step : 146 \n",
            "error : 0.710420, accuarcy : 0.802000\n",
            "total step : 147 \n",
            "error : 0.708305, accuarcy : 0.802900\n",
            "total step : 148 \n",
            "error : 0.706212, accuarcy : 0.803600\n",
            "total step : 149 \n",
            "error : 0.704142, accuarcy : 0.804200\n",
            "total step : 150 \n",
            "error : 0.702094, accuarcy : 0.804600\n",
            "total step : 151 \n",
            "error : 0.700068, accuarcy : 0.804800\n",
            "total step : 152 \n",
            "error : 0.698062, accuarcy : 0.805200\n",
            "total step : 153 \n",
            "error : 0.696078, accuarcy : 0.805800\n",
            "total step : 154 \n",
            "error : 0.694113, accuarcy : 0.806400\n",
            "total step : 155 \n",
            "error : 0.692169, accuarcy : 0.806800\n",
            "total step : 156 \n",
            "error : 0.690244, accuarcy : 0.807200\n",
            "total step : 157 \n",
            "error : 0.688338, accuarcy : 0.808200\n",
            "total step : 158 \n",
            "error : 0.686451, accuarcy : 0.807900\n",
            "total step : 159 \n",
            "error : 0.684582, accuarcy : 0.808500\n",
            "total step : 160 \n",
            "error : 0.682732, accuarcy : 0.808800\n",
            "total step : 161 \n",
            "error : 0.680899, accuarcy : 0.809100\n",
            "total step : 162 \n",
            "error : 0.679083, accuarcy : 0.809400\n",
            "total step : 163 \n",
            "error : 0.677285, accuarcy : 0.809300\n",
            "total step : 164 \n",
            "error : 0.675503, accuarcy : 0.809800\n",
            "total step : 165 \n",
            "error : 0.673738, accuarcy : 0.810200\n",
            "total step : 166 \n",
            "error : 0.671989, accuarcy : 0.810600\n",
            "total step : 167 \n",
            "error : 0.670255, accuarcy : 0.810900\n",
            "total step : 168 \n",
            "error : 0.668538, accuarcy : 0.811400\n",
            "total step : 169 \n",
            "error : 0.666835, accuarcy : 0.811900\n",
            "total step : 170 \n",
            "error : 0.665148, accuarcy : 0.812400\n",
            "total step : 171 \n",
            "error : 0.663475, accuarcy : 0.813200\n",
            "total step : 172 \n",
            "error : 0.661817, accuarcy : 0.813800\n",
            "total step : 173 \n",
            "error : 0.660174, accuarcy : 0.814200\n",
            "total step : 174 \n",
            "error : 0.658544, accuarcy : 0.814700\n",
            "total step : 175 \n",
            "error : 0.656928, accuarcy : 0.814900\n",
            "total step : 176 \n",
            "error : 0.655326, accuarcy : 0.815100\n",
            "total step : 177 \n",
            "error : 0.653737, accuarcy : 0.815700\n",
            "total step : 178 \n",
            "error : 0.652162, accuarcy : 0.816000\n",
            "total step : 179 \n",
            "error : 0.650599, accuarcy : 0.816200\n",
            "total step : 180 \n",
            "error : 0.649049, accuarcy : 0.817000\n",
            "total step : 181 \n",
            "error : 0.647512, accuarcy : 0.817300\n",
            "total step : 182 \n",
            "error : 0.645987, accuarcy : 0.818100\n",
            "total step : 183 \n",
            "error : 0.644474, accuarcy : 0.818100\n",
            "total step : 184 \n",
            "error : 0.642973, accuarcy : 0.818200\n",
            "total step : 185 \n",
            "error : 0.641485, accuarcy : 0.818700\n",
            "total step : 186 \n",
            "error : 0.640007, accuarcy : 0.818900\n",
            "total step : 187 \n",
            "error : 0.638542, accuarcy : 0.819200\n",
            "total step : 188 \n",
            "error : 0.637087, accuarcy : 0.819600\n",
            "total step : 189 \n",
            "error : 0.635644, accuarcy : 0.820000\n",
            "total step : 190 \n",
            "error : 0.634212, accuarcy : 0.820600\n",
            "total step : 191 \n",
            "error : 0.632791, accuarcy : 0.821000\n",
            "total step : 192 \n",
            "error : 0.631380, accuarcy : 0.821500\n",
            "total step : 193 \n",
            "error : 0.629980, accuarcy : 0.821700\n",
            "total step : 194 \n",
            "error : 0.628591, accuarcy : 0.822500\n",
            "total step : 195 \n",
            "error : 0.627211, accuarcy : 0.822800\n",
            "total step : 196 \n",
            "error : 0.625842, accuarcy : 0.822900\n",
            "total step : 197 \n",
            "error : 0.624483, accuarcy : 0.823400\n",
            "total step : 198 \n",
            "error : 0.623134, accuarcy : 0.823800\n",
            "total step : 199 \n",
            "error : 0.621794, accuarcy : 0.824400\n",
            "total step : 200 \n",
            "error : 0.620464, accuarcy : 0.824700\n",
            "total step : 201 \n",
            "error : 0.619144, accuarcy : 0.824700\n",
            "total step : 202 \n",
            "error : 0.617833, accuarcy : 0.824800\n",
            "total step : 203 \n",
            "error : 0.616531, accuarcy : 0.824800\n",
            "total step : 204 \n",
            "error : 0.615239, accuarcy : 0.825300\n",
            "total step : 205 \n",
            "error : 0.613955, accuarcy : 0.825600\n",
            "total step : 206 \n",
            "error : 0.612680, accuarcy : 0.825800\n",
            "total step : 207 \n",
            "error : 0.611415, accuarcy : 0.825900\n",
            "total step : 208 \n",
            "error : 0.610158, accuarcy : 0.826100\n",
            "total step : 209 \n",
            "error : 0.608909, accuarcy : 0.826200\n",
            "total step : 210 \n",
            "error : 0.607669, accuarcy : 0.826500\n",
            "total step : 211 \n",
            "error : 0.606438, accuarcy : 0.826800\n",
            "total step : 212 \n",
            "error : 0.605214, accuarcy : 0.827100\n",
            "total step : 213 \n",
            "error : 0.603999, accuarcy : 0.827700\n",
            "total step : 214 \n",
            "error : 0.602792, accuarcy : 0.827700\n",
            "total step : 215 \n",
            "error : 0.601593, accuarcy : 0.827800\n",
            "total step : 216 \n",
            "error : 0.600402, accuarcy : 0.827800\n",
            "total step : 217 \n",
            "error : 0.599219, accuarcy : 0.827800\n",
            "total step : 218 \n",
            "error : 0.598044, accuarcy : 0.828100\n",
            "total step : 219 \n",
            "error : 0.596876, accuarcy : 0.828600\n",
            "total step : 220 \n",
            "error : 0.595715, accuarcy : 0.829000\n",
            "total step : 221 \n",
            "error : 0.594563, accuarcy : 0.829200\n",
            "total step : 222 \n",
            "error : 0.593417, accuarcy : 0.829400\n",
            "total step : 223 \n",
            "error : 0.592279, accuarcy : 0.829600\n",
            "total step : 224 \n",
            "error : 0.591148, accuarcy : 0.830000\n",
            "total step : 225 \n",
            "error : 0.590025, accuarcy : 0.830200\n",
            "total step : 226 \n",
            "error : 0.588908, accuarcy : 0.830200\n",
            "total step : 227 \n",
            "error : 0.587798, accuarcy : 0.830400\n",
            "total step : 228 \n",
            "error : 0.586696, accuarcy : 0.830400\n",
            "total step : 229 \n",
            "error : 0.585600, accuarcy : 0.830500\n",
            "total step : 230 \n",
            "error : 0.584511, accuarcy : 0.830700\n",
            "total step : 231 \n",
            "error : 0.583428, accuarcy : 0.830700\n",
            "total step : 232 \n",
            "error : 0.582353, accuarcy : 0.831000\n",
            "total step : 233 \n",
            "error : 0.581283, accuarcy : 0.831000\n",
            "total step : 234 \n",
            "error : 0.580221, accuarcy : 0.831300\n",
            "total step : 235 \n",
            "error : 0.579164, accuarcy : 0.831500\n",
            "total step : 236 \n",
            "error : 0.578114, accuarcy : 0.831900\n",
            "total step : 237 \n",
            "error : 0.577071, accuarcy : 0.831800\n",
            "total step : 238 \n",
            "error : 0.576033, accuarcy : 0.832000\n",
            "total step : 239 \n",
            "error : 0.575002, accuarcy : 0.832100\n",
            "total step : 240 \n",
            "error : 0.573977, accuarcy : 0.832500\n",
            "total step : 241 \n",
            "error : 0.572958, accuarcy : 0.832600\n",
            "total step : 242 \n",
            "error : 0.571945, accuarcy : 0.832800\n",
            "total step : 243 \n",
            "error : 0.570937, accuarcy : 0.832800\n",
            "total step : 244 \n",
            "error : 0.569936, accuarcy : 0.833000\n",
            "total step : 245 \n",
            "error : 0.568940, accuarcy : 0.833100\n",
            "total step : 246 \n",
            "error : 0.567950, accuarcy : 0.833200\n",
            "total step : 247 \n",
            "error : 0.566966, accuarcy : 0.833700\n",
            "total step : 248 \n",
            "error : 0.565988, accuarcy : 0.834100\n",
            "total step : 249 \n",
            "error : 0.565015, accuarcy : 0.834100\n",
            "total step : 250 \n",
            "error : 0.564047, accuarcy : 0.834000\n",
            "total step : 251 \n",
            "error : 0.563085, accuarcy : 0.834200\n",
            "total step : 252 \n",
            "error : 0.562128, accuarcy : 0.834500\n",
            "total step : 253 \n",
            "error : 0.561177, accuarcy : 0.834600\n",
            "total step : 254 \n",
            "error : 0.560231, accuarcy : 0.834700\n",
            "total step : 255 \n",
            "error : 0.559290, accuarcy : 0.834700\n",
            "total step : 256 \n",
            "error : 0.558355, accuarcy : 0.834900\n",
            "total step : 257 \n",
            "error : 0.557424, accuarcy : 0.835100\n",
            "total step : 258 \n",
            "error : 0.556499, accuarcy : 0.835200\n",
            "total step : 259 \n",
            "error : 0.555578, accuarcy : 0.835200\n",
            "total step : 260 \n",
            "error : 0.554663, accuarcy : 0.835400\n",
            "total step : 261 \n",
            "error : 0.553753, accuarcy : 0.835600\n",
            "total step : 262 \n",
            "error : 0.552847, accuarcy : 0.835700\n",
            "total step : 263 \n",
            "error : 0.551947, accuarcy : 0.835800\n",
            "total step : 264 \n",
            "error : 0.551051, accuarcy : 0.836200\n",
            "total step : 265 \n",
            "error : 0.550160, accuarcy : 0.836300\n",
            "total step : 266 \n",
            "error : 0.549274, accuarcy : 0.836600\n",
            "total step : 267 \n",
            "error : 0.548392, accuarcy : 0.836800\n",
            "total step : 268 \n",
            "error : 0.547515, accuarcy : 0.837000\n",
            "total step : 269 \n",
            "error : 0.546643, accuarcy : 0.837300\n",
            "total step : 270 \n",
            "error : 0.545775, accuarcy : 0.837400\n",
            "total step : 271 \n",
            "error : 0.544912, accuarcy : 0.837500\n",
            "total step : 272 \n",
            "error : 0.544053, accuarcy : 0.837700\n",
            "total step : 273 \n",
            "error : 0.543198, accuarcy : 0.837800\n",
            "total step : 274 \n",
            "error : 0.542348, accuarcy : 0.837800\n",
            "total step : 275 \n",
            "error : 0.541502, accuarcy : 0.837800\n",
            "total step : 276 \n",
            "error : 0.540661, accuarcy : 0.838200\n",
            "total step : 277 \n",
            "error : 0.539824, accuarcy : 0.838200\n",
            "total step : 278 \n",
            "error : 0.538991, accuarcy : 0.838300\n",
            "total step : 279 \n",
            "error : 0.538162, accuarcy : 0.838500\n",
            "total step : 280 \n",
            "error : 0.537338, accuarcy : 0.838800\n",
            "total step : 281 \n",
            "error : 0.536517, accuarcy : 0.838800\n",
            "total step : 282 \n",
            "error : 0.535701, accuarcy : 0.838800\n",
            "total step : 283 \n",
            "error : 0.534889, accuarcy : 0.838700\n",
            "total step : 284 \n",
            "error : 0.534080, accuarcy : 0.838900\n",
            "total step : 285 \n",
            "error : 0.533276, accuarcy : 0.838900\n",
            "total step : 286 \n",
            "error : 0.532476, accuarcy : 0.839000\n",
            "total step : 287 \n",
            "error : 0.531679, accuarcy : 0.839100\n",
            "total step : 288 \n",
            "error : 0.530887, accuarcy : 0.839200\n",
            "total step : 289 \n",
            "error : 0.530098, accuarcy : 0.839200\n",
            "total step : 290 \n",
            "error : 0.529313, accuarcy : 0.839200\n",
            "total step : 291 \n",
            "error : 0.528532, accuarcy : 0.839600\n",
            "total step : 292 \n",
            "error : 0.527754, accuarcy : 0.839800\n",
            "total step : 293 \n",
            "error : 0.526981, accuarcy : 0.839900\n",
            "total step : 294 \n",
            "error : 0.526210, accuarcy : 0.840000\n",
            "total step : 295 \n",
            "error : 0.525444, accuarcy : 0.840200\n",
            "total step : 296 \n",
            "error : 0.524681, accuarcy : 0.840400\n",
            "total step : 297 \n",
            "error : 0.523922, accuarcy : 0.840600\n",
            "total step : 298 \n",
            "error : 0.523166, accuarcy : 0.841000\n",
            "total step : 299 \n",
            "error : 0.522414, accuarcy : 0.840900\n",
            "total step : 300 \n",
            "error : 0.521665, accuarcy : 0.841000\n",
            "total step : 301 \n",
            "error : 0.520920, accuarcy : 0.841100\n",
            "total step : 302 \n",
            "error : 0.520178, accuarcy : 0.841400\n",
            "total step : 303 \n",
            "error : 0.519440, accuarcy : 0.841600\n",
            "total step : 304 \n",
            "error : 0.518705, accuarcy : 0.841800\n",
            "total step : 305 \n",
            "error : 0.517973, accuarcy : 0.841800\n",
            "total step : 306 \n",
            "error : 0.517245, accuarcy : 0.842000\n",
            "total step : 307 \n",
            "error : 0.516520, accuarcy : 0.841900\n",
            "total step : 308 \n",
            "error : 0.515798, accuarcy : 0.842100\n",
            "total step : 309 \n",
            "error : 0.515080, accuarcy : 0.842300\n",
            "total step : 310 \n",
            "error : 0.514364, accuarcy : 0.842400\n",
            "total step : 311 \n",
            "error : 0.513652, accuarcy : 0.842700\n",
            "total step : 312 \n",
            "error : 0.512943, accuarcy : 0.842600\n",
            "total step : 313 \n",
            "error : 0.512237, accuarcy : 0.842700\n",
            "total step : 314 \n",
            "error : 0.511534, accuarcy : 0.842800\n",
            "total step : 315 \n",
            "error : 0.510835, accuarcy : 0.842800\n",
            "total step : 316 \n",
            "error : 0.510138, accuarcy : 0.842800\n",
            "total step : 317 \n",
            "error : 0.509444, accuarcy : 0.842800\n",
            "total step : 318 \n",
            "error : 0.508754, accuarcy : 0.842700\n",
            "total step : 319 \n",
            "error : 0.508066, accuarcy : 0.842800\n",
            "total step : 320 \n",
            "error : 0.507382, accuarcy : 0.842800\n",
            "total step : 321 \n",
            "error : 0.506700, accuarcy : 0.842800\n",
            "total step : 322 \n",
            "error : 0.506021, accuarcy : 0.842700\n",
            "total step : 323 \n",
            "error : 0.505345, accuarcy : 0.842800\n",
            "total step : 324 \n",
            "error : 0.504672, accuarcy : 0.842900\n",
            "total step : 325 \n",
            "error : 0.504002, accuarcy : 0.842900\n",
            "total step : 326 \n",
            "error : 0.503335, accuarcy : 0.843200\n",
            "total step : 327 \n",
            "error : 0.502670, accuarcy : 0.843600\n",
            "total step : 328 \n",
            "error : 0.502008, accuarcy : 0.843700\n",
            "total step : 329 \n",
            "error : 0.501349, accuarcy : 0.843800\n",
            "total step : 330 \n",
            "error : 0.500693, accuarcy : 0.844000\n",
            "total step : 331 \n",
            "error : 0.500039, accuarcy : 0.844100\n",
            "total step : 332 \n",
            "error : 0.499388, accuarcy : 0.844200\n",
            "total step : 333 \n",
            "error : 0.498740, accuarcy : 0.844300\n",
            "total step : 334 \n",
            "error : 0.498095, accuarcy : 0.844300\n",
            "total step : 335 \n",
            "error : 0.497452, accuarcy : 0.844600\n",
            "total step : 336 \n",
            "error : 0.496812, accuarcy : 0.844700\n",
            "total step : 337 \n",
            "error : 0.496174, accuarcy : 0.844700\n",
            "total step : 338 \n",
            "error : 0.495539, accuarcy : 0.844600\n",
            "total step : 339 \n",
            "error : 0.494906, accuarcy : 0.844800\n",
            "total step : 340 \n",
            "error : 0.494276, accuarcy : 0.844900\n",
            "total step : 341 \n",
            "error : 0.493649, accuarcy : 0.845100\n",
            "total step : 342 \n",
            "error : 0.493024, accuarcy : 0.845500\n",
            "total step : 343 \n",
            "error : 0.492401, accuarcy : 0.845600\n",
            "total step : 344 \n",
            "error : 0.491781, accuarcy : 0.845600\n",
            "total step : 345 \n",
            "error : 0.491164, accuarcy : 0.845900\n",
            "total step : 346 \n",
            "error : 0.490548, accuarcy : 0.845900\n",
            "total step : 347 \n",
            "error : 0.489936, accuarcy : 0.845900\n",
            "total step : 348 \n",
            "error : 0.489325, accuarcy : 0.845900\n",
            "total step : 349 \n",
            "error : 0.488717, accuarcy : 0.846100\n",
            "total step : 350 \n",
            "error : 0.488112, accuarcy : 0.846300\n",
            "total step : 351 \n",
            "error : 0.487509, accuarcy : 0.846500\n",
            "total step : 352 \n",
            "error : 0.486908, accuarcy : 0.846700\n",
            "total step : 353 \n",
            "error : 0.486309, accuarcy : 0.846800\n",
            "total step : 354 \n",
            "error : 0.485713, accuarcy : 0.846800\n",
            "total step : 355 \n",
            "error : 0.485119, accuarcy : 0.846800\n",
            "total step : 356 \n",
            "error : 0.484527, accuarcy : 0.847000\n",
            "total step : 357 \n",
            "error : 0.483938, accuarcy : 0.847300\n",
            "total step : 358 \n",
            "error : 0.483350, accuarcy : 0.847200\n",
            "total step : 359 \n",
            "error : 0.482765, accuarcy : 0.847400\n",
            "total step : 360 \n",
            "error : 0.482182, accuarcy : 0.847500\n",
            "total step : 361 \n",
            "error : 0.481602, accuarcy : 0.847700\n",
            "total step : 362 \n",
            "error : 0.481023, accuarcy : 0.847600\n",
            "total step : 363 \n",
            "error : 0.480447, accuarcy : 0.847700\n",
            "total step : 364 \n",
            "error : 0.479873, accuarcy : 0.847700\n",
            "total step : 365 \n",
            "error : 0.479301, accuarcy : 0.847800\n",
            "total step : 366 \n",
            "error : 0.478731, accuarcy : 0.848200\n",
            "total step : 367 \n",
            "error : 0.478163, accuarcy : 0.848400\n",
            "total step : 368 \n",
            "error : 0.477598, accuarcy : 0.848500\n",
            "total step : 369 \n",
            "error : 0.477034, accuarcy : 0.848500\n",
            "total step : 370 \n",
            "error : 0.476473, accuarcy : 0.848600\n",
            "total step : 371 \n",
            "error : 0.475913, accuarcy : 0.848700\n",
            "total step : 372 \n",
            "error : 0.475356, accuarcy : 0.848700\n",
            "total step : 373 \n",
            "error : 0.474800, accuarcy : 0.848600\n",
            "total step : 374 \n",
            "error : 0.474247, accuarcy : 0.848700\n",
            "total step : 375 \n",
            "error : 0.473696, accuarcy : 0.848900\n",
            "total step : 376 \n",
            "error : 0.473146, accuarcy : 0.848900\n",
            "total step : 377 \n",
            "error : 0.472599, accuarcy : 0.849000\n",
            "total step : 378 \n",
            "error : 0.472053, accuarcy : 0.849100\n",
            "total step : 379 \n",
            "error : 0.471510, accuarcy : 0.849400\n",
            "total step : 380 \n",
            "error : 0.470969, accuarcy : 0.849300\n",
            "total step : 381 \n",
            "error : 0.470429, accuarcy : 0.849100\n",
            "total step : 382 \n",
            "error : 0.469891, accuarcy : 0.849300\n",
            "total step : 383 \n",
            "error : 0.469356, accuarcy : 0.849300\n",
            "total step : 384 \n",
            "error : 0.468822, accuarcy : 0.849500\n",
            "total step : 385 \n",
            "error : 0.468290, accuarcy : 0.849500\n",
            "total step : 386 \n",
            "error : 0.467760, accuarcy : 0.849500\n",
            "total step : 387 \n",
            "error : 0.467231, accuarcy : 0.849600\n",
            "total step : 388 \n",
            "error : 0.466705, accuarcy : 0.849700\n",
            "total step : 389 \n",
            "error : 0.466180, accuarcy : 0.849900\n",
            "total step : 390 \n",
            "error : 0.465658, accuarcy : 0.849900\n",
            "total step : 391 \n",
            "error : 0.465137, accuarcy : 0.849900\n",
            "total step : 392 \n",
            "error : 0.464618, accuarcy : 0.850000\n",
            "total step : 393 \n",
            "error : 0.464101, accuarcy : 0.850000\n",
            "total step : 394 \n",
            "error : 0.463585, accuarcy : 0.850100\n"
          ]
        }
      ],
      "source": [
        "train_3X, train_3y = make_sample(idx = 3) # idx = target number\n",
        "train_3X = np.insert(train_3X, 0, 1, axis=1) # bias 추가\n",
        "w3,J3_history, ACC3_history = train(train_3X, train_3y,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total step : 1 \n",
            "error : 2.407625, accuarcy : 0.655100\n",
            "total step : 2 \n",
            "error : 2.273792, accuarcy : 0.641000\n",
            "total step : 3 \n",
            "error : 2.171872, accuarcy : 0.632000\n",
            "total step : 4 \n",
            "error : 2.091235, accuarcy : 0.623100\n",
            "total step : 5 \n",
            "error : 2.024788, accuarcy : 0.616900\n",
            "total step : 6 \n",
            "error : 1.967945, accuarcy : 0.611300\n",
            "total step : 7 \n",
            "error : 1.917744, accuarcy : 0.607600\n",
            "total step : 8 \n",
            "error : 1.872270, accuarcy : 0.605900\n",
            "total step : 9 \n",
            "error : 1.830281, accuarcy : 0.605200\n",
            "total step : 10 \n",
            "error : 1.790968, accuarcy : 0.605700\n",
            "total step : 11 \n",
            "error : 1.753796, accuarcy : 0.606700\n",
            "total step : 12 \n",
            "error : 1.718411, accuarcy : 0.609300\n",
            "total step : 13 \n",
            "error : 1.684568, accuarcy : 0.610600\n",
            "total step : 14 \n",
            "error : 1.652097, accuarcy : 0.612300\n",
            "total step : 15 \n",
            "error : 1.620873, accuarcy : 0.615100\n",
            "total step : 16 \n",
            "error : 1.590803, accuarcy : 0.617600\n",
            "total step : 17 \n",
            "error : 1.561813, accuarcy : 0.620500\n",
            "total step : 18 \n",
            "error : 1.533843, accuarcy : 0.623100\n",
            "total step : 19 \n",
            "error : 1.506840, accuarcy : 0.624500\n",
            "total step : 20 \n",
            "error : 1.480760, accuarcy : 0.627800\n",
            "total step : 21 \n",
            "error : 1.455561, accuarcy : 0.632200\n",
            "total step : 22 \n",
            "error : 1.431205, accuarcy : 0.634800\n",
            "total step : 23 \n",
            "error : 1.407656, accuarcy : 0.639000\n",
            "total step : 24 \n",
            "error : 1.384882, accuarcy : 0.641900\n",
            "total step : 25 \n",
            "error : 1.362851, accuarcy : 0.644600\n",
            "total step : 26 \n",
            "error : 1.341531, accuarcy : 0.647100\n",
            "total step : 27 \n",
            "error : 1.320895, accuarcy : 0.650200\n",
            "total step : 28 \n",
            "error : 1.300916, accuarcy : 0.652500\n",
            "total step : 29 \n",
            "error : 1.281567, accuarcy : 0.655600\n",
            "total step : 30 \n",
            "error : 1.262823, accuarcy : 0.658500\n",
            "total step : 31 \n",
            "error : 1.244662, accuarcy : 0.660900\n",
            "total step : 32 \n",
            "error : 1.227061, accuarcy : 0.664200\n",
            "total step : 33 \n",
            "error : 1.210000, accuarcy : 0.666500\n",
            "total step : 34 \n",
            "error : 1.193457, accuarcy : 0.669300\n",
            "total step : 35 \n",
            "error : 1.177413, accuarcy : 0.672200\n",
            "total step : 36 \n",
            "error : 1.161852, accuarcy : 0.675400\n",
            "total step : 37 \n",
            "error : 1.146754, accuarcy : 0.677300\n",
            "total step : 38 \n",
            "error : 1.132104, accuarcy : 0.680000\n",
            "total step : 39 \n",
            "error : 1.117885, accuarcy : 0.682800\n",
            "total step : 40 \n",
            "error : 1.104081, accuarcy : 0.685400\n",
            "total step : 41 \n",
            "error : 1.090679, accuarcy : 0.687300\n",
            "total step : 42 \n",
            "error : 1.077662, accuarcy : 0.689300\n",
            "total step : 43 \n",
            "error : 1.065019, accuarcy : 0.691600\n",
            "total step : 44 \n",
            "error : 1.052734, accuarcy : 0.693400\n",
            "total step : 45 \n",
            "error : 1.040796, accuarcy : 0.695500\n",
            "total step : 46 \n",
            "error : 1.029192, accuarcy : 0.698300\n",
            "total step : 47 \n",
            "error : 1.017909, accuarcy : 0.700100\n",
            "total step : 48 \n",
            "error : 1.006938, accuarcy : 0.702200\n",
            "total step : 49 \n",
            "error : 0.996265, accuarcy : 0.703800\n",
            "total step : 50 \n",
            "error : 0.985881, accuarcy : 0.705600\n",
            "total step : 51 \n",
            "error : 0.975775, accuarcy : 0.706700\n",
            "total step : 52 \n",
            "error : 0.965938, accuarcy : 0.708500\n",
            "total step : 53 \n",
            "error : 0.956359, accuarcy : 0.710200\n",
            "total step : 54 \n",
            "error : 0.947030, accuarcy : 0.712500\n",
            "total step : 55 \n",
            "error : 0.937941, accuarcy : 0.715500\n",
            "total step : 56 \n",
            "error : 0.929085, accuarcy : 0.717100\n",
            "total step : 57 \n",
            "error : 0.920453, accuarcy : 0.718200\n",
            "total step : 58 \n",
            "error : 0.912037, accuarcy : 0.720800\n",
            "total step : 59 \n",
            "error : 0.903829, accuarcy : 0.722800\n",
            "total step : 60 \n",
            "error : 0.895823, accuarcy : 0.724400\n",
            "total step : 61 \n",
            "error : 0.888010, accuarcy : 0.726800\n",
            "total step : 62 \n",
            "error : 0.880385, accuarcy : 0.728800\n",
            "total step : 63 \n",
            "error : 0.872941, accuarcy : 0.730800\n",
            "total step : 64 \n",
            "error : 0.865672, accuarcy : 0.732900\n",
            "total step : 65 \n",
            "error : 0.858570, accuarcy : 0.734400\n",
            "total step : 66 \n",
            "error : 0.851632, accuarcy : 0.735500\n",
            "total step : 67 \n",
            "error : 0.844850, accuarcy : 0.736500\n",
            "total step : 68 \n",
            "error : 0.838220, accuarcy : 0.738000\n",
            "total step : 69 \n",
            "error : 0.831737, accuarcy : 0.739300\n",
            "total step : 70 \n",
            "error : 0.825395, accuarcy : 0.740700\n",
            "total step : 71 \n",
            "error : 0.819189, accuarcy : 0.741900\n",
            "total step : 72 \n",
            "error : 0.813116, accuarcy : 0.742900\n",
            "total step : 73 \n",
            "error : 0.807170, accuarcy : 0.744400\n",
            "total step : 74 \n",
            "error : 0.801348, accuarcy : 0.745800\n",
            "total step : 75 \n",
            "error : 0.795645, accuarcy : 0.747600\n",
            "total step : 76 \n",
            "error : 0.790057, accuarcy : 0.749000\n",
            "total step : 77 \n",
            "error : 0.784581, accuarcy : 0.749900\n",
            "total step : 78 \n",
            "error : 0.779214, accuarcy : 0.750900\n",
            "total step : 79 \n",
            "error : 0.773950, accuarcy : 0.751300\n",
            "total step : 80 \n",
            "error : 0.768788, accuarcy : 0.752900\n",
            "total step : 81 \n",
            "error : 0.763724, accuarcy : 0.753500\n",
            "total step : 82 \n",
            "error : 0.758755, accuarcy : 0.754500\n",
            "total step : 83 \n",
            "error : 0.753878, accuarcy : 0.755700\n",
            "total step : 84 \n",
            "error : 0.749090, accuarcy : 0.756600\n",
            "total step : 85 \n",
            "error : 0.744389, accuarcy : 0.757900\n",
            "total step : 86 \n",
            "error : 0.739771, accuarcy : 0.758800\n",
            "total step : 87 \n",
            "error : 0.735236, accuarcy : 0.760600\n",
            "total step : 88 \n",
            "error : 0.730779, accuarcy : 0.761500\n",
            "total step : 89 \n",
            "error : 0.726399, accuarcy : 0.762400\n",
            "total step : 90 \n",
            "error : 0.722094, accuarcy : 0.763800\n",
            "total step : 91 \n",
            "error : 0.717861, accuarcy : 0.764600\n",
            "total step : 92 \n",
            "error : 0.713699, accuarcy : 0.765600\n",
            "total step : 93 \n",
            "error : 0.709605, accuarcy : 0.767000\n",
            "total step : 94 \n",
            "error : 0.705577, accuarcy : 0.768000\n",
            "total step : 95 \n",
            "error : 0.701615, accuarcy : 0.769300\n",
            "total step : 96 \n",
            "error : 0.697715, accuarcy : 0.770500\n",
            "total step : 97 \n",
            "error : 0.693877, accuarcy : 0.771100\n",
            "total step : 98 \n",
            "error : 0.690098, accuarcy : 0.771700\n",
            "total step : 99 \n",
            "error : 0.686378, accuarcy : 0.772000\n",
            "total step : 100 \n",
            "error : 0.682714, accuarcy : 0.773000\n",
            "total step : 101 \n",
            "error : 0.679105, accuarcy : 0.773500\n",
            "total step : 102 \n",
            "error : 0.675549, accuarcy : 0.775200\n",
            "total step : 103 \n",
            "error : 0.672047, accuarcy : 0.775900\n",
            "total step : 104 \n",
            "error : 0.668595, accuarcy : 0.776600\n",
            "total step : 105 \n",
            "error : 0.665192, accuarcy : 0.777200\n",
            "total step : 106 \n",
            "error : 0.661838, accuarcy : 0.778000\n",
            "total step : 107 \n",
            "error : 0.658532, accuarcy : 0.778100\n",
            "total step : 108 \n",
            "error : 0.655271, accuarcy : 0.779300\n",
            "total step : 109 \n",
            "error : 0.652056, accuarcy : 0.779800\n",
            "total step : 110 \n",
            "error : 0.648884, accuarcy : 0.780400\n",
            "total step : 111 \n",
            "error : 0.645755, accuarcy : 0.781500\n",
            "total step : 112 \n",
            "error : 0.642668, accuarcy : 0.782400\n",
            "total step : 113 \n",
            "error : 0.639622, accuarcy : 0.783100\n",
            "total step : 114 \n",
            "error : 0.636616, accuarcy : 0.784100\n",
            "total step : 115 \n",
            "error : 0.633648, accuarcy : 0.784800\n",
            "total step : 116 \n",
            "error : 0.630719, accuarcy : 0.785700\n",
            "total step : 117 \n",
            "error : 0.627827, accuarcy : 0.786000\n",
            "total step : 118 \n",
            "error : 0.624971, accuarcy : 0.786800\n",
            "total step : 119 \n",
            "error : 0.622151, accuarcy : 0.787400\n",
            "total step : 120 \n",
            "error : 0.619365, accuarcy : 0.788000\n",
            "total step : 121 \n",
            "error : 0.616613, accuarcy : 0.788100\n",
            "total step : 122 \n",
            "error : 0.613895, accuarcy : 0.788600\n",
            "total step : 123 \n",
            "error : 0.611209, accuarcy : 0.788900\n",
            "total step : 124 \n",
            "error : 0.608555, accuarcy : 0.789600\n",
            "total step : 125 \n",
            "error : 0.605931, accuarcy : 0.790600\n",
            "total step : 126 \n",
            "error : 0.603339, accuarcy : 0.791800\n",
            "total step : 127 \n",
            "error : 0.600776, accuarcy : 0.792300\n",
            "total step : 128 \n",
            "error : 0.598242, accuarcy : 0.792700\n",
            "total step : 129 \n",
            "error : 0.595737, accuarcy : 0.793600\n",
            "total step : 130 \n",
            "error : 0.593260, accuarcy : 0.794400\n",
            "total step : 131 \n",
            "error : 0.590810, accuarcy : 0.795100\n",
            "total step : 132 \n",
            "error : 0.588388, accuarcy : 0.795100\n",
            "total step : 133 \n",
            "error : 0.585991, accuarcy : 0.795400\n",
            "total step : 134 \n",
            "error : 0.583620, accuarcy : 0.795800\n",
            "total step : 135 \n",
            "error : 0.581275, accuarcy : 0.796400\n",
            "total step : 136 \n",
            "error : 0.578955, accuarcy : 0.796700\n",
            "total step : 137 \n",
            "error : 0.576658, accuarcy : 0.797400\n",
            "total step : 138 \n",
            "error : 0.574386, accuarcy : 0.797800\n",
            "total step : 139 \n",
            "error : 0.572137, accuarcy : 0.798300\n",
            "total step : 140 \n",
            "error : 0.569912, accuarcy : 0.798500\n",
            "total step : 141 \n",
            "error : 0.567708, accuarcy : 0.798800\n",
            "total step : 142 \n",
            "error : 0.565527, accuarcy : 0.799000\n",
            "total step : 143 \n",
            "error : 0.563368, accuarcy : 0.799500\n",
            "total step : 144 \n",
            "error : 0.561230, accuarcy : 0.800400\n",
            "total step : 145 \n",
            "error : 0.559114, accuarcy : 0.800700\n",
            "total step : 146 \n",
            "error : 0.557017, accuarcy : 0.801400\n",
            "total step : 147 \n",
            "error : 0.554941, accuarcy : 0.802100\n",
            "total step : 148 \n",
            "error : 0.552885, accuarcy : 0.802800\n",
            "total step : 149 \n",
            "error : 0.550849, accuarcy : 0.803400\n",
            "total step : 150 \n",
            "error : 0.548832, accuarcy : 0.804000\n",
            "total step : 151 \n",
            "error : 0.546834, accuarcy : 0.804500\n",
            "total step : 152 \n",
            "error : 0.544854, accuarcy : 0.805300\n",
            "total step : 153 \n",
            "error : 0.542893, accuarcy : 0.806100\n",
            "total step : 154 \n",
            "error : 0.540950, accuarcy : 0.806500\n",
            "total step : 155 \n",
            "error : 0.539024, accuarcy : 0.806700\n",
            "total step : 156 \n",
            "error : 0.537116, accuarcy : 0.806900\n",
            "total step : 157 \n",
            "error : 0.535225, accuarcy : 0.807300\n",
            "total step : 158 \n",
            "error : 0.533352, accuarcy : 0.807400\n",
            "total step : 159 \n",
            "error : 0.531494, accuarcy : 0.807800\n",
            "total step : 160 \n",
            "error : 0.529653, accuarcy : 0.808300\n",
            "total step : 161 \n",
            "error : 0.527829, accuarcy : 0.808300\n",
            "total step : 162 \n",
            "error : 0.526020, accuarcy : 0.808800\n",
            "total step : 163 \n",
            "error : 0.524227, accuarcy : 0.809100\n",
            "total step : 164 \n",
            "error : 0.522450, accuarcy : 0.809500\n",
            "total step : 165 \n",
            "error : 0.520687, accuarcy : 0.809900\n",
            "total step : 166 \n",
            "error : 0.518940, accuarcy : 0.810500\n",
            "total step : 167 \n",
            "error : 0.517207, accuarcy : 0.811300\n",
            "total step : 168 \n",
            "error : 0.515489, accuarcy : 0.811700\n",
            "total step : 169 \n",
            "error : 0.513786, accuarcy : 0.812200\n",
            "total step : 170 \n",
            "error : 0.512097, accuarcy : 0.812500\n",
            "total step : 171 \n",
            "error : 0.510421, accuarcy : 0.813000\n",
            "total step : 172 \n",
            "error : 0.508760, accuarcy : 0.813200\n",
            "total step : 173 \n",
            "error : 0.507112, accuarcy : 0.813700\n",
            "total step : 174 \n",
            "error : 0.505477, accuarcy : 0.813800\n",
            "total step : 175 \n",
            "error : 0.503856, accuarcy : 0.813900\n",
            "total step : 176 \n",
            "error : 0.502248, accuarcy : 0.814100\n",
            "total step : 177 \n",
            "error : 0.500653, accuarcy : 0.814500\n",
            "total step : 178 \n",
            "error : 0.499071, accuarcy : 0.814800\n",
            "total step : 179 \n",
            "error : 0.497501, accuarcy : 0.815400\n",
            "total step : 180 \n",
            "error : 0.495944, accuarcy : 0.815800\n",
            "total step : 181 \n",
            "error : 0.494398, accuarcy : 0.816200\n",
            "total step : 182 \n",
            "error : 0.492866, accuarcy : 0.816800\n",
            "total step : 183 \n",
            "error : 0.491345, accuarcy : 0.817100\n",
            "total step : 184 \n",
            "error : 0.489836, accuarcy : 0.817200\n",
            "total step : 185 \n",
            "error : 0.488338, accuarcy : 0.817400\n",
            "total step : 186 \n",
            "error : 0.486852, accuarcy : 0.818100\n",
            "total step : 187 \n",
            "error : 0.485378, accuarcy : 0.818500\n",
            "total step : 188 \n",
            "error : 0.483915, accuarcy : 0.818800\n",
            "total step : 189 \n",
            "error : 0.482463, accuarcy : 0.819000\n",
            "total step : 190 \n",
            "error : 0.481022, accuarcy : 0.819300\n",
            "total step : 191 \n",
            "error : 0.479592, accuarcy : 0.819900\n",
            "total step : 192 \n",
            "error : 0.478173, accuarcy : 0.820800\n",
            "total step : 193 \n",
            "error : 0.476764, accuarcy : 0.821200\n",
            "total step : 194 \n",
            "error : 0.475366, accuarcy : 0.821400\n",
            "total step : 195 \n",
            "error : 0.473979, accuarcy : 0.821700\n",
            "total step : 196 \n",
            "error : 0.472601, accuarcy : 0.822000\n",
            "total step : 197 \n",
            "error : 0.471234, accuarcy : 0.822700\n",
            "total step : 198 \n",
            "error : 0.469877, accuarcy : 0.822800\n",
            "total step : 199 \n",
            "error : 0.468530, accuarcy : 0.823100\n",
            "total step : 200 \n",
            "error : 0.467193, accuarcy : 0.823100\n",
            "total step : 201 \n",
            "error : 0.465865, accuarcy : 0.823400\n",
            "total step : 202 \n",
            "error : 0.464548, accuarcy : 0.823700\n",
            "total step : 203 \n",
            "error : 0.463239, accuarcy : 0.824200\n",
            "total step : 204 \n",
            "error : 0.461941, accuarcy : 0.824700\n",
            "total step : 205 \n",
            "error : 0.460651, accuarcy : 0.824800\n",
            "total step : 206 \n",
            "error : 0.459371, accuarcy : 0.825000\n",
            "total step : 207 \n",
            "error : 0.458100, accuarcy : 0.825200\n",
            "total step : 208 \n",
            "error : 0.456838, accuarcy : 0.825400\n",
            "total step : 209 \n",
            "error : 0.455585, accuarcy : 0.825500\n",
            "total step : 210 \n",
            "error : 0.454341, accuarcy : 0.825900\n",
            "total step : 211 \n",
            "error : 0.453105, accuarcy : 0.826100\n",
            "total step : 212 \n",
            "error : 0.451879, accuarcy : 0.826400\n",
            "total step : 213 \n",
            "error : 0.450661, accuarcy : 0.826800\n",
            "total step : 214 \n",
            "error : 0.449451, accuarcy : 0.827200\n",
            "total step : 215 \n",
            "error : 0.448250, accuarcy : 0.827900\n",
            "total step : 216 \n",
            "error : 0.447057, accuarcy : 0.828200\n",
            "total step : 217 \n",
            "error : 0.445873, accuarcy : 0.828400\n",
            "total step : 218 \n",
            "error : 0.444696, accuarcy : 0.828500\n",
            "total step : 219 \n",
            "error : 0.443528, accuarcy : 0.829100\n",
            "total step : 220 \n",
            "error : 0.442368, accuarcy : 0.829400\n",
            "total step : 221 \n",
            "error : 0.441216, accuarcy : 0.829700\n",
            "total step : 222 \n",
            "error : 0.440071, accuarcy : 0.829900\n",
            "total step : 223 \n",
            "error : 0.438935, accuarcy : 0.830200\n",
            "total step : 224 \n",
            "error : 0.437806, accuarcy : 0.830700\n",
            "total step : 225 \n",
            "error : 0.436684, accuarcy : 0.831000\n",
            "total step : 226 \n",
            "error : 0.435571, accuarcy : 0.831200\n",
            "total step : 227 \n",
            "error : 0.434465, accuarcy : 0.831700\n",
            "total step : 228 \n",
            "error : 0.433366, accuarcy : 0.831700\n",
            "total step : 229 \n",
            "error : 0.432274, accuarcy : 0.832200\n",
            "total step : 230 \n",
            "error : 0.431190, accuarcy : 0.832500\n",
            "total step : 231 \n",
            "error : 0.430113, accuarcy : 0.832700\n",
            "total step : 232 \n",
            "error : 0.429043, accuarcy : 0.833300\n",
            "total step : 233 \n",
            "error : 0.427981, accuarcy : 0.833800\n",
            "total step : 234 \n",
            "error : 0.426925, accuarcy : 0.834000\n",
            "total step : 235 \n",
            "error : 0.425876, accuarcy : 0.834400\n",
            "total step : 236 \n",
            "error : 0.424834, accuarcy : 0.834900\n",
            "total step : 237 \n",
            "error : 0.423799, accuarcy : 0.835000\n",
            "total step : 238 \n",
            "error : 0.422771, accuarcy : 0.835300\n",
            "total step : 239 \n",
            "error : 0.421749, accuarcy : 0.835500\n",
            "total step : 240 \n",
            "error : 0.420734, accuarcy : 0.835600\n",
            "total step : 241 \n",
            "error : 0.419725, accuarcy : 0.836000\n",
            "total step : 242 \n",
            "error : 0.418723, accuarcy : 0.836200\n",
            "total step : 243 \n",
            "error : 0.417727, accuarcy : 0.836300\n",
            "total step : 244 \n",
            "error : 0.416738, accuarcy : 0.836400\n",
            "total step : 245 \n",
            "error : 0.415755, accuarcy : 0.836400\n",
            "total step : 246 \n",
            "error : 0.414778, accuarcy : 0.836500\n",
            "total step : 247 \n",
            "error : 0.413808, accuarcy : 0.836700\n",
            "total step : 248 \n",
            "error : 0.412843, accuarcy : 0.837000\n",
            "total step : 249 \n",
            "error : 0.411885, accuarcy : 0.837200\n",
            "total step : 250 \n",
            "error : 0.410932, accuarcy : 0.837700\n",
            "total step : 251 \n",
            "error : 0.409986, accuarcy : 0.837600\n",
            "total step : 252 \n",
            "error : 0.409046, accuarcy : 0.837900\n",
            "total step : 253 \n",
            "error : 0.408111, accuarcy : 0.838300\n",
            "total step : 254 \n",
            "error : 0.407182, accuarcy : 0.838800\n",
            "total step : 255 \n",
            "error : 0.406259, accuarcy : 0.839300\n",
            "total step : 256 \n",
            "error : 0.405342, accuarcy : 0.839500\n",
            "total step : 257 \n",
            "error : 0.404430, accuarcy : 0.839700\n",
            "total step : 258 \n",
            "error : 0.403524, accuarcy : 0.840300\n",
            "total step : 259 \n",
            "error : 0.402623, accuarcy : 0.840600\n",
            "total step : 260 \n",
            "error : 0.401728, accuarcy : 0.840700\n",
            "total step : 261 \n",
            "error : 0.400838, accuarcy : 0.840800\n",
            "total step : 262 \n",
            "error : 0.399954, accuarcy : 0.840900\n",
            "total step : 263 \n",
            "error : 0.399075, accuarcy : 0.841200\n",
            "total step : 264 \n",
            "error : 0.398201, accuarcy : 0.841200\n",
            "total step : 265 \n",
            "error : 0.397332, accuarcy : 0.841400\n",
            "total step : 266 \n",
            "error : 0.396469, accuarcy : 0.841400\n",
            "total step : 267 \n",
            "error : 0.395611, accuarcy : 0.842100\n",
            "total step : 268 \n",
            "error : 0.394758, accuarcy : 0.842300\n",
            "total step : 269 \n",
            "error : 0.393910, accuarcy : 0.842400\n",
            "total step : 270 \n",
            "error : 0.393067, accuarcy : 0.842500\n",
            "total step : 271 \n",
            "error : 0.392229, accuarcy : 0.842700\n",
            "total step : 272 \n",
            "error : 0.391396, accuarcy : 0.842700\n",
            "total step : 273 \n",
            "error : 0.390567, accuarcy : 0.843300\n",
            "total step : 274 \n",
            "error : 0.389744, accuarcy : 0.843900\n",
            "total step : 275 \n",
            "error : 0.388925, accuarcy : 0.844100\n",
            "total step : 276 \n",
            "error : 0.388111, accuarcy : 0.844300\n",
            "total step : 277 \n",
            "error : 0.387302, accuarcy : 0.844400\n",
            "total step : 278 \n",
            "error : 0.386498, accuarcy : 0.844500\n",
            "total step : 279 \n",
            "error : 0.385698, accuarcy : 0.844600\n",
            "total step : 280 \n",
            "error : 0.384902, accuarcy : 0.844700\n",
            "total step : 281 \n",
            "error : 0.384111, accuarcy : 0.844700\n",
            "total step : 282 \n",
            "error : 0.383325, accuarcy : 0.844800\n",
            "total step : 283 \n",
            "error : 0.382543, accuarcy : 0.844900\n",
            "total step : 284 \n",
            "error : 0.381766, accuarcy : 0.845200\n",
            "total step : 285 \n",
            "error : 0.380993, accuarcy : 0.845200\n",
            "total step : 286 \n",
            "error : 0.380224, accuarcy : 0.845400\n",
            "total step : 287 \n",
            "error : 0.379460, accuarcy : 0.845500\n",
            "total step : 288 \n",
            "error : 0.378699, accuarcy : 0.845500\n",
            "total step : 289 \n",
            "error : 0.377943, accuarcy : 0.845800\n",
            "total step : 290 \n",
            "error : 0.377192, accuarcy : 0.846100\n",
            "total step : 291 \n",
            "error : 0.376444, accuarcy : 0.846200\n",
            "total step : 292 \n",
            "error : 0.375701, accuarcy : 0.846300\n",
            "total step : 293 \n",
            "error : 0.374961, accuarcy : 0.846700\n",
            "total step : 294 \n",
            "error : 0.374226, accuarcy : 0.846700\n",
            "total step : 295 \n",
            "error : 0.373495, accuarcy : 0.847000\n",
            "total step : 296 \n",
            "error : 0.372767, accuarcy : 0.847200\n",
            "total step : 297 \n",
            "error : 0.372044, accuarcy : 0.847400\n",
            "total step : 298 \n",
            "error : 0.371324, accuarcy : 0.847800\n",
            "total step : 299 \n",
            "error : 0.370609, accuarcy : 0.847900\n",
            "total step : 300 \n",
            "error : 0.369897, accuarcy : 0.847900\n",
            "total step : 301 \n",
            "error : 0.369189, accuarcy : 0.848100\n",
            "total step : 302 \n",
            "error : 0.368485, accuarcy : 0.848200\n",
            "total step : 303 \n",
            "error : 0.367785, accuarcy : 0.848300\n",
            "total step : 304 \n",
            "error : 0.367088, accuarcy : 0.848400\n",
            "total step : 305 \n",
            "error : 0.366395, accuarcy : 0.848600\n",
            "total step : 306 \n",
            "error : 0.365706, accuarcy : 0.848800\n",
            "total step : 307 \n",
            "error : 0.365020, accuarcy : 0.849200\n",
            "total step : 308 \n",
            "error : 0.364338, accuarcy : 0.849300\n",
            "total step : 309 \n",
            "error : 0.363659, accuarcy : 0.849600\n",
            "total step : 310 \n",
            "error : 0.362984, accuarcy : 0.850000\n",
            "total step : 311 \n",
            "error : 0.362313, accuarcy : 0.850100\n"
          ]
        }
      ],
      "source": [
        "train_4X, train_4y = make_sample(idx = 4) # idx = target number\n",
        "train_4X = np.insert(train_4X, 0, 1, axis=1) # bias 추가\n",
        "w4,J4_history, ACC4_history = train(train_4X, train_4y,4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total step : 1 \n",
            "error : 2.327907, accuarcy : 0.729000\n",
            "total step : 2 \n",
            "error : 2.062304, accuarcy : 0.706400\n",
            "total step : 3 \n",
            "error : 1.877190, accuarcy : 0.684800\n",
            "total step : 4 \n",
            "error : 1.751052, accuarcy : 0.667400\n",
            "total step : 5 \n",
            "error : 1.665293, accuarcy : 0.654600\n",
            "total step : 6 \n",
            "error : 1.605845, accuarcy : 0.642300\n",
            "total step : 7 \n",
            "error : 1.563147, accuarcy : 0.633800\n",
            "total step : 8 \n",
            "error : 1.531035, accuarcy : 0.627000\n",
            "total step : 9 \n",
            "error : 1.505587, accuarcy : 0.622400\n",
            "total step : 10 \n",
            "error : 1.484337, accuarcy : 0.619200\n",
            "total step : 11 \n",
            "error : 1.465753, accuarcy : 0.617200\n",
            "total step : 12 \n",
            "error : 1.448902, accuarcy : 0.616900\n",
            "total step : 13 \n",
            "error : 1.433214, accuarcy : 0.616700\n",
            "total step : 14 \n",
            "error : 1.418345, accuarcy : 0.617600\n",
            "total step : 15 \n",
            "error : 1.404084, accuarcy : 0.617700\n",
            "total step : 16 \n",
            "error : 1.390302, accuarcy : 0.619000\n",
            "total step : 17 \n",
            "error : 1.376917, accuarcy : 0.621500\n",
            "total step : 18 \n",
            "error : 1.363877, accuarcy : 0.622900\n",
            "total step : 19 \n",
            "error : 1.351146, accuarcy : 0.624400\n",
            "total step : 20 \n",
            "error : 1.338701, accuarcy : 0.626300\n",
            "total step : 21 \n",
            "error : 1.326523, accuarcy : 0.627900\n",
            "total step : 22 \n",
            "error : 1.314598, accuarcy : 0.629700\n",
            "total step : 23 \n",
            "error : 1.302917, accuarcy : 0.632200\n",
            "total step : 24 \n",
            "error : 1.291468, accuarcy : 0.634600\n",
            "total step : 25 \n",
            "error : 1.280245, accuarcy : 0.636600\n",
            "total step : 26 \n",
            "error : 1.269240, accuarcy : 0.638600\n",
            "total step : 27 \n",
            "error : 1.258445, accuarcy : 0.641500\n",
            "total step : 28 \n",
            "error : 1.247855, accuarcy : 0.643400\n",
            "total step : 29 \n",
            "error : 1.237464, accuarcy : 0.645300\n",
            "total step : 30 \n",
            "error : 1.227265, accuarcy : 0.646500\n",
            "total step : 31 \n",
            "error : 1.217254, accuarcy : 0.648200\n",
            "total step : 32 \n",
            "error : 1.207426, accuarcy : 0.650800\n",
            "total step : 33 \n",
            "error : 1.197775, accuarcy : 0.652600\n",
            "total step : 34 \n",
            "error : 1.188296, accuarcy : 0.654100\n",
            "total step : 35 \n",
            "error : 1.178986, accuarcy : 0.655700\n",
            "total step : 36 \n",
            "error : 1.169839, accuarcy : 0.657300\n",
            "total step : 37 \n",
            "error : 1.160851, accuarcy : 0.659100\n",
            "total step : 38 \n",
            "error : 1.152019, accuarcy : 0.661000\n",
            "total step : 39 \n",
            "error : 1.143337, accuarcy : 0.662700\n",
            "total step : 40 \n",
            "error : 1.134803, accuarcy : 0.664100\n",
            "total step : 41 \n",
            "error : 1.126412, accuarcy : 0.665600\n",
            "total step : 42 \n",
            "error : 1.118162, accuarcy : 0.667100\n",
            "total step : 43 \n",
            "error : 1.110047, accuarcy : 0.669300\n",
            "total step : 44 \n",
            "error : 1.102066, accuarcy : 0.670200\n",
            "total step : 45 \n",
            "error : 1.094214, accuarcy : 0.672400\n",
            "total step : 46 \n",
            "error : 1.086489, accuarcy : 0.674500\n",
            "total step : 47 \n",
            "error : 1.078887, accuarcy : 0.676600\n",
            "total step : 48 \n",
            "error : 1.071406, accuarcy : 0.678400\n",
            "total step : 49 \n",
            "error : 1.064042, accuarcy : 0.680500\n",
            "total step : 50 \n",
            "error : 1.056793, accuarcy : 0.682200\n",
            "total step : 51 \n",
            "error : 1.049657, accuarcy : 0.683500\n",
            "total step : 52 \n",
            "error : 1.042630, accuarcy : 0.684600\n",
            "total step : 53 \n",
            "error : 1.035709, accuarcy : 0.685800\n",
            "total step : 54 \n",
            "error : 1.028894, accuarcy : 0.687200\n",
            "total step : 55 \n",
            "error : 1.022181, accuarcy : 0.688300\n",
            "total step : 56 \n",
            "error : 1.015568, accuarcy : 0.689600\n",
            "total step : 57 \n",
            "error : 1.009052, accuarcy : 0.691200\n",
            "total step : 58 \n",
            "error : 1.002633, accuarcy : 0.693700\n",
            "total step : 59 \n",
            "error : 0.996307, accuarcy : 0.695300\n",
            "total step : 60 \n",
            "error : 0.990073, accuarcy : 0.696600\n",
            "total step : 61 \n",
            "error : 0.983928, accuarcy : 0.698200\n",
            "total step : 62 \n",
            "error : 0.977872, accuarcy : 0.699300\n",
            "total step : 63 \n",
            "error : 0.971901, accuarcy : 0.701300\n",
            "total step : 64 \n",
            "error : 0.966015, accuarcy : 0.702600\n",
            "total step : 65 \n",
            "error : 0.960212, accuarcy : 0.703600\n",
            "total step : 66 \n",
            "error : 0.954490, accuarcy : 0.704600\n",
            "total step : 67 \n",
            "error : 0.948847, accuarcy : 0.705100\n",
            "total step : 68 \n",
            "error : 0.943282, accuarcy : 0.706200\n",
            "total step : 69 \n",
            "error : 0.937793, accuarcy : 0.707600\n",
            "total step : 70 \n",
            "error : 0.932379, accuarcy : 0.709300\n",
            "total step : 71 \n",
            "error : 0.927039, accuarcy : 0.710900\n",
            "total step : 72 \n",
            "error : 0.921770, accuarcy : 0.711900\n",
            "total step : 73 \n",
            "error : 0.916573, accuarcy : 0.712800\n",
            "total step : 74 \n",
            "error : 0.911444, accuarcy : 0.714100\n",
            "total step : 75 \n",
            "error : 0.906384, accuarcy : 0.715500\n",
            "total step : 76 \n",
            "error : 0.901391, accuarcy : 0.716800\n",
            "total step : 77 \n",
            "error : 0.896463, accuarcy : 0.717500\n",
            "total step : 78 \n",
            "error : 0.891600, accuarcy : 0.718900\n",
            "total step : 79 \n",
            "error : 0.886799, accuarcy : 0.720300\n",
            "total step : 80 \n",
            "error : 0.882061, accuarcy : 0.721000\n",
            "total step : 81 \n",
            "error : 0.877384, accuarcy : 0.722800\n",
            "total step : 82 \n",
            "error : 0.872766, accuarcy : 0.723900\n",
            "total step : 83 \n",
            "error : 0.868207, accuarcy : 0.725000\n",
            "total step : 84 \n",
            "error : 0.863705, accuarcy : 0.726000\n",
            "total step : 85 \n",
            "error : 0.859261, accuarcy : 0.726600\n",
            "total step : 86 \n",
            "error : 0.854871, accuarcy : 0.727500\n",
            "total step : 87 \n",
            "error : 0.850536, accuarcy : 0.728400\n",
            "total step : 88 \n",
            "error : 0.846255, accuarcy : 0.729400\n",
            "total step : 89 \n",
            "error : 0.842026, accuarcy : 0.730400\n",
            "total step : 90 \n",
            "error : 0.837849, accuarcy : 0.731600\n",
            "total step : 91 \n",
            "error : 0.833723, accuarcy : 0.732600\n",
            "total step : 92 \n",
            "error : 0.829646, accuarcy : 0.733400\n",
            "total step : 93 \n",
            "error : 0.825619, accuarcy : 0.733800\n",
            "total step : 94 \n",
            "error : 0.821639, accuarcy : 0.734500\n",
            "total step : 95 \n",
            "error : 0.817706, accuarcy : 0.735800\n",
            "total step : 96 \n",
            "error : 0.813820, accuarcy : 0.736900\n",
            "total step : 97 \n",
            "error : 0.809979, accuarcy : 0.737200\n",
            "total step : 98 \n",
            "error : 0.806183, accuarcy : 0.737900\n",
            "total step : 99 \n",
            "error : 0.802431, accuarcy : 0.738500\n",
            "total step : 100 \n",
            "error : 0.798722, accuarcy : 0.739600\n",
            "total step : 101 \n",
            "error : 0.795055, accuarcy : 0.740500\n",
            "total step : 102 \n",
            "error : 0.791431, accuarcy : 0.741500\n",
            "total step : 103 \n",
            "error : 0.787847, accuarcy : 0.742400\n",
            "total step : 104 \n",
            "error : 0.784303, accuarcy : 0.742900\n",
            "total step : 105 \n",
            "error : 0.780799, accuarcy : 0.743600\n",
            "total step : 106 \n",
            "error : 0.777334, accuarcy : 0.744100\n",
            "total step : 107 \n",
            "error : 0.773907, accuarcy : 0.744800\n",
            "total step : 108 \n",
            "error : 0.770518, accuarcy : 0.745600\n",
            "total step : 109 \n",
            "error : 0.767166, accuarcy : 0.746300\n",
            "total step : 110 \n",
            "error : 0.763850, accuarcy : 0.746700\n",
            "total step : 111 \n",
            "error : 0.760570, accuarcy : 0.747000\n",
            "total step : 112 \n",
            "error : 0.757326, accuarcy : 0.747800\n",
            "total step : 113 \n",
            "error : 0.754116, accuarcy : 0.748700\n",
            "total step : 114 \n",
            "error : 0.750940, accuarcy : 0.749300\n",
            "total step : 115 \n",
            "error : 0.747798, accuarcy : 0.750300\n",
            "total step : 116 \n",
            "error : 0.744689, accuarcy : 0.751000\n",
            "total step : 117 \n",
            "error : 0.741613, accuarcy : 0.751200\n",
            "total step : 118 \n",
            "error : 0.738569, accuarcy : 0.751700\n",
            "total step : 119 \n",
            "error : 0.735556, accuarcy : 0.752600\n",
            "total step : 120 \n",
            "error : 0.732575, accuarcy : 0.753600\n",
            "total step : 121 \n",
            "error : 0.729624, accuarcy : 0.754100\n",
            "total step : 122 \n",
            "error : 0.726703, accuarcy : 0.755000\n",
            "total step : 123 \n",
            "error : 0.723812, accuarcy : 0.755500\n",
            "total step : 124 \n",
            "error : 0.720951, accuarcy : 0.755700\n",
            "total step : 125 \n",
            "error : 0.718119, accuarcy : 0.756300\n",
            "total step : 126 \n",
            "error : 0.715315, accuarcy : 0.756900\n",
            "total step : 127 \n",
            "error : 0.712539, accuarcy : 0.757500\n",
            "total step : 128 \n",
            "error : 0.709791, accuarcy : 0.758400\n",
            "total step : 129 \n",
            "error : 0.707070, accuarcy : 0.758600\n",
            "total step : 130 \n",
            "error : 0.704376, accuarcy : 0.759500\n",
            "total step : 131 \n",
            "error : 0.701709, accuarcy : 0.760000\n",
            "total step : 132 \n",
            "error : 0.699068, accuarcy : 0.761200\n",
            "total step : 133 \n",
            "error : 0.696453, accuarcy : 0.761800\n",
            "total step : 134 \n",
            "error : 0.693863, accuarcy : 0.762600\n",
            "total step : 135 \n",
            "error : 0.691298, accuarcy : 0.763700\n",
            "total step : 136 \n",
            "error : 0.688759, accuarcy : 0.763800\n",
            "total step : 137 \n",
            "error : 0.686243, accuarcy : 0.764300\n",
            "total step : 138 \n",
            "error : 0.683752, accuarcy : 0.765100\n",
            "total step : 139 \n",
            "error : 0.681285, accuarcy : 0.765400\n",
            "total step : 140 \n",
            "error : 0.678841, accuarcy : 0.766100\n",
            "total step : 141 \n",
            "error : 0.676421, accuarcy : 0.766500\n",
            "total step : 142 \n",
            "error : 0.674023, accuarcy : 0.767200\n",
            "total step : 143 \n",
            "error : 0.671648, accuarcy : 0.768100\n",
            "total step : 144 \n",
            "error : 0.669295, accuarcy : 0.768700\n",
            "total step : 145 \n",
            "error : 0.666964, accuarcy : 0.769100\n",
            "total step : 146 \n",
            "error : 0.664655, accuarcy : 0.769800\n",
            "total step : 147 \n",
            "error : 0.662368, accuarcy : 0.770000\n",
            "total step : 148 \n",
            "error : 0.660101, accuarcy : 0.770700\n",
            "total step : 149 \n",
            "error : 0.657856, accuarcy : 0.771000\n",
            "total step : 150 \n",
            "error : 0.655631, accuarcy : 0.771500\n",
            "total step : 151 \n",
            "error : 0.653426, accuarcy : 0.772600\n",
            "total step : 152 \n",
            "error : 0.651242, accuarcy : 0.773600\n",
            "total step : 153 \n",
            "error : 0.649078, accuarcy : 0.773900\n",
            "total step : 154 \n",
            "error : 0.646933, accuarcy : 0.774200\n",
            "total step : 155 \n",
            "error : 0.644807, accuarcy : 0.774500\n",
            "total step : 156 \n",
            "error : 0.642701, accuarcy : 0.774900\n",
            "total step : 157 \n",
            "error : 0.640613, accuarcy : 0.775800\n",
            "total step : 158 \n",
            "error : 0.638544, accuarcy : 0.776200\n",
            "total step : 159 \n",
            "error : 0.636494, accuarcy : 0.776500\n",
            "total step : 160 \n",
            "error : 0.634462, accuarcy : 0.776800\n",
            "total step : 161 \n",
            "error : 0.632447, accuarcy : 0.777200\n",
            "total step : 162 \n",
            "error : 0.630451, accuarcy : 0.777300\n",
            "total step : 163 \n",
            "error : 0.628472, accuarcy : 0.778000\n",
            "total step : 164 \n",
            "error : 0.626510, accuarcy : 0.778300\n",
            "total step : 165 \n",
            "error : 0.624565, accuarcy : 0.778700\n",
            "total step : 166 \n",
            "error : 0.622637, accuarcy : 0.778500\n",
            "total step : 167 \n",
            "error : 0.620726, accuarcy : 0.779000\n",
            "total step : 168 \n",
            "error : 0.618831, accuarcy : 0.779800\n",
            "total step : 169 \n",
            "error : 0.616953, accuarcy : 0.780400\n",
            "total step : 170 \n",
            "error : 0.615090, accuarcy : 0.781100\n",
            "total step : 171 \n",
            "error : 0.613244, accuarcy : 0.781700\n",
            "total step : 172 \n",
            "error : 0.611413, accuarcy : 0.782100\n",
            "total step : 173 \n",
            "error : 0.609597, accuarcy : 0.782600\n",
            "total step : 174 \n",
            "error : 0.607797, accuarcy : 0.782900\n",
            "total step : 175 \n",
            "error : 0.606012, accuarcy : 0.783400\n",
            "total step : 176 \n",
            "error : 0.604242, accuarcy : 0.784100\n",
            "total step : 177 \n",
            "error : 0.602486, accuarcy : 0.784800\n",
            "total step : 178 \n",
            "error : 0.600745, accuarcy : 0.784900\n",
            "total step : 179 \n",
            "error : 0.599019, accuarcy : 0.785400\n",
            "total step : 180 \n",
            "error : 0.597307, accuarcy : 0.785900\n",
            "total step : 181 \n",
            "error : 0.595608, accuarcy : 0.785900\n",
            "total step : 182 \n",
            "error : 0.593924, accuarcy : 0.786400\n",
            "total step : 183 \n",
            "error : 0.592253, accuarcy : 0.787100\n",
            "total step : 184 \n",
            "error : 0.590596, accuarcy : 0.787100\n",
            "total step : 185 \n",
            "error : 0.588952, accuarcy : 0.787200\n",
            "total step : 186 \n",
            "error : 0.587322, accuarcy : 0.787600\n",
            "total step : 187 \n",
            "error : 0.585704, accuarcy : 0.787700\n",
            "total step : 188 \n",
            "error : 0.584099, accuarcy : 0.788100\n",
            "total step : 189 \n",
            "error : 0.582508, accuarcy : 0.788400\n",
            "total step : 190 \n",
            "error : 0.580928, accuarcy : 0.788800\n",
            "total step : 191 \n",
            "error : 0.579361, accuarcy : 0.789300\n",
            "total step : 192 \n",
            "error : 0.577807, accuarcy : 0.789400\n",
            "total step : 193 \n",
            "error : 0.576264, accuarcy : 0.789700\n",
            "total step : 194 \n",
            "error : 0.574734, accuarcy : 0.790100\n",
            "total step : 195 \n",
            "error : 0.573216, accuarcy : 0.790800\n",
            "total step : 196 \n",
            "error : 0.571709, accuarcy : 0.791000\n",
            "total step : 197 \n",
            "error : 0.570214, accuarcy : 0.791500\n",
            "total step : 198 \n",
            "error : 0.568730, accuarcy : 0.792200\n",
            "total step : 199 \n",
            "error : 0.567258, accuarcy : 0.792800\n",
            "total step : 200 \n",
            "error : 0.565796, accuarcy : 0.793300\n",
            "total step : 201 \n",
            "error : 0.564346, accuarcy : 0.793300\n",
            "total step : 202 \n",
            "error : 0.562907, accuarcy : 0.793900\n",
            "total step : 203 \n",
            "error : 0.561479, accuarcy : 0.794200\n",
            "total step : 204 \n",
            "error : 0.560061, accuarcy : 0.794600\n",
            "total step : 205 \n",
            "error : 0.558654, accuarcy : 0.795000\n",
            "total step : 206 \n",
            "error : 0.557257, accuarcy : 0.795400\n",
            "total step : 207 \n",
            "error : 0.555871, accuarcy : 0.795900\n",
            "total step : 208 \n",
            "error : 0.554495, accuarcy : 0.795900\n",
            "total step : 209 \n",
            "error : 0.553129, accuarcy : 0.796500\n",
            "total step : 210 \n",
            "error : 0.551773, accuarcy : 0.796800\n",
            "total step : 211 \n",
            "error : 0.550427, accuarcy : 0.797200\n",
            "total step : 212 \n",
            "error : 0.549090, accuarcy : 0.797900\n",
            "total step : 213 \n",
            "error : 0.547764, accuarcy : 0.798500\n",
            "total step : 214 \n",
            "error : 0.546447, accuarcy : 0.798600\n",
            "total step : 215 \n",
            "error : 0.545139, accuarcy : 0.798900\n",
            "total step : 216 \n",
            "error : 0.543841, accuarcy : 0.799300\n",
            "total step : 217 \n",
            "error : 0.542551, accuarcy : 0.799600\n",
            "total step : 218 \n",
            "error : 0.541271, accuarcy : 0.799800\n",
            "total step : 219 \n",
            "error : 0.540000, accuarcy : 0.800200\n",
            "total step : 220 \n",
            "error : 0.538738, accuarcy : 0.800500\n",
            "total step : 221 \n",
            "error : 0.537485, accuarcy : 0.800900\n",
            "total step : 222 \n",
            "error : 0.536241, accuarcy : 0.801200\n",
            "total step : 223 \n",
            "error : 0.535005, accuarcy : 0.801800\n",
            "total step : 224 \n",
            "error : 0.533778, accuarcy : 0.802200\n",
            "total step : 225 \n",
            "error : 0.532559, accuarcy : 0.802800\n",
            "total step : 226 \n",
            "error : 0.531349, accuarcy : 0.803000\n",
            "total step : 227 \n",
            "error : 0.530147, accuarcy : 0.803300\n",
            "total step : 228 \n",
            "error : 0.528953, accuarcy : 0.803300\n",
            "total step : 229 \n",
            "error : 0.527767, accuarcy : 0.804000\n",
            "total step : 230 \n",
            "error : 0.526590, accuarcy : 0.804100\n",
            "total step : 231 \n",
            "error : 0.525420, accuarcy : 0.804200\n",
            "total step : 232 \n",
            "error : 0.524258, accuarcy : 0.804700\n",
            "total step : 233 \n",
            "error : 0.523104, accuarcy : 0.805100\n",
            "total step : 234 \n",
            "error : 0.521958, accuarcy : 0.805500\n",
            "total step : 235 \n",
            "error : 0.520819, accuarcy : 0.805700\n",
            "total step : 236 \n",
            "error : 0.519688, accuarcy : 0.806200\n",
            "total step : 237 \n",
            "error : 0.518564, accuarcy : 0.806600\n",
            "total step : 238 \n",
            "error : 0.517448, accuarcy : 0.807000\n",
            "total step : 239 \n",
            "error : 0.516339, accuarcy : 0.807200\n",
            "total step : 240 \n",
            "error : 0.515237, accuarcy : 0.807700\n",
            "total step : 241 \n",
            "error : 0.514143, accuarcy : 0.808000\n",
            "total step : 242 \n",
            "error : 0.513055, accuarcy : 0.808400\n",
            "total step : 243 \n",
            "error : 0.511975, accuarcy : 0.808800\n",
            "total step : 244 \n",
            "error : 0.510901, accuarcy : 0.809300\n",
            "total step : 245 \n",
            "error : 0.509835, accuarcy : 0.809300\n",
            "total step : 246 \n",
            "error : 0.508775, accuarcy : 0.809500\n",
            "total step : 247 \n",
            "error : 0.507722, accuarcy : 0.809800\n",
            "total step : 248 \n",
            "error : 0.506676, accuarcy : 0.810500\n",
            "total step : 249 \n",
            "error : 0.505637, accuarcy : 0.810800\n",
            "total step : 250 \n",
            "error : 0.504604, accuarcy : 0.810800\n",
            "total step : 251 \n",
            "error : 0.503577, accuarcy : 0.810900\n",
            "total step : 252 \n",
            "error : 0.502557, accuarcy : 0.811100\n",
            "total step : 253 \n",
            "error : 0.501544, accuarcy : 0.811100\n",
            "total step : 254 \n",
            "error : 0.500536, accuarcy : 0.811200\n",
            "total step : 255 \n",
            "error : 0.499535, accuarcy : 0.811500\n",
            "total step : 256 \n",
            "error : 0.498541, accuarcy : 0.811700\n",
            "total step : 257 \n",
            "error : 0.497552, accuarcy : 0.812100\n",
            "total step : 258 \n",
            "error : 0.496569, accuarcy : 0.812300\n",
            "total step : 259 \n",
            "error : 0.495593, accuarcy : 0.812500\n",
            "total step : 260 \n",
            "error : 0.494622, accuarcy : 0.813000\n",
            "total step : 261 \n",
            "error : 0.493658, accuarcy : 0.813200\n",
            "total step : 262 \n",
            "error : 0.492699, accuarcy : 0.813100\n",
            "total step : 263 \n",
            "error : 0.491746, accuarcy : 0.813200\n",
            "total step : 264 \n",
            "error : 0.490799, accuarcy : 0.813700\n",
            "total step : 265 \n",
            "error : 0.489858, accuarcy : 0.814000\n",
            "total step : 266 \n",
            "error : 0.488922, accuarcy : 0.814000\n",
            "total step : 267 \n",
            "error : 0.487992, accuarcy : 0.814200\n",
            "total step : 268 \n",
            "error : 0.487067, accuarcy : 0.814700\n",
            "total step : 269 \n",
            "error : 0.486148, accuarcy : 0.815000\n",
            "total step : 270 \n",
            "error : 0.485235, accuarcy : 0.815300\n",
            "total step : 271 \n",
            "error : 0.484326, accuarcy : 0.816100\n",
            "total step : 272 \n",
            "error : 0.483424, accuarcy : 0.816700\n",
            "total step : 273 \n",
            "error : 0.482526, accuarcy : 0.816900\n",
            "total step : 274 \n",
            "error : 0.481634, accuarcy : 0.817000\n",
            "total step : 275 \n",
            "error : 0.480747, accuarcy : 0.817000\n",
            "total step : 276 \n",
            "error : 0.479865, accuarcy : 0.817100\n",
            "total step : 277 \n",
            "error : 0.478988, accuarcy : 0.817200\n",
            "total step : 278 \n",
            "error : 0.478117, accuarcy : 0.817800\n",
            "total step : 279 \n",
            "error : 0.477250, accuarcy : 0.818000\n",
            "total step : 280 \n",
            "error : 0.476389, accuarcy : 0.818200\n",
            "total step : 281 \n",
            "error : 0.475532, accuarcy : 0.818400\n",
            "total step : 282 \n",
            "error : 0.474681, accuarcy : 0.818700\n",
            "total step : 283 \n",
            "error : 0.473834, accuarcy : 0.819100\n",
            "total step : 284 \n",
            "error : 0.472992, accuarcy : 0.819200\n",
            "total step : 285 \n",
            "error : 0.472155, accuarcy : 0.819500\n",
            "total step : 286 \n",
            "error : 0.471322, accuarcy : 0.819500\n",
            "total step : 287 \n",
            "error : 0.470495, accuarcy : 0.819600\n",
            "total step : 288 \n",
            "error : 0.469671, accuarcy : 0.819600\n",
            "total step : 289 \n",
            "error : 0.468853, accuarcy : 0.819900\n",
            "total step : 290 \n",
            "error : 0.468039, accuarcy : 0.820200\n",
            "total step : 291 \n",
            "error : 0.467230, accuarcy : 0.820400\n",
            "total step : 292 \n",
            "error : 0.466425, accuarcy : 0.820400\n",
            "total step : 293 \n",
            "error : 0.465625, accuarcy : 0.820600\n",
            "total step : 294 \n",
            "error : 0.464829, accuarcy : 0.820800\n",
            "total step : 295 \n",
            "error : 0.464038, accuarcy : 0.821200\n",
            "total step : 296 \n",
            "error : 0.463251, accuarcy : 0.821300\n",
            "total step : 297 \n",
            "error : 0.462468, accuarcy : 0.821700\n",
            "total step : 298 \n",
            "error : 0.461690, accuarcy : 0.822000\n",
            "total step : 299 \n",
            "error : 0.460915, accuarcy : 0.822300\n",
            "total step : 300 \n",
            "error : 0.460145, accuarcy : 0.822300\n",
            "total step : 301 \n",
            "error : 0.459380, accuarcy : 0.822800\n",
            "total step : 302 \n",
            "error : 0.458618, accuarcy : 0.822900\n",
            "total step : 303 \n",
            "error : 0.457861, accuarcy : 0.823100\n",
            "total step : 304 \n",
            "error : 0.457107, accuarcy : 0.823300\n",
            "total step : 305 \n",
            "error : 0.456358, accuarcy : 0.823300\n",
            "total step : 306 \n",
            "error : 0.455613, accuarcy : 0.823600\n",
            "total step : 307 \n",
            "error : 0.454871, accuarcy : 0.823700\n",
            "total step : 308 \n",
            "error : 0.454134, accuarcy : 0.824000\n",
            "total step : 309 \n",
            "error : 0.453400, accuarcy : 0.824000\n",
            "total step : 310 \n",
            "error : 0.452671, accuarcy : 0.824100\n",
            "total step : 311 \n",
            "error : 0.451945, accuarcy : 0.824300\n",
            "total step : 312 \n",
            "error : 0.451223, accuarcy : 0.824400\n",
            "total step : 313 \n",
            "error : 0.450505, accuarcy : 0.824500\n",
            "total step : 314 \n",
            "error : 0.449791, accuarcy : 0.825000\n",
            "total step : 315 \n",
            "error : 0.449080, accuarcy : 0.825400\n",
            "total step : 316 \n",
            "error : 0.448374, accuarcy : 0.825600\n",
            "total step : 317 \n",
            "error : 0.447670, accuarcy : 0.825800\n",
            "total step : 318 \n",
            "error : 0.446971, accuarcy : 0.825600\n",
            "total step : 319 \n",
            "error : 0.446275, accuarcy : 0.825700\n",
            "total step : 320 \n",
            "error : 0.445583, accuarcy : 0.825900\n",
            "total step : 321 \n",
            "error : 0.444894, accuarcy : 0.826200\n",
            "total step : 322 \n",
            "error : 0.444209, accuarcy : 0.826300\n",
            "total step : 323 \n",
            "error : 0.443527, accuarcy : 0.826400\n",
            "total step : 324 \n",
            "error : 0.442849, accuarcy : 0.826600\n",
            "total step : 325 \n",
            "error : 0.442174, accuarcy : 0.826700\n",
            "total step : 326 \n",
            "error : 0.441503, accuarcy : 0.827100\n",
            "total step : 327 \n",
            "error : 0.440835, accuarcy : 0.827000\n",
            "total step : 328 \n",
            "error : 0.440170, accuarcy : 0.827300\n",
            "total step : 329 \n",
            "error : 0.439509, accuarcy : 0.827700\n",
            "total step : 330 \n",
            "error : 0.438851, accuarcy : 0.827600\n",
            "total step : 331 \n",
            "error : 0.438197, accuarcy : 0.827600\n",
            "total step : 332 \n",
            "error : 0.437546, accuarcy : 0.828000\n",
            "total step : 333 \n",
            "error : 0.436898, accuarcy : 0.828100\n",
            "total step : 334 \n",
            "error : 0.436253, accuarcy : 0.828200\n",
            "total step : 335 \n",
            "error : 0.435611, accuarcy : 0.828600\n",
            "total step : 336 \n",
            "error : 0.434973, accuarcy : 0.828500\n",
            "total step : 337 \n",
            "error : 0.434337, accuarcy : 0.828600\n",
            "total step : 338 \n",
            "error : 0.433705, accuarcy : 0.828900\n",
            "total step : 339 \n",
            "error : 0.433076, accuarcy : 0.829100\n",
            "total step : 340 \n",
            "error : 0.432450, accuarcy : 0.829200\n",
            "total step : 341 \n",
            "error : 0.431827, accuarcy : 0.829500\n",
            "total step : 342 \n",
            "error : 0.431207, accuarcy : 0.829600\n",
            "total step : 343 \n",
            "error : 0.430590, accuarcy : 0.829800\n",
            "total step : 344 \n",
            "error : 0.429976, accuarcy : 0.829900\n",
            "total step : 345 \n",
            "error : 0.429365, accuarcy : 0.829900\n",
            "total step : 346 \n",
            "error : 0.428757, accuarcy : 0.830300\n",
            "total step : 347 \n",
            "error : 0.428152, accuarcy : 0.830900\n",
            "total step : 348 \n",
            "error : 0.427550, accuarcy : 0.831100\n",
            "total step : 349 \n",
            "error : 0.426951, accuarcy : 0.831300\n",
            "total step : 350 \n",
            "error : 0.426354, accuarcy : 0.831300\n",
            "total step : 351 \n",
            "error : 0.425761, accuarcy : 0.831400\n",
            "total step : 352 \n",
            "error : 0.425170, accuarcy : 0.831500\n",
            "total step : 353 \n",
            "error : 0.424582, accuarcy : 0.831700\n",
            "total step : 354 \n",
            "error : 0.423997, accuarcy : 0.831800\n",
            "total step : 355 \n",
            "error : 0.423414, accuarcy : 0.831800\n",
            "total step : 356 \n",
            "error : 0.422834, accuarcy : 0.831800\n",
            "total step : 357 \n",
            "error : 0.422257, accuarcy : 0.831900\n",
            "total step : 358 \n",
            "error : 0.421683, accuarcy : 0.831900\n",
            "total step : 359 \n",
            "error : 0.421111, accuarcy : 0.832100\n",
            "total step : 360 \n",
            "error : 0.420542, accuarcy : 0.832100\n",
            "total step : 361 \n",
            "error : 0.419976, accuarcy : 0.832200\n",
            "total step : 362 \n",
            "error : 0.419412, accuarcy : 0.832700\n",
            "total step : 363 \n",
            "error : 0.418851, accuarcy : 0.832800\n",
            "total step : 364 \n",
            "error : 0.418292, accuarcy : 0.832800\n",
            "total step : 365 \n",
            "error : 0.417736, accuarcy : 0.832900\n",
            "total step : 366 \n",
            "error : 0.417183, accuarcy : 0.832900\n",
            "total step : 367 \n",
            "error : 0.416632, accuarcy : 0.833000\n",
            "total step : 368 \n",
            "error : 0.416083, accuarcy : 0.833400\n",
            "total step : 369 \n",
            "error : 0.415537, accuarcy : 0.833300\n",
            "total step : 370 \n",
            "error : 0.414994, accuarcy : 0.833600\n",
            "total step : 371 \n",
            "error : 0.414452, accuarcy : 0.833500\n",
            "total step : 372 \n",
            "error : 0.413914, accuarcy : 0.833500\n",
            "total step : 373 \n",
            "error : 0.413378, accuarcy : 0.833600\n",
            "total step : 374 \n",
            "error : 0.412844, accuarcy : 0.833600\n",
            "total step : 375 \n",
            "error : 0.412312, accuarcy : 0.833700\n",
            "total step : 376 \n",
            "error : 0.411783, accuarcy : 0.833700\n",
            "total step : 377 \n",
            "error : 0.411256, accuarcy : 0.833700\n",
            "total step : 378 \n",
            "error : 0.410732, accuarcy : 0.833800\n",
            "total step : 379 \n",
            "error : 0.410210, accuarcy : 0.834000\n",
            "total step : 380 \n",
            "error : 0.409690, accuarcy : 0.834200\n",
            "total step : 381 \n",
            "error : 0.409173, accuarcy : 0.834300\n",
            "total step : 382 \n",
            "error : 0.408658, accuarcy : 0.834300\n",
            "total step : 383 \n",
            "error : 0.408145, accuarcy : 0.834500\n",
            "total step : 384 \n",
            "error : 0.407634, accuarcy : 0.834600\n",
            "total step : 385 \n",
            "error : 0.407125, accuarcy : 0.834900\n",
            "total step : 386 \n",
            "error : 0.406619, accuarcy : 0.835200\n",
            "total step : 387 \n",
            "error : 0.406115, accuarcy : 0.835200\n",
            "total step : 388 \n",
            "error : 0.405613, accuarcy : 0.835300\n",
            "total step : 389 \n",
            "error : 0.405113, accuarcy : 0.835400\n",
            "total step : 390 \n",
            "error : 0.404616, accuarcy : 0.835500\n",
            "total step : 391 \n",
            "error : 0.404120, accuarcy : 0.835500\n",
            "total step : 392 \n",
            "error : 0.403627, accuarcy : 0.835600\n",
            "total step : 393 \n",
            "error : 0.403136, accuarcy : 0.835700\n",
            "total step : 394 \n",
            "error : 0.402647, accuarcy : 0.835800\n",
            "total step : 395 \n",
            "error : 0.402160, accuarcy : 0.835900\n",
            "total step : 396 \n",
            "error : 0.401675, accuarcy : 0.835900\n",
            "total step : 397 \n",
            "error : 0.401192, accuarcy : 0.836000\n",
            "total step : 398 \n",
            "error : 0.400711, accuarcy : 0.836300\n",
            "total step : 399 \n",
            "error : 0.400232, accuarcy : 0.836600\n",
            "total step : 400 \n",
            "error : 0.399755, accuarcy : 0.836800\n",
            "total step : 401 \n",
            "error : 0.399280, accuarcy : 0.836700\n",
            "total step : 402 \n",
            "error : 0.398808, accuarcy : 0.836900\n",
            "total step : 403 \n",
            "error : 0.398337, accuarcy : 0.837200\n",
            "total step : 404 \n",
            "error : 0.397868, accuarcy : 0.837400\n",
            "total step : 405 \n",
            "error : 0.397401, accuarcy : 0.837500\n",
            "total step : 406 \n",
            "error : 0.396936, accuarcy : 0.837600\n",
            "total step : 407 \n",
            "error : 0.396473, accuarcy : 0.837500\n",
            "total step : 408 \n",
            "error : 0.396012, accuarcy : 0.837400\n",
            "total step : 409 \n",
            "error : 0.395552, accuarcy : 0.837600\n",
            "total step : 410 \n",
            "error : 0.395095, accuarcy : 0.837600\n",
            "total step : 411 \n",
            "error : 0.394639, accuarcy : 0.837600\n",
            "total step : 412 \n",
            "error : 0.394186, accuarcy : 0.837600\n",
            "total step : 413 \n",
            "error : 0.393734, accuarcy : 0.837600\n",
            "total step : 414 \n",
            "error : 0.393284, accuarcy : 0.837700\n",
            "total step : 415 \n",
            "error : 0.392836, accuarcy : 0.837800\n",
            "total step : 416 \n",
            "error : 0.392389, accuarcy : 0.838000\n",
            "total step : 417 \n",
            "error : 0.391945, accuarcy : 0.838100\n",
            "total step : 418 \n",
            "error : 0.391502, accuarcy : 0.838400\n",
            "total step : 419 \n",
            "error : 0.391061, accuarcy : 0.838800\n",
            "total step : 420 \n",
            "error : 0.390622, accuarcy : 0.839100\n",
            "total step : 421 \n",
            "error : 0.390185, accuarcy : 0.839100\n",
            "total step : 422 \n",
            "error : 0.389749, accuarcy : 0.839400\n",
            "total step : 423 \n",
            "error : 0.389315, accuarcy : 0.839600\n",
            "total step : 424 \n",
            "error : 0.388883, accuarcy : 0.839600\n",
            "total step : 425 \n",
            "error : 0.388453, accuarcy : 0.839800\n",
            "total step : 426 \n",
            "error : 0.388024, accuarcy : 0.839800\n",
            "total step : 427 \n",
            "error : 0.387597, accuarcy : 0.840100\n",
            "total step : 428 \n",
            "error : 0.387171, accuarcy : 0.840200\n",
            "total step : 429 \n",
            "error : 0.386748, accuarcy : 0.840300\n",
            "total step : 430 \n",
            "error : 0.386326, accuarcy : 0.840400\n",
            "total step : 431 \n",
            "error : 0.385905, accuarcy : 0.840500\n",
            "total step : 432 \n",
            "error : 0.385487, accuarcy : 0.840700\n",
            "total step : 433 \n",
            "error : 0.385070, accuarcy : 0.840900\n",
            "total step : 434 \n",
            "error : 0.384654, accuarcy : 0.841200\n",
            "total step : 435 \n",
            "error : 0.384240, accuarcy : 0.841200\n",
            "total step : 436 \n",
            "error : 0.383828, accuarcy : 0.841200\n",
            "total step : 437 \n",
            "error : 0.383417, accuarcy : 0.841500\n",
            "total step : 438 \n",
            "error : 0.383008, accuarcy : 0.841500\n",
            "total step : 439 \n",
            "error : 0.382601, accuarcy : 0.841500\n",
            "total step : 440 \n",
            "error : 0.382195, accuarcy : 0.841700\n",
            "total step : 441 \n",
            "error : 0.381791, accuarcy : 0.841700\n",
            "total step : 442 \n",
            "error : 0.381388, accuarcy : 0.841800\n",
            "total step : 443 \n",
            "error : 0.380987, accuarcy : 0.841900\n",
            "total step : 444 \n",
            "error : 0.380587, accuarcy : 0.841900\n",
            "total step : 445 \n",
            "error : 0.380189, accuarcy : 0.842100\n",
            "total step : 446 \n",
            "error : 0.379792, accuarcy : 0.842300\n",
            "total step : 447 \n",
            "error : 0.379397, accuarcy : 0.842400\n",
            "total step : 448 \n",
            "error : 0.379004, accuarcy : 0.842500\n",
            "total step : 449 \n",
            "error : 0.378611, accuarcy : 0.842700\n",
            "total step : 450 \n",
            "error : 0.378221, accuarcy : 0.842900\n",
            "total step : 451 \n",
            "error : 0.377832, accuarcy : 0.843100\n",
            "total step : 452 \n",
            "error : 0.377444, accuarcy : 0.843200\n",
            "total step : 453 \n",
            "error : 0.377058, accuarcy : 0.843300\n",
            "total step : 454 \n",
            "error : 0.376673, accuarcy : 0.843400\n",
            "total step : 455 \n",
            "error : 0.376289, accuarcy : 0.843600\n",
            "total step : 456 \n",
            "error : 0.375907, accuarcy : 0.843700\n",
            "total step : 457 \n",
            "error : 0.375527, accuarcy : 0.843700\n",
            "total step : 458 \n",
            "error : 0.375148, accuarcy : 0.843800\n",
            "total step : 459 \n",
            "error : 0.374770, accuarcy : 0.843900\n",
            "total step : 460 \n",
            "error : 0.374394, accuarcy : 0.844000\n",
            "total step : 461 \n",
            "error : 0.374019, accuarcy : 0.844100\n",
            "total step : 462 \n",
            "error : 0.373645, accuarcy : 0.844300\n",
            "total step : 463 \n",
            "error : 0.373273, accuarcy : 0.844400\n",
            "total step : 464 \n",
            "error : 0.372902, accuarcy : 0.844500\n",
            "total step : 465 \n",
            "error : 0.372533, accuarcy : 0.844500\n",
            "total step : 466 \n",
            "error : 0.372165, accuarcy : 0.844600\n",
            "total step : 467 \n",
            "error : 0.371798, accuarcy : 0.844800\n",
            "total step : 468 \n",
            "error : 0.371433, accuarcy : 0.845000\n",
            "total step : 469 \n",
            "error : 0.371069, accuarcy : 0.845200\n",
            "total step : 470 \n",
            "error : 0.370706, accuarcy : 0.845300\n",
            "total step : 471 \n",
            "error : 0.370345, accuarcy : 0.845400\n",
            "total step : 472 \n",
            "error : 0.369985, accuarcy : 0.845400\n",
            "total step : 473 \n",
            "error : 0.369626, accuarcy : 0.845400\n",
            "total step : 474 \n",
            "error : 0.369268, accuarcy : 0.845400\n",
            "total step : 475 \n",
            "error : 0.368912, accuarcy : 0.845600\n",
            "total step : 476 \n",
            "error : 0.368557, accuarcy : 0.845700\n",
            "total step : 477 \n",
            "error : 0.368204, accuarcy : 0.845700\n",
            "total step : 478 \n",
            "error : 0.367851, accuarcy : 0.845900\n",
            "total step : 479 \n",
            "error : 0.367500, accuarcy : 0.846000\n",
            "total step : 480 \n",
            "error : 0.367150, accuarcy : 0.846000\n",
            "total step : 481 \n",
            "error : 0.366802, accuarcy : 0.846100\n",
            "total step : 482 \n",
            "error : 0.366454, accuarcy : 0.846100\n",
            "total step : 483 \n",
            "error : 0.366108, accuarcy : 0.846000\n",
            "total step : 484 \n",
            "error : 0.365763, accuarcy : 0.846200\n",
            "total step : 485 \n",
            "error : 0.365419, accuarcy : 0.846200\n",
            "total step : 486 \n",
            "error : 0.365077, accuarcy : 0.846500\n",
            "total step : 487 \n",
            "error : 0.364736, accuarcy : 0.846400\n",
            "total step : 488 \n",
            "error : 0.364395, accuarcy : 0.846600\n",
            "total step : 489 \n",
            "error : 0.364057, accuarcy : 0.846600\n",
            "total step : 490 \n",
            "error : 0.363719, accuarcy : 0.846700\n",
            "total step : 491 \n",
            "error : 0.363382, accuarcy : 0.846700\n",
            "total step : 492 \n",
            "error : 0.363047, accuarcy : 0.846800\n",
            "total step : 493 \n",
            "error : 0.362713, accuarcy : 0.846800\n",
            "total step : 494 \n",
            "error : 0.362380, accuarcy : 0.847000\n",
            "total step : 495 \n",
            "error : 0.362048, accuarcy : 0.847100\n",
            "total step : 496 \n",
            "error : 0.361717, accuarcy : 0.847200\n",
            "total step : 497 \n",
            "error : 0.361388, accuarcy : 0.847300\n",
            "total step : 498 \n",
            "error : 0.361059, accuarcy : 0.847400\n",
            "total step : 499 \n",
            "error : 0.360732, accuarcy : 0.847400\n",
            "total step : 500 \n",
            "error : 0.360406, accuarcy : 0.847500\n",
            "total step : 501 \n",
            "error : 0.360081, accuarcy : 0.847600\n",
            "total step : 502 \n",
            "error : 0.359757, accuarcy : 0.847600\n",
            "total step : 503 \n",
            "error : 0.359434, accuarcy : 0.847700\n",
            "total step : 504 \n",
            "error : 0.359112, accuarcy : 0.848000\n",
            "total step : 505 \n",
            "error : 0.358792, accuarcy : 0.848000\n",
            "total step : 506 \n",
            "error : 0.358472, accuarcy : 0.848100\n",
            "total step : 507 \n",
            "error : 0.358154, accuarcy : 0.848300\n",
            "total step : 508 \n",
            "error : 0.357836, accuarcy : 0.848400\n",
            "total step : 509 \n",
            "error : 0.357520, accuarcy : 0.848500\n",
            "total step : 510 \n",
            "error : 0.357205, accuarcy : 0.848600\n",
            "total step : 511 \n",
            "error : 0.356890, accuarcy : 0.848800\n",
            "total step : 512 \n",
            "error : 0.356577, accuarcy : 0.849000\n",
            "total step : 513 \n",
            "error : 0.356265, accuarcy : 0.849000\n",
            "total step : 514 \n",
            "error : 0.355954, accuarcy : 0.849000\n",
            "total step : 515 \n",
            "error : 0.355644, accuarcy : 0.849100\n",
            "total step : 516 \n",
            "error : 0.355335, accuarcy : 0.849100\n",
            "total step : 517 \n",
            "error : 0.355027, accuarcy : 0.849300\n",
            "total step : 518 \n",
            "error : 0.354721, accuarcy : 0.849400\n",
            "total step : 519 \n",
            "error : 0.354415, accuarcy : 0.849600\n",
            "total step : 520 \n",
            "error : 0.354110, accuarcy : 0.849700\n",
            "total step : 521 \n",
            "error : 0.353806, accuarcy : 0.849700\n",
            "total step : 522 \n",
            "error : 0.353503, accuarcy : 0.849800\n",
            "total step : 523 \n",
            "error : 0.353201, accuarcy : 0.849700\n",
            "total step : 524 \n",
            "error : 0.352901, accuarcy : 0.849800\n",
            "total step : 525 \n",
            "error : 0.352601, accuarcy : 0.849800\n",
            "total step : 526 \n",
            "error : 0.352302, accuarcy : 0.849700\n",
            "total step : 527 \n",
            "error : 0.352004, accuarcy : 0.849800\n",
            "total step : 528 \n",
            "error : 0.351707, accuarcy : 0.849800\n",
            "total step : 529 \n",
            "error : 0.351411, accuarcy : 0.849900\n",
            "total step : 530 \n",
            "error : 0.351116, accuarcy : 0.849800\n",
            "total step : 531 \n",
            "error : 0.350822, accuarcy : 0.849900\n",
            "total step : 532 \n",
            "error : 0.350529, accuarcy : 0.849900\n",
            "total step : 533 \n",
            "error : 0.350237, accuarcy : 0.849900\n",
            "total step : 534 \n",
            "error : 0.349946, accuarcy : 0.850000\n",
            "total step : 535 \n",
            "error : 0.349656, accuarcy : 0.850000\n",
            "total step : 536 \n",
            "error : 0.349366, accuarcy : 0.850500\n"
          ]
        }
      ],
      "source": [
        "train_5X, train_5y = make_sample(idx = 5) # idx = target number\n",
        "train_5X = np.insert(train_5X, 0, 1, axis=1) # bias 추가\n",
        "w5,J5_history, ACC5_history = train(train_5X, train_5y,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total step : 1 \n",
            "error : 4.465913, accuarcy : 0.294900\n",
            "total step : 2 \n",
            "error : 4.184590, accuarcy : 0.329200\n",
            "total step : 3 \n",
            "error : 3.922252, accuarcy : 0.361600\n",
            "total step : 4 \n",
            "error : 3.685758, accuarcy : 0.391800\n",
            "total step : 5 \n",
            "error : 3.479538, accuarcy : 0.422300\n",
            "total step : 6 \n",
            "error : 3.303322, accuarcy : 0.449800\n",
            "total step : 7 \n",
            "error : 3.153100, accuarcy : 0.477200\n",
            "total step : 8 \n",
            "error : 3.023312, accuarcy : 0.499000\n",
            "total step : 9 \n",
            "error : 2.909161, accuarcy : 0.518500\n",
            "total step : 10 \n",
            "error : 2.807577, accuarcy : 0.536700\n",
            "total step : 11 \n",
            "error : 2.716441, accuarcy : 0.554500\n",
            "total step : 12 \n",
            "error : 2.633873, accuarcy : 0.570600\n",
            "total step : 13 \n",
            "error : 2.558173, accuarcy : 0.584300\n",
            "total step : 14 \n",
            "error : 2.487895, accuarcy : 0.595500\n",
            "total step : 15 \n",
            "error : 2.421872, accuarcy : 0.606300\n",
            "total step : 16 \n",
            "error : 2.359214, accuarcy : 0.614900\n",
            "total step : 17 \n",
            "error : 2.299288, accuarcy : 0.623300\n",
            "total step : 18 \n",
            "error : 2.241665, accuarcy : 0.631800\n",
            "total step : 19 \n",
            "error : 2.186065, accuarcy : 0.640300\n",
            "total step : 20 \n",
            "error : 2.132311, accuarcy : 0.646900\n",
            "total step : 21 \n",
            "error : 2.080299, accuarcy : 0.653200\n",
            "total step : 22 \n",
            "error : 2.029966, accuarcy : 0.658000\n",
            "total step : 23 \n",
            "error : 1.981274, accuarcy : 0.664300\n",
            "total step : 24 \n",
            "error : 1.934195, accuarcy : 0.669500\n",
            "total step : 25 \n",
            "error : 1.888703, accuarcy : 0.675000\n",
            "total step : 26 \n",
            "error : 1.844770, accuarcy : 0.680800\n",
            "total step : 27 \n",
            "error : 1.802357, accuarcy : 0.685800\n",
            "total step : 28 \n",
            "error : 1.761418, accuarcy : 0.690100\n",
            "total step : 29 \n",
            "error : 1.721899, accuarcy : 0.694700\n",
            "total step : 30 \n",
            "error : 1.683739, accuarcy : 0.698200\n",
            "total step : 31 \n",
            "error : 1.646871, accuarcy : 0.701900\n",
            "total step : 32 \n",
            "error : 1.611229, accuarcy : 0.706200\n",
            "total step : 33 \n",
            "error : 1.576744, accuarcy : 0.710100\n",
            "total step : 34 \n",
            "error : 1.543354, accuarcy : 0.715000\n",
            "total step : 35 \n",
            "error : 1.511000, accuarcy : 0.719800\n",
            "total step : 36 \n",
            "error : 1.479629, accuarcy : 0.723200\n",
            "total step : 37 \n",
            "error : 1.449195, accuarcy : 0.726300\n",
            "total step : 38 \n",
            "error : 1.419655, accuarcy : 0.729100\n",
            "total step : 39 \n",
            "error : 1.390971, accuarcy : 0.731900\n",
            "total step : 40 \n",
            "error : 1.363111, accuarcy : 0.734500\n",
            "total step : 41 \n",
            "error : 1.336042, accuarcy : 0.738000\n",
            "total step : 42 \n",
            "error : 1.309737, accuarcy : 0.741200\n",
            "total step : 43 \n",
            "error : 1.284169, accuarcy : 0.743300\n",
            "total step : 44 \n",
            "error : 1.259314, accuarcy : 0.746700\n",
            "total step : 45 \n",
            "error : 1.235149, accuarcy : 0.751100\n",
            "total step : 46 \n",
            "error : 1.211651, accuarcy : 0.753900\n",
            "total step : 47 \n",
            "error : 1.188801, accuarcy : 0.756800\n",
            "total step : 48 \n",
            "error : 1.166581, accuarcy : 0.759300\n",
            "total step : 49 \n",
            "error : 1.144974, accuarcy : 0.762300\n",
            "total step : 50 \n",
            "error : 1.123962, accuarcy : 0.765100\n",
            "total step : 51 \n",
            "error : 1.103530, accuarcy : 0.767700\n",
            "total step : 52 \n",
            "error : 1.083664, accuarcy : 0.770300\n",
            "total step : 53 \n",
            "error : 1.064349, accuarcy : 0.772300\n",
            "total step : 54 \n",
            "error : 1.045571, accuarcy : 0.775300\n",
            "total step : 55 \n",
            "error : 1.027316, accuarcy : 0.777600\n",
            "total step : 56 \n",
            "error : 1.009571, accuarcy : 0.781000\n",
            "total step : 57 \n",
            "error : 0.992322, accuarcy : 0.782700\n",
            "total step : 58 \n",
            "error : 0.975556, accuarcy : 0.785700\n",
            "total step : 59 \n",
            "error : 0.959258, accuarcy : 0.788500\n",
            "total step : 60 \n",
            "error : 0.943415, accuarcy : 0.791900\n",
            "total step : 61 \n",
            "error : 0.928015, accuarcy : 0.794600\n",
            "total step : 62 \n",
            "error : 0.913044, accuarcy : 0.796000\n",
            "total step : 63 \n",
            "error : 0.898489, accuarcy : 0.798800\n",
            "total step : 64 \n",
            "error : 0.884338, accuarcy : 0.801700\n",
            "total step : 65 \n",
            "error : 0.870578, accuarcy : 0.804200\n",
            "total step : 66 \n",
            "error : 0.857197, accuarcy : 0.806200\n",
            "total step : 67 \n",
            "error : 0.844184, accuarcy : 0.808700\n",
            "total step : 68 \n",
            "error : 0.831527, accuarcy : 0.810300\n",
            "total step : 69 \n",
            "error : 0.819216, accuarcy : 0.811300\n",
            "total step : 70 \n",
            "error : 0.807239, accuarcy : 0.813800\n",
            "total step : 71 \n",
            "error : 0.795587, accuarcy : 0.816000\n",
            "total step : 72 \n",
            "error : 0.784250, accuarcy : 0.817500\n",
            "total step : 73 \n",
            "error : 0.773218, accuarcy : 0.818100\n",
            "total step : 74 \n",
            "error : 0.762481, accuarcy : 0.819700\n",
            "total step : 75 \n",
            "error : 0.752031, accuarcy : 0.821400\n",
            "total step : 76 \n",
            "error : 0.741859, accuarcy : 0.823000\n",
            "total step : 77 \n",
            "error : 0.731956, accuarcy : 0.824600\n",
            "total step : 78 \n",
            "error : 0.722314, accuarcy : 0.825800\n",
            "total step : 79 \n",
            "error : 0.712926, accuarcy : 0.826700\n",
            "total step : 80 \n",
            "error : 0.703783, accuarcy : 0.827600\n",
            "total step : 81 \n",
            "error : 0.694878, accuarcy : 0.828900\n",
            "total step : 82 \n",
            "error : 0.686204, accuarcy : 0.830400\n",
            "total step : 83 \n",
            "error : 0.677753, accuarcy : 0.831700\n",
            "total step : 84 \n",
            "error : 0.669519, accuarcy : 0.832500\n",
            "total step : 85 \n",
            "error : 0.661495, accuarcy : 0.833300\n",
            "total step : 86 \n",
            "error : 0.653675, accuarcy : 0.834100\n",
            "total step : 87 \n",
            "error : 0.646052, accuarcy : 0.835600\n",
            "total step : 88 \n",
            "error : 0.638619, accuarcy : 0.837100\n",
            "total step : 89 \n",
            "error : 0.631371, accuarcy : 0.838700\n",
            "total step : 90 \n",
            "error : 0.624302, accuarcy : 0.839400\n",
            "total step : 91 \n",
            "error : 0.617406, accuarcy : 0.840800\n",
            "total step : 92 \n",
            "error : 0.610676, accuarcy : 0.841800\n",
            "total step : 93 \n",
            "error : 0.604109, accuarcy : 0.842600\n",
            "total step : 94 \n",
            "error : 0.597697, accuarcy : 0.843900\n",
            "total step : 95 \n",
            "error : 0.591436, accuarcy : 0.845500\n",
            "total step : 96 \n",
            "error : 0.585321, accuarcy : 0.845700\n",
            "total step : 97 \n",
            "error : 0.579347, accuarcy : 0.846800\n",
            "total step : 98 \n",
            "error : 0.573509, accuarcy : 0.847700\n",
            "total step : 99 \n",
            "error : 0.567803, accuarcy : 0.848400\n",
            "total step : 100 \n",
            "error : 0.562223, accuarcy : 0.850000\n",
            "total step : 101 \n",
            "error : 0.556767, accuarcy : 0.851000\n"
          ]
        }
      ],
      "source": [
        "train_6X, train_6y = make_sample(idx = 6) # idx = target number\n",
        "train_6X = np.insert(train_6X, 0, 1, axis=1) # bias 추가\n",
        "w6,J6_history, ACC6_history = train(train_6X, train_6y,6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total step : 1 \n",
            "error : 3.418792, accuarcy : 0.453800\n",
            "total step : 2 \n",
            "error : 3.304486, accuarcy : 0.458900\n",
            "total step : 3 \n",
            "error : 3.194500, accuarcy : 0.465000\n",
            "total step : 4 \n",
            "error : 3.088676, accuarcy : 0.470500\n",
            "total step : 5 \n",
            "error : 2.986866, accuarcy : 0.477800\n",
            "total step : 6 \n",
            "error : 2.888932, accuarcy : 0.484200\n",
            "total step : 7 \n",
            "error : 2.794749, accuarcy : 0.491600\n",
            "total step : 8 \n",
            "error : 2.704204, accuarcy : 0.500600\n",
            "total step : 9 \n",
            "error : 2.617196, accuarcy : 0.508300\n",
            "total step : 10 \n",
            "error : 2.533631, accuarcy : 0.516200\n",
            "total step : 11 \n",
            "error : 2.453417, accuarcy : 0.524800\n",
            "total step : 12 \n",
            "error : 2.376465, accuarcy : 0.532700\n",
            "total step : 13 \n",
            "error : 2.302682, accuarcy : 0.542000\n",
            "total step : 14 \n",
            "error : 2.231973, accuarcy : 0.548800\n",
            "total step : 15 \n",
            "error : 2.164238, accuarcy : 0.555800\n",
            "total step : 16 \n",
            "error : 2.099376, accuarcy : 0.563300\n",
            "total step : 17 \n",
            "error : 2.037283, accuarcy : 0.569700\n",
            "total step : 18 \n",
            "error : 1.977860, accuarcy : 0.576900\n",
            "total step : 19 \n",
            "error : 1.921008, accuarcy : 0.583300\n",
            "total step : 20 \n",
            "error : 1.866634, accuarcy : 0.590700\n",
            "total step : 21 \n",
            "error : 1.814646, accuarcy : 0.597100\n",
            "total step : 22 \n",
            "error : 1.764954, accuarcy : 0.603000\n",
            "total step : 23 \n",
            "error : 1.717474, accuarcy : 0.608500\n",
            "total step : 24 \n",
            "error : 1.672121, accuarcy : 0.614800\n",
            "total step : 25 \n",
            "error : 1.628811, accuarcy : 0.621100\n",
            "total step : 26 \n",
            "error : 1.587461, accuarcy : 0.627200\n",
            "total step : 27 \n",
            "error : 1.547989, accuarcy : 0.633800\n",
            "total step : 28 \n",
            "error : 1.510311, accuarcy : 0.638400\n",
            "total step : 29 \n",
            "error : 1.474342, accuarcy : 0.643700\n",
            "total step : 30 \n",
            "error : 1.439998, accuarcy : 0.648400\n",
            "total step : 31 \n",
            "error : 1.407195, accuarcy : 0.652800\n",
            "total step : 32 \n",
            "error : 1.375851, accuarcy : 0.657900\n",
            "total step : 33 \n",
            "error : 1.345884, accuarcy : 0.661500\n",
            "total step : 34 \n",
            "error : 1.317218, accuarcy : 0.665700\n",
            "total step : 35 \n",
            "error : 1.289781, accuarcy : 0.669100\n",
            "total step : 36 \n",
            "error : 1.263504, accuarcy : 0.673100\n",
            "total step : 37 \n",
            "error : 1.238323, accuarcy : 0.677200\n",
            "total step : 38 \n",
            "error : 1.214178, accuarcy : 0.681400\n",
            "total step : 39 \n",
            "error : 1.191013, accuarcy : 0.683600\n",
            "total step : 40 \n",
            "error : 1.168775, accuarcy : 0.687400\n",
            "total step : 41 \n",
            "error : 1.147416, accuarcy : 0.691000\n",
            "total step : 42 \n",
            "error : 1.126889, accuarcy : 0.694600\n",
            "total step : 43 \n",
            "error : 1.107152, accuarcy : 0.697800\n",
            "total step : 44 \n",
            "error : 1.088165, accuarcy : 0.700600\n",
            "total step : 45 \n",
            "error : 1.069891, accuarcy : 0.704000\n",
            "total step : 46 \n",
            "error : 1.052296, accuarcy : 0.707400\n",
            "total step : 47 \n",
            "error : 1.035347, accuarcy : 0.710400\n",
            "total step : 48 \n",
            "error : 1.019014, accuarcy : 0.713600\n",
            "total step : 49 \n",
            "error : 1.003269, accuarcy : 0.717100\n",
            "total step : 50 \n",
            "error : 0.988084, accuarcy : 0.720300\n",
            "total step : 51 \n",
            "error : 0.973435, accuarcy : 0.723400\n",
            "total step : 52 \n",
            "error : 0.959297, accuarcy : 0.726100\n",
            "total step : 53 \n",
            "error : 0.945648, accuarcy : 0.728900\n",
            "total step : 54 \n",
            "error : 0.932465, accuarcy : 0.731400\n",
            "total step : 55 \n",
            "error : 0.919727, accuarcy : 0.733600\n",
            "total step : 56 \n",
            "error : 0.907414, accuarcy : 0.736400\n",
            "total step : 57 \n",
            "error : 0.895507, accuarcy : 0.739400\n",
            "total step : 58 \n",
            "error : 0.883988, accuarcy : 0.741400\n",
            "total step : 59 \n",
            "error : 0.872839, accuarcy : 0.744000\n",
            "total step : 60 \n",
            "error : 0.862044, accuarcy : 0.746100\n",
            "total step : 61 \n",
            "error : 0.851585, accuarcy : 0.748600\n",
            "total step : 62 \n",
            "error : 0.841448, accuarcy : 0.751300\n",
            "total step : 63 \n",
            "error : 0.831618, accuarcy : 0.753100\n",
            "total step : 64 \n",
            "error : 0.822082, accuarcy : 0.755600\n",
            "total step : 65 \n",
            "error : 0.812827, accuarcy : 0.757100\n",
            "total step : 66 \n",
            "error : 0.803839, accuarcy : 0.759300\n",
            "total step : 67 \n",
            "error : 0.795107, accuarcy : 0.760800\n",
            "total step : 68 \n",
            "error : 0.786621, accuarcy : 0.762500\n",
            "total step : 69 \n",
            "error : 0.778368, accuarcy : 0.764100\n",
            "total step : 70 \n",
            "error : 0.770341, accuarcy : 0.765200\n",
            "total step : 71 \n",
            "error : 0.762528, accuarcy : 0.767300\n",
            "total step : 72 \n",
            "error : 0.754921, accuarcy : 0.768700\n",
            "total step : 73 \n",
            "error : 0.747511, accuarcy : 0.770700\n",
            "total step : 74 \n",
            "error : 0.740291, accuarcy : 0.773000\n",
            "total step : 75 \n",
            "error : 0.733253, accuarcy : 0.775100\n",
            "total step : 76 \n",
            "error : 0.726389, accuarcy : 0.777400\n",
            "total step : 77 \n",
            "error : 0.719693, accuarcy : 0.779000\n",
            "total step : 78 \n",
            "error : 0.713158, accuarcy : 0.780300\n",
            "total step : 79 \n",
            "error : 0.706778, accuarcy : 0.782000\n",
            "total step : 80 \n",
            "error : 0.700548, accuarcy : 0.783900\n",
            "total step : 81 \n",
            "error : 0.694461, accuarcy : 0.785600\n",
            "total step : 82 \n",
            "error : 0.688514, accuarcy : 0.787200\n",
            "total step : 83 \n",
            "error : 0.682700, accuarcy : 0.788100\n",
            "total step : 84 \n",
            "error : 0.677016, accuarcy : 0.790100\n",
            "total step : 85 \n",
            "error : 0.671456, accuarcy : 0.791300\n",
            "total step : 86 \n",
            "error : 0.666017, accuarcy : 0.792500\n",
            "total step : 87 \n",
            "error : 0.660694, accuarcy : 0.793800\n",
            "total step : 88 \n",
            "error : 0.655485, accuarcy : 0.795700\n",
            "total step : 89 \n",
            "error : 0.650385, accuarcy : 0.797000\n",
            "total step : 90 \n",
            "error : 0.645391, accuarcy : 0.798100\n",
            "total step : 91 \n",
            "error : 0.640499, accuarcy : 0.799300\n",
            "total step : 92 \n",
            "error : 0.635708, accuarcy : 0.800300\n",
            "total step : 93 \n",
            "error : 0.631013, accuarcy : 0.801200\n",
            "total step : 94 \n",
            "error : 0.626411, accuarcy : 0.802400\n",
            "total step : 95 \n",
            "error : 0.621901, accuarcy : 0.804000\n",
            "total step : 96 \n",
            "error : 0.617480, accuarcy : 0.805000\n",
            "total step : 97 \n",
            "error : 0.613145, accuarcy : 0.806300\n",
            "total step : 98 \n",
            "error : 0.608894, accuarcy : 0.807300\n",
            "total step : 99 \n",
            "error : 0.604724, accuarcy : 0.808100\n",
            "total step : 100 \n",
            "error : 0.600633, accuarcy : 0.809500\n",
            "total step : 101 \n",
            "error : 0.596620, accuarcy : 0.810300\n",
            "total step : 102 \n",
            "error : 0.592682, accuarcy : 0.812200\n",
            "total step : 103 \n",
            "error : 0.588817, accuarcy : 0.813100\n",
            "total step : 104 \n",
            "error : 0.585023, accuarcy : 0.814000\n",
            "total step : 105 \n",
            "error : 0.581299, accuarcy : 0.815200\n",
            "total step : 106 \n",
            "error : 0.577643, accuarcy : 0.816300\n",
            "total step : 107 \n",
            "error : 0.574052, accuarcy : 0.817500\n",
            "total step : 108 \n",
            "error : 0.570526, accuarcy : 0.818700\n",
            "total step : 109 \n",
            "error : 0.567063, accuarcy : 0.819500\n",
            "total step : 110 \n",
            "error : 0.563660, accuarcy : 0.821500\n",
            "total step : 111 \n",
            "error : 0.560318, accuarcy : 0.821800\n",
            "total step : 112 \n",
            "error : 0.557033, accuarcy : 0.823600\n",
            "total step : 113 \n",
            "error : 0.553806, accuarcy : 0.824700\n",
            "total step : 114 \n",
            "error : 0.550633, accuarcy : 0.825500\n",
            "total step : 115 \n",
            "error : 0.547515, accuarcy : 0.826500\n",
            "total step : 116 \n",
            "error : 0.544449, accuarcy : 0.827500\n",
            "total step : 117 \n",
            "error : 0.541435, accuarcy : 0.828600\n",
            "total step : 118 \n",
            "error : 0.538471, accuarcy : 0.829200\n",
            "total step : 119 \n",
            "error : 0.535556, accuarcy : 0.829800\n",
            "total step : 120 \n",
            "error : 0.532688, accuarcy : 0.830600\n",
            "total step : 121 \n",
            "error : 0.529868, accuarcy : 0.831300\n",
            "total step : 122 \n",
            "error : 0.527093, accuarcy : 0.832100\n",
            "total step : 123 \n",
            "error : 0.524362, accuarcy : 0.832900\n",
            "total step : 124 \n",
            "error : 0.521675, accuarcy : 0.833600\n",
            "total step : 125 \n",
            "error : 0.519031, accuarcy : 0.834500\n",
            "total step : 126 \n",
            "error : 0.516428, accuarcy : 0.834700\n",
            "total step : 127 \n",
            "error : 0.513865, accuarcy : 0.835700\n",
            "total step : 128 \n",
            "error : 0.511342, accuarcy : 0.836900\n",
            "total step : 129 \n",
            "error : 0.508857, accuarcy : 0.837900\n",
            "total step : 130 \n",
            "error : 0.506411, accuarcy : 0.838400\n",
            "total step : 131 \n",
            "error : 0.504001, accuarcy : 0.840000\n",
            "total step : 132 \n",
            "error : 0.501627, accuarcy : 0.840800\n",
            "total step : 133 \n",
            "error : 0.499288, accuarcy : 0.841400\n",
            "total step : 134 \n",
            "error : 0.496984, accuarcy : 0.842400\n",
            "total step : 135 \n",
            "error : 0.494714, accuarcy : 0.842700\n",
            "total step : 136 \n",
            "error : 0.492476, accuarcy : 0.843200\n",
            "total step : 137 \n",
            "error : 0.490270, accuarcy : 0.843600\n",
            "total step : 138 \n",
            "error : 0.488096, accuarcy : 0.844300\n",
            "total step : 139 \n",
            "error : 0.485953, accuarcy : 0.844800\n",
            "total step : 140 \n",
            "error : 0.483840, accuarcy : 0.845200\n",
            "total step : 141 \n",
            "error : 0.481756, accuarcy : 0.845900\n",
            "total step : 142 \n",
            "error : 0.479700, accuarcy : 0.846700\n",
            "total step : 143 \n",
            "error : 0.477673, accuarcy : 0.847500\n",
            "total step : 144 \n",
            "error : 0.475674, accuarcy : 0.848000\n",
            "total step : 145 \n",
            "error : 0.473701, accuarcy : 0.848500\n",
            "total step : 146 \n",
            "error : 0.471754, accuarcy : 0.849100\n",
            "total step : 147 \n",
            "error : 0.469834, accuarcy : 0.849500\n",
            "total step : 148 \n",
            "error : 0.467938, accuarcy : 0.850100\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_7X, train_7y = make_sample(idx = 7) # idx = target number\n",
        "train_7X = np.insert(train_7X, 0, 1, axis=1) # bias 추가\n",
        "w7,J7_history, ACC7_history = train(train_7X, train_7y,7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total step : 1 \n",
            "error : 3.007812, accuarcy : 0.312700\n",
            "total step : 2 \n",
            "error : 2.782736, accuarcy : 0.350400\n",
            "total step : 3 \n",
            "error : 2.621146, accuarcy : 0.385900\n",
            "total step : 4 \n",
            "error : 2.506950, accuarcy : 0.415100\n",
            "total step : 5 \n",
            "error : 2.425020, accuarcy : 0.439200\n",
            "total step : 6 \n",
            "error : 2.363474, accuarcy : 0.460000\n",
            "total step : 7 \n",
            "error : 2.314166, accuarcy : 0.472600\n",
            "total step : 8 \n",
            "error : 2.272029, accuarcy : 0.482300\n",
            "total step : 9 \n",
            "error : 2.234118, accuarcy : 0.490400\n",
            "total step : 10 \n",
            "error : 2.198796, accuarcy : 0.499000\n",
            "total step : 11 \n",
            "error : 2.165177, accuarcy : 0.505300\n",
            "total step : 12 \n",
            "error : 2.132792, accuarcy : 0.511300\n",
            "total step : 13 \n",
            "error : 2.101396, accuarcy : 0.516100\n",
            "total step : 14 \n",
            "error : 2.070857, accuarcy : 0.519200\n",
            "total step : 15 \n",
            "error : 2.041109, accuarcy : 0.523000\n",
            "total step : 16 \n",
            "error : 2.012112, accuarcy : 0.527800\n",
            "total step : 17 \n",
            "error : 1.983845, accuarcy : 0.531200\n",
            "total step : 18 \n",
            "error : 1.956293, accuarcy : 0.535100\n",
            "total step : 19 \n",
            "error : 1.929445, accuarcy : 0.539000\n",
            "total step : 20 \n",
            "error : 1.903290, accuarcy : 0.542200\n",
            "total step : 21 \n",
            "error : 1.877815, accuarcy : 0.544600\n",
            "total step : 22 \n",
            "error : 1.853009, accuarcy : 0.547700\n",
            "total step : 23 \n",
            "error : 1.828858, accuarcy : 0.550900\n",
            "total step : 24 \n",
            "error : 1.805347, accuarcy : 0.553100\n",
            "total step : 25 \n",
            "error : 1.782462, accuarcy : 0.557100\n",
            "total step : 26 \n",
            "error : 1.760186, accuarcy : 0.559500\n",
            "total step : 27 \n",
            "error : 1.738503, accuarcy : 0.562300\n",
            "total step : 28 \n",
            "error : 1.717397, accuarcy : 0.565600\n",
            "total step : 29 \n",
            "error : 1.696852, accuarcy : 0.568600\n",
            "total step : 30 \n",
            "error : 1.676853, accuarcy : 0.572700\n",
            "total step : 31 \n",
            "error : 1.657383, accuarcy : 0.575700\n",
            "total step : 32 \n",
            "error : 1.638428, accuarcy : 0.578600\n",
            "total step : 33 \n",
            "error : 1.619973, accuarcy : 0.582200\n",
            "total step : 34 \n",
            "error : 1.602004, accuarcy : 0.585200\n",
            "total step : 35 \n",
            "error : 1.584508, accuarcy : 0.588600\n",
            "total step : 36 \n",
            "error : 1.567470, accuarcy : 0.590800\n",
            "total step : 37 \n",
            "error : 1.550879, accuarcy : 0.593100\n",
            "total step : 38 \n",
            "error : 1.534722, accuarcy : 0.596600\n",
            "total step : 39 \n",
            "error : 1.518986, accuarcy : 0.598800\n",
            "total step : 40 \n",
            "error : 1.503660, accuarcy : 0.602300\n",
            "total step : 41 \n",
            "error : 1.488733, accuarcy : 0.604900\n",
            "total step : 42 \n",
            "error : 1.474193, accuarcy : 0.607500\n",
            "total step : 43 \n",
            "error : 1.460029, accuarcy : 0.610000\n",
            "total step : 44 \n",
            "error : 1.446230, accuarcy : 0.612500\n",
            "total step : 45 \n",
            "error : 1.432787, accuarcy : 0.615000\n",
            "total step : 46 \n",
            "error : 1.419688, accuarcy : 0.617100\n",
            "total step : 47 \n",
            "error : 1.406925, accuarcy : 0.619600\n",
            "total step : 48 \n",
            "error : 1.394487, accuarcy : 0.622000\n",
            "total step : 49 \n",
            "error : 1.382365, accuarcy : 0.624200\n",
            "total step : 50 \n",
            "error : 1.370550, accuarcy : 0.625900\n",
            "total step : 51 \n",
            "error : 1.359032, accuarcy : 0.629100\n",
            "total step : 52 \n",
            "error : 1.347803, accuarcy : 0.630300\n",
            "total step : 53 \n",
            "error : 1.336855, accuarcy : 0.632800\n",
            "total step : 54 \n",
            "error : 1.326178, accuarcy : 0.635200\n",
            "total step : 55 \n",
            "error : 1.315764, accuarcy : 0.637500\n",
            "total step : 56 \n",
            "error : 1.305606, accuarcy : 0.639700\n",
            "total step : 57 \n",
            "error : 1.295694, accuarcy : 0.642400\n",
            "total step : 58 \n",
            "error : 1.286022, accuarcy : 0.644800\n",
            "total step : 59 \n",
            "error : 1.276582, accuarcy : 0.646400\n",
            "total step : 60 \n",
            "error : 1.267365, accuarcy : 0.648600\n",
            "total step : 61 \n",
            "error : 1.258366, accuarcy : 0.651100\n",
            "total step : 62 \n",
            "error : 1.249575, accuarcy : 0.652600\n",
            "total step : 63 \n",
            "error : 1.240987, accuarcy : 0.655200\n",
            "total step : 64 \n",
            "error : 1.232593, accuarcy : 0.657600\n",
            "total step : 65 \n",
            "error : 1.224389, accuarcy : 0.659100\n",
            "total step : 66 \n",
            "error : 1.216366, accuarcy : 0.660900\n",
            "total step : 67 \n",
            "error : 1.208518, accuarcy : 0.662000\n",
            "total step : 68 \n",
            "error : 1.200841, accuarcy : 0.663900\n",
            "total step : 69 \n",
            "error : 1.193326, accuarcy : 0.666000\n",
            "total step : 70 \n",
            "error : 1.185969, accuarcy : 0.667700\n",
            "total step : 71 \n",
            "error : 1.178764, accuarcy : 0.669100\n",
            "total step : 72 \n",
            "error : 1.171706, accuarcy : 0.670000\n",
            "total step : 73 \n",
            "error : 1.164790, accuarcy : 0.671700\n",
            "total step : 74 \n",
            "error : 1.158010, accuarcy : 0.673700\n",
            "total step : 75 \n",
            "error : 1.151363, accuarcy : 0.674600\n",
            "total step : 76 \n",
            "error : 1.144843, accuarcy : 0.676000\n",
            "total step : 77 \n",
            "error : 1.138446, accuarcy : 0.676800\n",
            "total step : 78 \n",
            "error : 1.132168, accuarcy : 0.677900\n",
            "total step : 79 \n",
            "error : 1.126006, accuarcy : 0.678800\n",
            "total step : 80 \n",
            "error : 1.119955, accuarcy : 0.680000\n",
            "total step : 81 \n",
            "error : 1.114011, accuarcy : 0.681200\n",
            "total step : 82 \n",
            "error : 1.108173, accuarcy : 0.683000\n",
            "total step : 83 \n",
            "error : 1.102435, accuarcy : 0.684800\n",
            "total step : 84 \n",
            "error : 1.096795, accuarcy : 0.685800\n",
            "total step : 85 \n",
            "error : 1.091250, accuarcy : 0.686800\n",
            "total step : 86 \n",
            "error : 1.085798, accuarcy : 0.687800\n",
            "total step : 87 \n",
            "error : 1.080435, accuarcy : 0.689200\n",
            "total step : 88 \n",
            "error : 1.075159, accuarcy : 0.691100\n",
            "total step : 89 \n",
            "error : 1.069967, accuarcy : 0.692200\n",
            "total step : 90 \n",
            "error : 1.064857, accuarcy : 0.693400\n",
            "total step : 91 \n",
            "error : 1.059827, accuarcy : 0.694900\n",
            "total step : 92 \n",
            "error : 1.054874, accuarcy : 0.696100\n",
            "total step : 93 \n",
            "error : 1.049998, accuarcy : 0.697300\n",
            "total step : 94 \n",
            "error : 1.045194, accuarcy : 0.698200\n",
            "total step : 95 \n",
            "error : 1.040462, accuarcy : 0.698800\n",
            "total step : 96 \n",
            "error : 1.035800, accuarcy : 0.700100\n",
            "total step : 97 \n",
            "error : 1.031206, accuarcy : 0.701000\n",
            "total step : 98 \n",
            "error : 1.026679, accuarcy : 0.701700\n",
            "total step : 99 \n",
            "error : 1.022216, accuarcy : 0.702500\n",
            "total step : 100 \n",
            "error : 1.017816, accuarcy : 0.703600\n",
            "total step : 101 \n",
            "error : 1.013478, accuarcy : 0.704700\n",
            "total step : 102 \n",
            "error : 1.009200, accuarcy : 0.705800\n",
            "total step : 103 \n",
            "error : 1.004982, accuarcy : 0.706000\n",
            "total step : 104 \n",
            "error : 1.000820, accuarcy : 0.706700\n",
            "total step : 105 \n",
            "error : 0.996715, accuarcy : 0.707500\n",
            "total step : 106 \n",
            "error : 0.992665, accuarcy : 0.708400\n",
            "total step : 107 \n",
            "error : 0.988668, accuarcy : 0.709100\n",
            "total step : 108 \n",
            "error : 0.984724, accuarcy : 0.710500\n",
            "total step : 109 \n",
            "error : 0.980832, accuarcy : 0.711700\n",
            "total step : 110 \n",
            "error : 0.976990, accuarcy : 0.712700\n",
            "total step : 111 \n",
            "error : 0.973197, accuarcy : 0.713900\n",
            "total step : 112 \n",
            "error : 0.969452, accuarcy : 0.715000\n",
            "total step : 113 \n",
            "error : 0.965755, accuarcy : 0.716000\n",
            "total step : 114 \n",
            "error : 0.962104, accuarcy : 0.717100\n",
            "total step : 115 \n",
            "error : 0.958498, accuarcy : 0.717700\n",
            "total step : 116 \n",
            "error : 0.954936, accuarcy : 0.718100\n",
            "total step : 117 \n",
            "error : 0.951419, accuarcy : 0.718600\n",
            "total step : 118 \n",
            "error : 0.947943, accuarcy : 0.719500\n",
            "total step : 119 \n",
            "error : 0.944510, accuarcy : 0.720200\n",
            "total step : 120 \n",
            "error : 0.941118, accuarcy : 0.720300\n",
            "total step : 121 \n",
            "error : 0.937766, accuarcy : 0.720800\n",
            "total step : 122 \n",
            "error : 0.934453, accuarcy : 0.721600\n",
            "total step : 123 \n",
            "error : 0.931179, accuarcy : 0.723000\n",
            "total step : 124 \n",
            "error : 0.927943, accuarcy : 0.723800\n",
            "total step : 125 \n",
            "error : 0.924744, accuarcy : 0.724300\n",
            "total step : 126 \n",
            "error : 0.921581, accuarcy : 0.725000\n",
            "total step : 127 \n",
            "error : 0.918455, accuarcy : 0.725800\n",
            "total step : 128 \n",
            "error : 0.915363, accuarcy : 0.726600\n",
            "total step : 129 \n",
            "error : 0.912307, accuarcy : 0.727300\n",
            "total step : 130 \n",
            "error : 0.909284, accuarcy : 0.728600\n",
            "total step : 131 \n",
            "error : 0.906294, accuarcy : 0.729400\n",
            "total step : 132 \n",
            "error : 0.903337, accuarcy : 0.730000\n",
            "total step : 133 \n",
            "error : 0.900413, accuarcy : 0.730600\n",
            "total step : 134 \n",
            "error : 0.897519, accuarcy : 0.731700\n",
            "total step : 135 \n",
            "error : 0.894657, accuarcy : 0.732400\n",
            "total step : 136 \n",
            "error : 0.891826, accuarcy : 0.733100\n",
            "total step : 137 \n",
            "error : 0.889024, accuarcy : 0.734500\n",
            "total step : 138 \n",
            "error : 0.886252, accuarcy : 0.735100\n",
            "total step : 139 \n",
            "error : 0.883509, accuarcy : 0.735800\n",
            "total step : 140 \n",
            "error : 0.880794, accuarcy : 0.736500\n",
            "total step : 141 \n",
            "error : 0.878107, accuarcy : 0.737500\n",
            "total step : 142 \n",
            "error : 0.875448, accuarcy : 0.738000\n",
            "total step : 143 \n",
            "error : 0.872816, accuarcy : 0.738600\n",
            "total step : 144 \n",
            "error : 0.870210, accuarcy : 0.739000\n",
            "total step : 145 \n",
            "error : 0.867631, accuarcy : 0.739900\n",
            "total step : 146 \n",
            "error : 0.865078, accuarcy : 0.740400\n",
            "total step : 147 \n",
            "error : 0.862550, accuarcy : 0.740800\n",
            "total step : 148 \n",
            "error : 0.860047, accuarcy : 0.741500\n",
            "total step : 149 \n",
            "error : 0.857569, accuarcy : 0.742000\n",
            "total step : 150 \n",
            "error : 0.855115, accuarcy : 0.742600\n",
            "total step : 151 \n",
            "error : 0.852685, accuarcy : 0.743600\n",
            "total step : 152 \n",
            "error : 0.850278, accuarcy : 0.744100\n",
            "total step : 153 \n",
            "error : 0.847894, accuarcy : 0.744500\n",
            "total step : 154 \n",
            "error : 0.845534, accuarcy : 0.745200\n",
            "total step : 155 \n",
            "error : 0.843196, accuarcy : 0.745800\n",
            "total step : 156 \n",
            "error : 0.840880, accuarcy : 0.746500\n",
            "total step : 157 \n",
            "error : 0.838586, accuarcy : 0.747100\n",
            "total step : 158 \n",
            "error : 0.836313, accuarcy : 0.747600\n",
            "total step : 159 \n",
            "error : 0.834062, accuarcy : 0.747700\n",
            "total step : 160 \n",
            "error : 0.831831, accuarcy : 0.748000\n",
            "total step : 161 \n",
            "error : 0.829621, accuarcy : 0.748300\n",
            "total step : 162 \n",
            "error : 0.827432, accuarcy : 0.749100\n",
            "total step : 163 \n",
            "error : 0.825262, accuarcy : 0.749500\n",
            "total step : 164 \n",
            "error : 0.823112, accuarcy : 0.749800\n",
            "total step : 165 \n",
            "error : 0.820982, accuarcy : 0.750100\n",
            "total step : 166 \n",
            "error : 0.818870, accuarcy : 0.750800\n",
            "total step : 167 \n",
            "error : 0.816778, accuarcy : 0.751500\n",
            "total step : 168 \n",
            "error : 0.814705, accuarcy : 0.752000\n",
            "total step : 169 \n",
            "error : 0.812649, accuarcy : 0.753000\n",
            "total step : 170 \n",
            "error : 0.810612, accuarcy : 0.753500\n",
            "total step : 171 \n",
            "error : 0.808593, accuarcy : 0.754000\n",
            "total step : 172 \n",
            "error : 0.806592, accuarcy : 0.754500\n",
            "total step : 173 \n",
            "error : 0.804608, accuarcy : 0.755100\n",
            "total step : 174 \n",
            "error : 0.802641, accuarcy : 0.755800\n",
            "total step : 175 \n",
            "error : 0.800691, accuarcy : 0.756000\n",
            "total step : 176 \n",
            "error : 0.798758, accuarcy : 0.756700\n",
            "total step : 177 \n",
            "error : 0.796842, accuarcy : 0.757100\n",
            "total step : 178 \n",
            "error : 0.794941, accuarcy : 0.757400\n",
            "total step : 179 \n",
            "error : 0.793057, accuarcy : 0.757900\n",
            "total step : 180 \n",
            "error : 0.791189, accuarcy : 0.758100\n",
            "total step : 181 \n",
            "error : 0.789336, accuarcy : 0.758300\n",
            "total step : 182 \n",
            "error : 0.787499, accuarcy : 0.758800\n",
            "total step : 183 \n",
            "error : 0.785678, accuarcy : 0.759200\n",
            "total step : 184 \n",
            "error : 0.783871, accuarcy : 0.759400\n",
            "total step : 185 \n",
            "error : 0.782079, accuarcy : 0.760100\n",
            "total step : 186 \n",
            "error : 0.780302, accuarcy : 0.760400\n",
            "total step : 187 \n",
            "error : 0.778540, accuarcy : 0.760600\n",
            "total step : 188 \n",
            "error : 0.776791, accuarcy : 0.761000\n",
            "total step : 189 \n",
            "error : 0.775057, accuarcy : 0.761100\n",
            "total step : 190 \n",
            "error : 0.773337, accuarcy : 0.761300\n",
            "total step : 191 \n",
            "error : 0.771631, accuarcy : 0.761600\n",
            "total step : 192 \n",
            "error : 0.769938, accuarcy : 0.761800\n",
            "total step : 193 \n",
            "error : 0.768259, accuarcy : 0.762100\n",
            "total step : 194 \n",
            "error : 0.766593, accuarcy : 0.762800\n",
            "total step : 195 \n",
            "error : 0.764941, accuarcy : 0.763500\n",
            "total step : 196 \n",
            "error : 0.763301, accuarcy : 0.763800\n",
            "total step : 197 \n",
            "error : 0.761674, accuarcy : 0.763900\n",
            "total step : 198 \n",
            "error : 0.760060, accuarcy : 0.764200\n",
            "total step : 199 \n",
            "error : 0.758458, accuarcy : 0.764500\n",
            "total step : 200 \n",
            "error : 0.756869, accuarcy : 0.764800\n",
            "total step : 201 \n",
            "error : 0.755292, accuarcy : 0.765000\n",
            "total step : 202 \n",
            "error : 0.753727, accuarcy : 0.765200\n",
            "total step : 203 \n",
            "error : 0.752173, accuarcy : 0.765400\n",
            "total step : 204 \n",
            "error : 0.750632, accuarcy : 0.765800\n",
            "total step : 205 \n",
            "error : 0.749102, accuarcy : 0.766200\n",
            "total step : 206 \n",
            "error : 0.747584, accuarcy : 0.766500\n",
            "total step : 207 \n",
            "error : 0.746077, accuarcy : 0.766800\n",
            "total step : 208 \n",
            "error : 0.744581, accuarcy : 0.767000\n",
            "total step : 209 \n",
            "error : 0.743097, accuarcy : 0.767600\n",
            "total step : 210 \n",
            "error : 0.741623, accuarcy : 0.767800\n",
            "total step : 211 \n",
            "error : 0.740160, accuarcy : 0.768000\n",
            "total step : 212 \n",
            "error : 0.738708, accuarcy : 0.768200\n",
            "total step : 213 \n",
            "error : 0.737267, accuarcy : 0.768700\n",
            "total step : 214 \n",
            "error : 0.735835, accuarcy : 0.769500\n",
            "total step : 215 \n",
            "error : 0.734415, accuarcy : 0.769900\n",
            "total step : 216 \n",
            "error : 0.733004, accuarcy : 0.769800\n",
            "total step : 217 \n",
            "error : 0.731604, accuarcy : 0.770000\n",
            "total step : 218 \n",
            "error : 0.730213, accuarcy : 0.770200\n",
            "total step : 219 \n",
            "error : 0.728833, accuarcy : 0.770100\n",
            "total step : 220 \n",
            "error : 0.727462, accuarcy : 0.770000\n",
            "total step : 221 \n",
            "error : 0.726101, accuarcy : 0.770600\n",
            "total step : 222 \n",
            "error : 0.724749, accuarcy : 0.770800\n",
            "total step : 223 \n",
            "error : 0.723407, accuarcy : 0.771000\n",
            "total step : 224 \n",
            "error : 0.722074, accuarcy : 0.771200\n",
            "total step : 225 \n",
            "error : 0.720751, accuarcy : 0.771700\n",
            "total step : 226 \n",
            "error : 0.719436, accuarcy : 0.771900\n",
            "total step : 227 \n",
            "error : 0.718131, accuarcy : 0.772000\n",
            "total step : 228 \n",
            "error : 0.716834, accuarcy : 0.772300\n",
            "total step : 229 \n",
            "error : 0.715546, accuarcy : 0.772700\n",
            "total step : 230 \n",
            "error : 0.714267, accuarcy : 0.773000\n",
            "total step : 231 \n",
            "error : 0.712997, accuarcy : 0.773200\n",
            "total step : 232 \n",
            "error : 0.711735, accuarcy : 0.773300\n",
            "total step : 233 \n",
            "error : 0.710481, accuarcy : 0.773400\n",
            "total step : 234 \n",
            "error : 0.709236, accuarcy : 0.774000\n",
            "total step : 235 \n",
            "error : 0.707999, accuarcy : 0.774200\n",
            "total step : 236 \n",
            "error : 0.706770, accuarcy : 0.774400\n",
            "total step : 237 \n",
            "error : 0.705549, accuarcy : 0.774900\n",
            "total step : 238 \n",
            "error : 0.704336, accuarcy : 0.775000\n",
            "total step : 239 \n",
            "error : 0.703131, accuarcy : 0.775300\n",
            "total step : 240 \n",
            "error : 0.701934, accuarcy : 0.775700\n",
            "total step : 241 \n",
            "error : 0.700744, accuarcy : 0.776100\n",
            "total step : 242 \n",
            "error : 0.699562, accuarcy : 0.776400\n",
            "total step : 243 \n",
            "error : 0.698388, accuarcy : 0.777100\n",
            "total step : 244 \n",
            "error : 0.697221, accuarcy : 0.777400\n",
            "total step : 245 \n",
            "error : 0.696062, accuarcy : 0.777400\n",
            "total step : 246 \n",
            "error : 0.694909, accuarcy : 0.777700\n",
            "total step : 247 \n",
            "error : 0.693764, accuarcy : 0.778100\n",
            "total step : 248 \n",
            "error : 0.692626, accuarcy : 0.778300\n",
            "total step : 249 \n",
            "error : 0.691495, accuarcy : 0.779000\n",
            "total step : 250 \n",
            "error : 0.690372, accuarcy : 0.778900\n",
            "total step : 251 \n",
            "error : 0.689255, accuarcy : 0.779200\n",
            "total step : 252 \n",
            "error : 0.688144, accuarcy : 0.779300\n",
            "total step : 253 \n",
            "error : 0.687041, accuarcy : 0.779400\n",
            "total step : 254 \n",
            "error : 0.685944, accuarcy : 0.779700\n",
            "total step : 255 \n",
            "error : 0.684854, accuarcy : 0.779700\n",
            "total step : 256 \n",
            "error : 0.683770, accuarcy : 0.779900\n",
            "total step : 257 \n",
            "error : 0.682693, accuarcy : 0.780200\n",
            "total step : 258 \n",
            "error : 0.681623, accuarcy : 0.780700\n",
            "total step : 259 \n",
            "error : 0.680558, accuarcy : 0.780700\n",
            "total step : 260 \n",
            "error : 0.679500, accuarcy : 0.780800\n",
            "total step : 261 \n",
            "error : 0.678448, accuarcy : 0.781400\n",
            "total step : 262 \n",
            "error : 0.677402, accuarcy : 0.781600\n",
            "total step : 263 \n",
            "error : 0.676362, accuarcy : 0.781900\n",
            "total step : 264 \n",
            "error : 0.675329, accuarcy : 0.782300\n",
            "total step : 265 \n",
            "error : 0.674301, accuarcy : 0.782800\n",
            "total step : 266 \n",
            "error : 0.673279, accuarcy : 0.782700\n",
            "total step : 267 \n",
            "error : 0.672263, accuarcy : 0.782900\n",
            "total step : 268 \n",
            "error : 0.671252, accuarcy : 0.783300\n",
            "total step : 269 \n",
            "error : 0.670247, accuarcy : 0.783400\n",
            "total step : 270 \n",
            "error : 0.669248, accuarcy : 0.783700\n",
            "total step : 271 \n",
            "error : 0.668255, accuarcy : 0.784000\n",
            "total step : 272 \n",
            "error : 0.667267, accuarcy : 0.784200\n",
            "total step : 273 \n",
            "error : 0.666284, accuarcy : 0.784400\n",
            "total step : 274 \n",
            "error : 0.665307, accuarcy : 0.784600\n",
            "total step : 275 \n",
            "error : 0.664335, accuarcy : 0.784900\n",
            "total step : 276 \n",
            "error : 0.663369, accuarcy : 0.784900\n",
            "total step : 277 \n",
            "error : 0.662408, accuarcy : 0.785000\n",
            "total step : 278 \n",
            "error : 0.661451, accuarcy : 0.785400\n",
            "total step : 279 \n",
            "error : 0.660500, accuarcy : 0.785700\n",
            "total step : 280 \n",
            "error : 0.659555, accuarcy : 0.785900\n",
            "total step : 281 \n",
            "error : 0.658614, accuarcy : 0.786200\n",
            "total step : 282 \n",
            "error : 0.657678, accuarcy : 0.786300\n",
            "total step : 283 \n",
            "error : 0.656747, accuarcy : 0.786600\n",
            "total step : 284 \n",
            "error : 0.655821, accuarcy : 0.786900\n",
            "total step : 285 \n",
            "error : 0.654899, accuarcy : 0.787300\n",
            "total step : 286 \n",
            "error : 0.653983, accuarcy : 0.787200\n",
            "total step : 287 \n",
            "error : 0.653071, accuarcy : 0.787700\n",
            "total step : 288 \n",
            "error : 0.652164, accuarcy : 0.787700\n",
            "total step : 289 \n",
            "error : 0.651261, accuarcy : 0.787800\n",
            "total step : 290 \n",
            "error : 0.650363, accuarcy : 0.788000\n",
            "total step : 291 \n",
            "error : 0.649470, accuarcy : 0.788200\n",
            "total step : 292 \n",
            "error : 0.648581, accuarcy : 0.788600\n",
            "total step : 293 \n",
            "error : 0.647697, accuarcy : 0.788600\n",
            "total step : 294 \n",
            "error : 0.646817, accuarcy : 0.789000\n",
            "total step : 295 \n",
            "error : 0.645941, accuarcy : 0.789100\n",
            "total step : 296 \n",
            "error : 0.645070, accuarcy : 0.789300\n",
            "total step : 297 \n",
            "error : 0.644203, accuarcy : 0.789400\n",
            "total step : 298 \n",
            "error : 0.643340, accuarcy : 0.789500\n",
            "total step : 299 \n",
            "error : 0.642481, accuarcy : 0.789600\n",
            "total step : 300 \n",
            "error : 0.641627, accuarcy : 0.789700\n",
            "total step : 301 \n",
            "error : 0.640776, accuarcy : 0.790000\n",
            "total step : 302 \n",
            "error : 0.639930, accuarcy : 0.789900\n",
            "total step : 303 \n",
            "error : 0.639088, accuarcy : 0.790200\n",
            "total step : 304 \n",
            "error : 0.638249, accuarcy : 0.790300\n",
            "total step : 305 \n",
            "error : 0.637415, accuarcy : 0.790300\n",
            "total step : 306 \n",
            "error : 0.636584, accuarcy : 0.790700\n",
            "total step : 307 \n",
            "error : 0.635758, accuarcy : 0.790700\n",
            "total step : 308 \n",
            "error : 0.634935, accuarcy : 0.790800\n",
            "total step : 309 \n",
            "error : 0.634116, accuarcy : 0.791100\n",
            "total step : 310 \n",
            "error : 0.633301, accuarcy : 0.791300\n",
            "total step : 311 \n",
            "error : 0.632490, accuarcy : 0.791500\n",
            "total step : 312 \n",
            "error : 0.631682, accuarcy : 0.791600\n",
            "total step : 313 \n",
            "error : 0.630878, accuarcy : 0.791400\n",
            "total step : 314 \n",
            "error : 0.630078, accuarcy : 0.791500\n",
            "total step : 315 \n",
            "error : 0.629281, accuarcy : 0.791900\n",
            "total step : 316 \n",
            "error : 0.628487, accuarcy : 0.792000\n",
            "total step : 317 \n",
            "error : 0.627698, accuarcy : 0.792200\n",
            "total step : 318 \n",
            "error : 0.626912, accuarcy : 0.792500\n",
            "total step : 319 \n",
            "error : 0.626129, accuarcy : 0.792600\n",
            "total step : 320 \n",
            "error : 0.625349, accuarcy : 0.792700\n",
            "total step : 321 \n",
            "error : 0.624573, accuarcy : 0.793100\n",
            "total step : 322 \n",
            "error : 0.623801, accuarcy : 0.793400\n",
            "total step : 323 \n",
            "error : 0.623032, accuarcy : 0.793500\n",
            "total step : 324 \n",
            "error : 0.622266, accuarcy : 0.793600\n",
            "total step : 325 \n",
            "error : 0.621503, accuarcy : 0.793900\n",
            "total step : 326 \n",
            "error : 0.620744, accuarcy : 0.794200\n",
            "total step : 327 \n",
            "error : 0.619988, accuarcy : 0.794500\n",
            "total step : 328 \n",
            "error : 0.619235, accuarcy : 0.794900\n",
            "total step : 329 \n",
            "error : 0.618485, accuarcy : 0.794900\n",
            "total step : 330 \n",
            "error : 0.617738, accuarcy : 0.795300\n",
            "total step : 331 \n",
            "error : 0.616995, accuarcy : 0.795400\n",
            "total step : 332 \n",
            "error : 0.616254, accuarcy : 0.795700\n",
            "total step : 333 \n",
            "error : 0.615517, accuarcy : 0.796000\n",
            "total step : 334 \n",
            "error : 0.614782, accuarcy : 0.796000\n",
            "total step : 335 \n",
            "error : 0.614051, accuarcy : 0.796000\n",
            "total step : 336 \n",
            "error : 0.613323, accuarcy : 0.796200\n",
            "total step : 337 \n",
            "error : 0.612597, accuarcy : 0.796300\n",
            "total step : 338 \n",
            "error : 0.611875, accuarcy : 0.796400\n",
            "total step : 339 \n",
            "error : 0.611155, accuarcy : 0.796400\n",
            "total step : 340 \n",
            "error : 0.610438, accuarcy : 0.796500\n",
            "total step : 341 \n",
            "error : 0.609724, accuarcy : 0.796600\n",
            "total step : 342 \n",
            "error : 0.609013, accuarcy : 0.796700\n",
            "total step : 343 \n",
            "error : 0.608305, accuarcy : 0.796700\n",
            "total step : 344 \n",
            "error : 0.607600, accuarcy : 0.796900\n",
            "total step : 345 \n",
            "error : 0.606897, accuarcy : 0.797000\n",
            "total step : 346 \n",
            "error : 0.606197, accuarcy : 0.797100\n",
            "total step : 347 \n",
            "error : 0.605499, accuarcy : 0.797100\n",
            "total step : 348 \n",
            "error : 0.604805, accuarcy : 0.797300\n",
            "total step : 349 \n",
            "error : 0.604113, accuarcy : 0.797800\n",
            "total step : 350 \n",
            "error : 0.603424, accuarcy : 0.797900\n",
            "total step : 351 \n",
            "error : 0.602737, accuarcy : 0.798000\n",
            "total step : 352 \n",
            "error : 0.602053, accuarcy : 0.797900\n",
            "total step : 353 \n",
            "error : 0.601371, accuarcy : 0.798100\n",
            "total step : 354 \n",
            "error : 0.600692, accuarcy : 0.798400\n",
            "total step : 355 \n",
            "error : 0.600016, accuarcy : 0.798700\n",
            "total step : 356 \n",
            "error : 0.599342, accuarcy : 0.799200\n",
            "total step : 357 \n",
            "error : 0.598671, accuarcy : 0.799500\n",
            "total step : 358 \n",
            "error : 0.598002, accuarcy : 0.799700\n",
            "total step : 359 \n",
            "error : 0.597335, accuarcy : 0.799900\n",
            "total step : 360 \n",
            "error : 0.596671, accuarcy : 0.800000\n",
            "total step : 361 \n",
            "error : 0.596010, accuarcy : 0.800100\n",
            "total step : 362 \n",
            "error : 0.595351, accuarcy : 0.800400\n",
            "total step : 363 \n",
            "error : 0.594694, accuarcy : 0.800500\n",
            "total step : 364 \n",
            "error : 0.594039, accuarcy : 0.800400\n",
            "total step : 365 \n",
            "error : 0.593387, accuarcy : 0.800600\n",
            "total step : 366 \n",
            "error : 0.592737, accuarcy : 0.800600\n",
            "total step : 367 \n",
            "error : 0.592090, accuarcy : 0.800700\n",
            "total step : 368 \n",
            "error : 0.591445, accuarcy : 0.800900\n",
            "total step : 369 \n",
            "error : 0.590802, accuarcy : 0.801000\n",
            "total step : 370 \n",
            "error : 0.590161, accuarcy : 0.801100\n",
            "total step : 371 \n",
            "error : 0.589523, accuarcy : 0.801300\n",
            "total step : 372 \n",
            "error : 0.588887, accuarcy : 0.801800\n",
            "total step : 373 \n",
            "error : 0.588253, accuarcy : 0.802100\n",
            "total step : 374 \n",
            "error : 0.587621, accuarcy : 0.802400\n",
            "total step : 375 \n",
            "error : 0.586991, accuarcy : 0.802500\n",
            "total step : 376 \n",
            "error : 0.586364, accuarcy : 0.802400\n",
            "total step : 377 \n",
            "error : 0.585739, accuarcy : 0.802600\n",
            "total step : 378 \n",
            "error : 0.585115, accuarcy : 0.802500\n",
            "total step : 379 \n",
            "error : 0.584494, accuarcy : 0.802400\n",
            "total step : 380 \n",
            "error : 0.583875, accuarcy : 0.802400\n",
            "total step : 381 \n",
            "error : 0.583258, accuarcy : 0.802400\n",
            "total step : 382 \n",
            "error : 0.582644, accuarcy : 0.802500\n",
            "total step : 383 \n",
            "error : 0.582031, accuarcy : 0.802800\n",
            "total step : 384 \n",
            "error : 0.581420, accuarcy : 0.802600\n",
            "total step : 385 \n",
            "error : 0.580811, accuarcy : 0.802600\n",
            "total step : 386 \n",
            "error : 0.580205, accuarcy : 0.802600\n",
            "total step : 387 \n",
            "error : 0.579600, accuarcy : 0.802800\n",
            "total step : 388 \n",
            "error : 0.578997, accuarcy : 0.802800\n",
            "total step : 389 \n",
            "error : 0.578396, accuarcy : 0.802800\n",
            "total step : 390 \n",
            "error : 0.577798, accuarcy : 0.803000\n",
            "total step : 391 \n",
            "error : 0.577201, accuarcy : 0.803000\n",
            "total step : 392 \n",
            "error : 0.576606, accuarcy : 0.803100\n",
            "total step : 393 \n",
            "error : 0.576013, accuarcy : 0.803100\n",
            "total step : 394 \n",
            "error : 0.575421, accuarcy : 0.803000\n",
            "total step : 395 \n",
            "error : 0.574832, accuarcy : 0.803200\n",
            "total step : 396 \n",
            "error : 0.574245, accuarcy : 0.803400\n",
            "total step : 397 \n",
            "error : 0.573659, accuarcy : 0.803700\n",
            "total step : 398 \n",
            "error : 0.573076, accuarcy : 0.803800\n",
            "total step : 399 \n",
            "error : 0.572494, accuarcy : 0.803900\n",
            "total step : 400 \n",
            "error : 0.571914, accuarcy : 0.804000\n",
            "total step : 401 \n",
            "error : 0.571335, accuarcy : 0.804000\n",
            "total step : 402 \n",
            "error : 0.570759, accuarcy : 0.804000\n",
            "total step : 403 \n",
            "error : 0.570184, accuarcy : 0.803900\n",
            "total step : 404 \n",
            "error : 0.569611, accuarcy : 0.804100\n",
            "total step : 405 \n",
            "error : 0.569040, accuarcy : 0.804100\n",
            "total step : 406 \n",
            "error : 0.568471, accuarcy : 0.804100\n",
            "total step : 407 \n",
            "error : 0.567903, accuarcy : 0.804200\n",
            "total step : 408 \n",
            "error : 0.567337, accuarcy : 0.804300\n",
            "total step : 409 \n",
            "error : 0.566773, accuarcy : 0.804400\n",
            "total step : 410 \n",
            "error : 0.566211, accuarcy : 0.804700\n",
            "total step : 411 \n",
            "error : 0.565650, accuarcy : 0.804700\n",
            "total step : 412 \n",
            "error : 0.565091, accuarcy : 0.804900\n",
            "total step : 413 \n",
            "error : 0.564534, accuarcy : 0.804900\n",
            "total step : 414 \n",
            "error : 0.563978, accuarcy : 0.805100\n",
            "total step : 415 \n",
            "error : 0.563424, accuarcy : 0.805300\n",
            "total step : 416 \n",
            "error : 0.562871, accuarcy : 0.805100\n",
            "total step : 417 \n",
            "error : 0.562320, accuarcy : 0.805200\n",
            "total step : 418 \n",
            "error : 0.561771, accuarcy : 0.805300\n",
            "total step : 419 \n",
            "error : 0.561224, accuarcy : 0.805400\n",
            "total step : 420 \n",
            "error : 0.560678, accuarcy : 0.805400\n",
            "total step : 421 \n",
            "error : 0.560133, accuarcy : 0.805600\n",
            "total step : 422 \n",
            "error : 0.559591, accuarcy : 0.805800\n",
            "total step : 423 \n",
            "error : 0.559049, accuarcy : 0.805600\n",
            "total step : 424 \n",
            "error : 0.558510, accuarcy : 0.805600\n",
            "total step : 425 \n",
            "error : 0.557971, accuarcy : 0.805600\n",
            "total step : 426 \n",
            "error : 0.557435, accuarcy : 0.805900\n",
            "total step : 427 \n",
            "error : 0.556900, accuarcy : 0.805900\n",
            "total step : 428 \n",
            "error : 0.556366, accuarcy : 0.806300\n",
            "total step : 429 \n",
            "error : 0.555834, accuarcy : 0.806300\n",
            "total step : 430 \n",
            "error : 0.555304, accuarcy : 0.806600\n",
            "total step : 431 \n",
            "error : 0.554775, accuarcy : 0.806600\n",
            "total step : 432 \n",
            "error : 0.554247, accuarcy : 0.806600\n",
            "total step : 433 \n",
            "error : 0.553721, accuarcy : 0.806600\n",
            "total step : 434 \n",
            "error : 0.553197, accuarcy : 0.806600\n",
            "total step : 435 \n",
            "error : 0.552674, accuarcy : 0.806600\n",
            "total step : 436 \n",
            "error : 0.552152, accuarcy : 0.807100\n",
            "total step : 437 \n",
            "error : 0.551632, accuarcy : 0.807400\n",
            "total step : 438 \n",
            "error : 0.551113, accuarcy : 0.807400\n",
            "total step : 439 \n",
            "error : 0.550596, accuarcy : 0.807400\n",
            "total step : 440 \n",
            "error : 0.550080, accuarcy : 0.807500\n",
            "total step : 441 \n",
            "error : 0.549566, accuarcy : 0.807500\n",
            "total step : 442 \n",
            "error : 0.549053, accuarcy : 0.807500\n",
            "total step : 443 \n",
            "error : 0.548541, accuarcy : 0.807600\n",
            "total step : 444 \n",
            "error : 0.548031, accuarcy : 0.807600\n",
            "total step : 445 \n",
            "error : 0.547522, accuarcy : 0.807700\n",
            "total step : 446 \n",
            "error : 0.547015, accuarcy : 0.807800\n",
            "total step : 447 \n",
            "error : 0.546508, accuarcy : 0.808200\n",
            "total step : 448 \n",
            "error : 0.546004, accuarcy : 0.808300\n",
            "total step : 449 \n",
            "error : 0.545500, accuarcy : 0.808300\n",
            "total step : 450 \n",
            "error : 0.544998, accuarcy : 0.808300\n",
            "total step : 451 \n",
            "error : 0.544498, accuarcy : 0.808400\n",
            "total step : 452 \n",
            "error : 0.543998, accuarcy : 0.808600\n",
            "total step : 453 \n",
            "error : 0.543500, accuarcy : 0.808500\n",
            "total step : 454 \n",
            "error : 0.543003, accuarcy : 0.808500\n",
            "total step : 455 \n",
            "error : 0.542508, accuarcy : 0.808600\n",
            "total step : 456 \n",
            "error : 0.542014, accuarcy : 0.808500\n",
            "total step : 457 \n",
            "error : 0.541521, accuarcy : 0.808600\n",
            "total step : 458 \n",
            "error : 0.541030, accuarcy : 0.808700\n",
            "total step : 459 \n",
            "error : 0.540539, accuarcy : 0.808900\n",
            "total step : 460 \n",
            "error : 0.540050, accuarcy : 0.808900\n",
            "total step : 461 \n",
            "error : 0.539563, accuarcy : 0.809100\n",
            "total step : 462 \n",
            "error : 0.539076, accuarcy : 0.809200\n",
            "total step : 463 \n",
            "error : 0.538591, accuarcy : 0.809400\n",
            "total step : 464 \n",
            "error : 0.538107, accuarcy : 0.809500\n",
            "total step : 465 \n",
            "error : 0.537625, accuarcy : 0.809700\n",
            "total step : 466 \n",
            "error : 0.537143, accuarcy : 0.809900\n",
            "total step : 467 \n",
            "error : 0.536663, accuarcy : 0.810100\n",
            "total step : 468 \n",
            "error : 0.536184, accuarcy : 0.810200\n",
            "total step : 469 \n",
            "error : 0.535706, accuarcy : 0.810200\n",
            "total step : 470 \n",
            "error : 0.535230, accuarcy : 0.810200\n",
            "total step : 471 \n",
            "error : 0.534754, accuarcy : 0.810200\n",
            "total step : 472 \n",
            "error : 0.534280, accuarcy : 0.810400\n",
            "total step : 473 \n",
            "error : 0.533807, accuarcy : 0.810400\n",
            "total step : 474 \n",
            "error : 0.533335, accuarcy : 0.810500\n",
            "total step : 475 \n",
            "error : 0.532865, accuarcy : 0.810500\n",
            "total step : 476 \n",
            "error : 0.532396, accuarcy : 0.810600\n",
            "total step : 477 \n",
            "error : 0.531927, accuarcy : 0.810800\n",
            "total step : 478 \n",
            "error : 0.531460, accuarcy : 0.810800\n",
            "total step : 479 \n",
            "error : 0.530994, accuarcy : 0.810800\n",
            "total step : 480 \n",
            "error : 0.530530, accuarcy : 0.810800\n",
            "total step : 481 \n",
            "error : 0.530066, accuarcy : 0.810900\n",
            "total step : 482 \n",
            "error : 0.529604, accuarcy : 0.811200\n",
            "total step : 483 \n",
            "error : 0.529142, accuarcy : 0.811200\n",
            "total step : 484 \n",
            "error : 0.528682, accuarcy : 0.811500\n",
            "total step : 485 \n",
            "error : 0.528223, accuarcy : 0.811600\n",
            "total step : 486 \n",
            "error : 0.527765, accuarcy : 0.811600\n",
            "total step : 487 \n",
            "error : 0.527308, accuarcy : 0.811700\n",
            "total step : 488 \n",
            "error : 0.526853, accuarcy : 0.811800\n",
            "total step : 489 \n",
            "error : 0.526398, accuarcy : 0.811900\n",
            "total step : 490 \n",
            "error : 0.525945, accuarcy : 0.811800\n",
            "total step : 491 \n",
            "error : 0.525492, accuarcy : 0.811800\n",
            "total step : 492 \n",
            "error : 0.525041, accuarcy : 0.811800\n",
            "total step : 493 \n",
            "error : 0.524591, accuarcy : 0.811900\n",
            "total step : 494 \n",
            "error : 0.524142, accuarcy : 0.811800\n",
            "total step : 495 \n",
            "error : 0.523694, accuarcy : 0.812000\n",
            "total step : 496 \n",
            "error : 0.523247, accuarcy : 0.811800\n",
            "total step : 497 \n",
            "error : 0.522801, accuarcy : 0.812000\n",
            "total step : 498 \n",
            "error : 0.522356, accuarcy : 0.812000\n",
            "total step : 499 \n",
            "error : 0.521912, accuarcy : 0.812000\n",
            "total step : 500 \n",
            "error : 0.521470, accuarcy : 0.812200\n",
            "total step : 501 \n",
            "error : 0.521028, accuarcy : 0.812300\n",
            "total step : 502 \n",
            "error : 0.520587, accuarcy : 0.812400\n",
            "total step : 503 \n",
            "error : 0.520148, accuarcy : 0.812400\n",
            "total step : 504 \n",
            "error : 0.519709, accuarcy : 0.812500\n",
            "total step : 505 \n",
            "error : 0.519272, accuarcy : 0.812500\n",
            "total step : 506 \n",
            "error : 0.518835, accuarcy : 0.812700\n",
            "total step : 507 \n",
            "error : 0.518400, accuarcy : 0.812800\n",
            "total step : 508 \n",
            "error : 0.517965, accuarcy : 0.812800\n",
            "total step : 509 \n",
            "error : 0.517532, accuarcy : 0.812900\n",
            "total step : 510 \n",
            "error : 0.517099, accuarcy : 0.812700\n",
            "total step : 511 \n",
            "error : 0.516668, accuarcy : 0.812700\n",
            "total step : 512 \n",
            "error : 0.516237, accuarcy : 0.812900\n",
            "total step : 513 \n",
            "error : 0.515808, accuarcy : 0.812900\n",
            "total step : 514 \n",
            "error : 0.515379, accuarcy : 0.812800\n",
            "total step : 515 \n",
            "error : 0.514952, accuarcy : 0.813000\n",
            "total step : 516 \n",
            "error : 0.514525, accuarcy : 0.813000\n",
            "total step : 517 \n",
            "error : 0.514100, accuarcy : 0.813100\n",
            "total step : 518 \n",
            "error : 0.513675, accuarcy : 0.813400\n",
            "total step : 519 \n",
            "error : 0.513252, accuarcy : 0.813600\n",
            "total step : 520 \n",
            "error : 0.512829, accuarcy : 0.813700\n",
            "total step : 521 \n",
            "error : 0.512407, accuarcy : 0.814000\n",
            "total step : 522 \n",
            "error : 0.511987, accuarcy : 0.813900\n",
            "total step : 523 \n",
            "error : 0.511567, accuarcy : 0.814100\n",
            "total step : 524 \n",
            "error : 0.511148, accuarcy : 0.814100\n",
            "total step : 525 \n",
            "error : 0.510730, accuarcy : 0.814100\n",
            "total step : 526 \n",
            "error : 0.510313, accuarcy : 0.814100\n",
            "total step : 527 \n",
            "error : 0.509897, accuarcy : 0.814000\n",
            "total step : 528 \n",
            "error : 0.509482, accuarcy : 0.814000\n",
            "total step : 529 \n",
            "error : 0.509068, accuarcy : 0.814100\n",
            "total step : 530 \n",
            "error : 0.508655, accuarcy : 0.814100\n",
            "total step : 531 \n",
            "error : 0.508243, accuarcy : 0.813900\n",
            "total step : 532 \n",
            "error : 0.507831, accuarcy : 0.813900\n",
            "total step : 533 \n",
            "error : 0.507421, accuarcy : 0.814000\n",
            "total step : 534 \n",
            "error : 0.507011, accuarcy : 0.814100\n",
            "total step : 535 \n",
            "error : 0.506603, accuarcy : 0.814200\n",
            "total step : 536 \n",
            "error : 0.506195, accuarcy : 0.814200\n",
            "total step : 537 \n",
            "error : 0.505788, accuarcy : 0.814300\n",
            "total step : 538 \n",
            "error : 0.505382, accuarcy : 0.814600\n",
            "total step : 539 \n",
            "error : 0.504977, accuarcy : 0.814800\n",
            "total step : 540 \n",
            "error : 0.504573, accuarcy : 0.814800\n",
            "total step : 541 \n",
            "error : 0.504170, accuarcy : 0.815000\n",
            "total step : 542 \n",
            "error : 0.503767, accuarcy : 0.814900\n",
            "total step : 543 \n",
            "error : 0.503366, accuarcy : 0.815000\n",
            "total step : 544 \n",
            "error : 0.502965, accuarcy : 0.815100\n",
            "total step : 545 \n",
            "error : 0.502565, accuarcy : 0.815200\n",
            "total step : 546 \n",
            "error : 0.502167, accuarcy : 0.815200\n",
            "total step : 547 \n",
            "error : 0.501769, accuarcy : 0.815300\n",
            "total step : 548 \n",
            "error : 0.501371, accuarcy : 0.815400\n",
            "total step : 549 \n",
            "error : 0.500975, accuarcy : 0.815700\n",
            "total step : 550 \n",
            "error : 0.500580, accuarcy : 0.815900\n",
            "total step : 551 \n",
            "error : 0.500185, accuarcy : 0.816000\n",
            "total step : 552 \n",
            "error : 0.499791, accuarcy : 0.816100\n",
            "total step : 553 \n",
            "error : 0.499398, accuarcy : 0.816100\n",
            "total step : 554 \n",
            "error : 0.499006, accuarcy : 0.816300\n",
            "total step : 555 \n",
            "error : 0.498615, accuarcy : 0.816400\n",
            "total step : 556 \n",
            "error : 0.498225, accuarcy : 0.816600\n",
            "total step : 557 \n",
            "error : 0.497835, accuarcy : 0.816500\n",
            "total step : 558 \n",
            "error : 0.497447, accuarcy : 0.816600\n",
            "total step : 559 \n",
            "error : 0.497059, accuarcy : 0.816500\n",
            "total step : 560 \n",
            "error : 0.496672, accuarcy : 0.816500\n",
            "total step : 561 \n",
            "error : 0.496285, accuarcy : 0.816400\n",
            "total step : 562 \n",
            "error : 0.495900, accuarcy : 0.816300\n",
            "total step : 563 \n",
            "error : 0.495515, accuarcy : 0.816500\n",
            "total step : 564 \n",
            "error : 0.495131, accuarcy : 0.816600\n",
            "total step : 565 \n",
            "error : 0.494748, accuarcy : 0.816800\n",
            "total step : 566 \n",
            "error : 0.494366, accuarcy : 0.816800\n",
            "total step : 567 \n",
            "error : 0.493985, accuarcy : 0.816900\n",
            "total step : 568 \n",
            "error : 0.493604, accuarcy : 0.816900\n",
            "total step : 569 \n",
            "error : 0.493224, accuarcy : 0.817000\n",
            "total step : 570 \n",
            "error : 0.492845, accuarcy : 0.817100\n",
            "total step : 571 \n",
            "error : 0.492467, accuarcy : 0.817200\n",
            "total step : 572 \n",
            "error : 0.492090, accuarcy : 0.817300\n",
            "total step : 573 \n",
            "error : 0.491713, accuarcy : 0.817200\n",
            "total step : 574 \n",
            "error : 0.491337, accuarcy : 0.817300\n",
            "total step : 575 \n",
            "error : 0.490962, accuarcy : 0.817300\n",
            "total step : 576 \n",
            "error : 0.490588, accuarcy : 0.817700\n",
            "total step : 577 \n",
            "error : 0.490214, accuarcy : 0.817900\n",
            "total step : 578 \n",
            "error : 0.489842, accuarcy : 0.817900\n",
            "total step : 579 \n",
            "error : 0.489470, accuarcy : 0.817800\n",
            "total step : 580 \n",
            "error : 0.489098, accuarcy : 0.817700\n",
            "total step : 581 \n",
            "error : 0.488728, accuarcy : 0.817700\n",
            "total step : 582 \n",
            "error : 0.488358, accuarcy : 0.817800\n",
            "total step : 583 \n",
            "error : 0.487989, accuarcy : 0.818100\n",
            "total step : 584 \n",
            "error : 0.487621, accuarcy : 0.818200\n",
            "total step : 585 \n",
            "error : 0.487253, accuarcy : 0.818100\n",
            "total step : 586 \n",
            "error : 0.486887, accuarcy : 0.818400\n",
            "total step : 587 \n",
            "error : 0.486521, accuarcy : 0.818400\n",
            "total step : 588 \n",
            "error : 0.486156, accuarcy : 0.818500\n",
            "total step : 589 \n",
            "error : 0.485791, accuarcy : 0.818400\n",
            "total step : 590 \n",
            "error : 0.485427, accuarcy : 0.818500\n",
            "total step : 591 \n",
            "error : 0.485064, accuarcy : 0.818400\n",
            "total step : 592 \n",
            "error : 0.484702, accuarcy : 0.818300\n",
            "total step : 593 \n",
            "error : 0.484341, accuarcy : 0.818400\n",
            "total step : 594 \n",
            "error : 0.483980, accuarcy : 0.818600\n",
            "total step : 595 \n",
            "error : 0.483620, accuarcy : 0.818600\n",
            "total step : 596 \n",
            "error : 0.483260, accuarcy : 0.818800\n",
            "total step : 597 \n",
            "error : 0.482902, accuarcy : 0.818800\n",
            "total step : 598 \n",
            "error : 0.482544, accuarcy : 0.818900\n",
            "total step : 599 \n",
            "error : 0.482187, accuarcy : 0.818900\n",
            "total step : 600 \n",
            "error : 0.481830, accuarcy : 0.819000\n",
            "total step : 601 \n",
            "error : 0.481474, accuarcy : 0.819000\n",
            "total step : 602 \n",
            "error : 0.481119, accuarcy : 0.819000\n",
            "total step : 603 \n",
            "error : 0.480765, accuarcy : 0.819100\n",
            "total step : 604 \n",
            "error : 0.480411, accuarcy : 0.818900\n",
            "total step : 605 \n",
            "error : 0.480058, accuarcy : 0.819000\n",
            "total step : 606 \n",
            "error : 0.479706, accuarcy : 0.819000\n",
            "total step : 607 \n",
            "error : 0.479355, accuarcy : 0.819200\n",
            "total step : 608 \n",
            "error : 0.479004, accuarcy : 0.819200\n",
            "total step : 609 \n",
            "error : 0.478654, accuarcy : 0.819100\n",
            "total step : 610 \n",
            "error : 0.478304, accuarcy : 0.819100\n",
            "total step : 611 \n",
            "error : 0.477955, accuarcy : 0.819100\n",
            "total step : 612 \n",
            "error : 0.477607, accuarcy : 0.819200\n",
            "total step : 613 \n",
            "error : 0.477260, accuarcy : 0.819200\n",
            "total step : 614 \n",
            "error : 0.476913, accuarcy : 0.819200\n",
            "total step : 615 \n",
            "error : 0.476567, accuarcy : 0.819300\n",
            "total step : 616 \n",
            "error : 0.476222, accuarcy : 0.819200\n",
            "total step : 617 \n",
            "error : 0.475877, accuarcy : 0.819200\n",
            "total step : 618 \n",
            "error : 0.475533, accuarcy : 0.819100\n",
            "total step : 619 \n",
            "error : 0.475189, accuarcy : 0.819000\n",
            "total step : 620 \n",
            "error : 0.474847, accuarcy : 0.818900\n",
            "total step : 621 \n",
            "error : 0.474505, accuarcy : 0.819000\n",
            "total step : 622 \n",
            "error : 0.474163, accuarcy : 0.818900\n",
            "total step : 623 \n",
            "error : 0.473823, accuarcy : 0.819000\n",
            "total step : 624 \n",
            "error : 0.473482, accuarcy : 0.819100\n",
            "total step : 625 \n",
            "error : 0.473143, accuarcy : 0.819100\n",
            "total step : 626 \n",
            "error : 0.472804, accuarcy : 0.819100\n",
            "total step : 627 \n",
            "error : 0.472466, accuarcy : 0.819100\n",
            "total step : 628 \n",
            "error : 0.472129, accuarcy : 0.819300\n",
            "total step : 629 \n",
            "error : 0.471792, accuarcy : 0.819300\n",
            "total step : 630 \n",
            "error : 0.471456, accuarcy : 0.819400\n",
            "total step : 631 \n",
            "error : 0.471120, accuarcy : 0.819500\n",
            "total step : 632 \n",
            "error : 0.470785, accuarcy : 0.819500\n",
            "total step : 633 \n",
            "error : 0.470451, accuarcy : 0.819600\n",
            "total step : 634 \n",
            "error : 0.470118, accuarcy : 0.819600\n",
            "total step : 635 \n",
            "error : 0.469785, accuarcy : 0.819600\n",
            "total step : 636 \n",
            "error : 0.469452, accuarcy : 0.819700\n",
            "total step : 637 \n",
            "error : 0.469121, accuarcy : 0.819700\n",
            "total step : 638 \n",
            "error : 0.468790, accuarcy : 0.819700\n",
            "total step : 639 \n",
            "error : 0.468459, accuarcy : 0.819600\n",
            "total step : 640 \n",
            "error : 0.468129, accuarcy : 0.819500\n",
            "total step : 641 \n",
            "error : 0.467800, accuarcy : 0.819500\n",
            "total step : 642 \n",
            "error : 0.467472, accuarcy : 0.819500\n",
            "total step : 643 \n",
            "error : 0.467144, accuarcy : 0.819500\n",
            "total step : 644 \n",
            "error : 0.466816, accuarcy : 0.819600\n",
            "total step : 645 \n",
            "error : 0.466490, accuarcy : 0.819700\n",
            "total step : 646 \n",
            "error : 0.466164, accuarcy : 0.819700\n",
            "total step : 647 \n",
            "error : 0.465838, accuarcy : 0.819700\n",
            "total step : 648 \n",
            "error : 0.465513, accuarcy : 0.819600\n",
            "total step : 649 \n",
            "error : 0.465189, accuarcy : 0.819600\n",
            "total step : 650 \n",
            "error : 0.464865, accuarcy : 0.819700\n",
            "total step : 651 \n",
            "error : 0.464542, accuarcy : 0.819600\n",
            "total step : 652 \n",
            "error : 0.464220, accuarcy : 0.819600\n",
            "total step : 653 \n",
            "error : 0.463898, accuarcy : 0.819600\n",
            "total step : 654 \n",
            "error : 0.463577, accuarcy : 0.819700\n",
            "total step : 655 \n",
            "error : 0.463256, accuarcy : 0.819600\n",
            "total step : 656 \n",
            "error : 0.462936, accuarcy : 0.819700\n",
            "total step : 657 \n",
            "error : 0.462617, accuarcy : 0.819800\n",
            "total step : 658 \n",
            "error : 0.462298, accuarcy : 0.819800\n",
            "total step : 659 \n",
            "error : 0.461980, accuarcy : 0.820000\n",
            "total step : 660 \n",
            "error : 0.461662, accuarcy : 0.820000\n",
            "total step : 661 \n",
            "error : 0.461345, accuarcy : 0.820100\n",
            "total step : 662 \n",
            "error : 0.461028, accuarcy : 0.820200\n",
            "total step : 663 \n",
            "error : 0.460712, accuarcy : 0.820200\n",
            "total step : 664 \n",
            "error : 0.460397, accuarcy : 0.820300\n",
            "total step : 665 \n",
            "error : 0.460082, accuarcy : 0.820500\n",
            "total step : 666 \n",
            "error : 0.459768, accuarcy : 0.820500\n",
            "total step : 667 \n",
            "error : 0.459454, accuarcy : 0.820500\n",
            "total step : 668 \n",
            "error : 0.459141, accuarcy : 0.820700\n",
            "total step : 669 \n",
            "error : 0.458829, accuarcy : 0.820700\n",
            "total step : 670 \n",
            "error : 0.458517, accuarcy : 0.820800\n",
            "total step : 671 \n",
            "error : 0.458206, accuarcy : 0.820800\n",
            "total step : 672 \n",
            "error : 0.457895, accuarcy : 0.820800\n",
            "total step : 673 \n",
            "error : 0.457585, accuarcy : 0.820900\n",
            "total step : 674 \n",
            "error : 0.457275, accuarcy : 0.821000\n",
            "total step : 675 \n",
            "error : 0.456966, accuarcy : 0.821100\n",
            "total step : 676 \n",
            "error : 0.456658, accuarcy : 0.821100\n",
            "total step : 677 \n",
            "error : 0.456350, accuarcy : 0.821100\n",
            "total step : 678 \n",
            "error : 0.456042, accuarcy : 0.821200\n",
            "total step : 679 \n",
            "error : 0.455735, accuarcy : 0.821200\n",
            "total step : 680 \n",
            "error : 0.455429, accuarcy : 0.821500\n",
            "total step : 681 \n",
            "error : 0.455123, accuarcy : 0.821500\n",
            "total step : 682 \n",
            "error : 0.454818, accuarcy : 0.821700\n",
            "total step : 683 \n",
            "error : 0.454514, accuarcy : 0.821700\n",
            "total step : 684 \n",
            "error : 0.454209, accuarcy : 0.821700\n",
            "total step : 685 \n",
            "error : 0.453906, accuarcy : 0.821700\n",
            "total step : 686 \n",
            "error : 0.453603, accuarcy : 0.821700\n",
            "total step : 687 \n",
            "error : 0.453300, accuarcy : 0.821700\n",
            "total step : 688 \n",
            "error : 0.452999, accuarcy : 0.821800\n",
            "total step : 689 \n",
            "error : 0.452697, accuarcy : 0.821800\n",
            "total step : 690 \n",
            "error : 0.452396, accuarcy : 0.821800\n",
            "total step : 691 \n",
            "error : 0.452096, accuarcy : 0.821900\n",
            "total step : 692 \n",
            "error : 0.451796, accuarcy : 0.821800\n",
            "total step : 693 \n",
            "error : 0.451497, accuarcy : 0.821600\n",
            "total step : 694 \n",
            "error : 0.451198, accuarcy : 0.821700\n",
            "total step : 695 \n",
            "error : 0.450900, accuarcy : 0.821800\n",
            "total step : 696 \n",
            "error : 0.450602, accuarcy : 0.821800\n",
            "total step : 697 \n",
            "error : 0.450305, accuarcy : 0.821600\n",
            "total step : 698 \n",
            "error : 0.450009, accuarcy : 0.822000\n",
            "total step : 699 \n",
            "error : 0.449713, accuarcy : 0.822100\n",
            "total step : 700 \n",
            "error : 0.449417, accuarcy : 0.822000\n",
            "total step : 701 \n",
            "error : 0.449122, accuarcy : 0.822000\n",
            "total step : 702 \n",
            "error : 0.448828, accuarcy : 0.822100\n",
            "total step : 703 \n",
            "error : 0.448534, accuarcy : 0.822000\n",
            "total step : 704 \n",
            "error : 0.448240, accuarcy : 0.822100\n",
            "total step : 705 \n",
            "error : 0.447947, accuarcy : 0.822100\n",
            "total step : 706 \n",
            "error : 0.447655, accuarcy : 0.822100\n",
            "total step : 707 \n",
            "error : 0.447363, accuarcy : 0.822400\n",
            "total step : 708 \n",
            "error : 0.447071, accuarcy : 0.822400\n",
            "total step : 709 \n",
            "error : 0.446781, accuarcy : 0.822400\n",
            "total step : 710 \n",
            "error : 0.446490, accuarcy : 0.822400\n",
            "total step : 711 \n",
            "error : 0.446200, accuarcy : 0.822400\n",
            "total step : 712 \n",
            "error : 0.445911, accuarcy : 0.822400\n",
            "total step : 713 \n",
            "error : 0.445622, accuarcy : 0.822400\n",
            "total step : 714 \n",
            "error : 0.445334, accuarcy : 0.822400\n",
            "total step : 715 \n",
            "error : 0.445046, accuarcy : 0.822400\n",
            "total step : 716 \n",
            "error : 0.444758, accuarcy : 0.822400\n",
            "total step : 717 \n",
            "error : 0.444471, accuarcy : 0.822400\n",
            "total step : 718 \n",
            "error : 0.444185, accuarcy : 0.822400\n",
            "total step : 719 \n",
            "error : 0.443899, accuarcy : 0.822400\n",
            "total step : 720 \n",
            "error : 0.443614, accuarcy : 0.822500\n",
            "total step : 721 \n",
            "error : 0.443329, accuarcy : 0.822600\n",
            "total step : 722 \n",
            "error : 0.443044, accuarcy : 0.822600\n",
            "total step : 723 \n",
            "error : 0.442761, accuarcy : 0.822600\n",
            "total step : 724 \n",
            "error : 0.442477, accuarcy : 0.822700\n",
            "total step : 725 \n",
            "error : 0.442194, accuarcy : 0.822800\n",
            "total step : 726 \n",
            "error : 0.441912, accuarcy : 0.822800\n",
            "total step : 727 \n",
            "error : 0.441630, accuarcy : 0.822800\n",
            "total step : 728 \n",
            "error : 0.441348, accuarcy : 0.822800\n",
            "total step : 729 \n",
            "error : 0.441067, accuarcy : 0.822900\n",
            "total step : 730 \n",
            "error : 0.440787, accuarcy : 0.822900\n",
            "total step : 731 \n",
            "error : 0.440507, accuarcy : 0.823000\n",
            "total step : 732 \n",
            "error : 0.440227, accuarcy : 0.822900\n",
            "total step : 733 \n",
            "error : 0.439948, accuarcy : 0.823000\n",
            "total step : 734 \n",
            "error : 0.439669, accuarcy : 0.823000\n",
            "total step : 735 \n",
            "error : 0.439391, accuarcy : 0.823100\n",
            "total step : 736 \n",
            "error : 0.439113, accuarcy : 0.823100\n",
            "total step : 737 \n",
            "error : 0.438836, accuarcy : 0.823100\n",
            "total step : 738 \n",
            "error : 0.438559, accuarcy : 0.823100\n",
            "total step : 739 \n",
            "error : 0.438283, accuarcy : 0.823100\n",
            "total step : 740 \n",
            "error : 0.438007, accuarcy : 0.823300\n",
            "total step : 741 \n",
            "error : 0.437732, accuarcy : 0.823400\n",
            "total step : 742 \n",
            "error : 0.437457, accuarcy : 0.823400\n",
            "total step : 743 \n",
            "error : 0.437183, accuarcy : 0.823500\n",
            "total step : 744 \n",
            "error : 0.436909, accuarcy : 0.823600\n",
            "total step : 745 \n",
            "error : 0.436635, accuarcy : 0.823700\n",
            "total step : 746 \n",
            "error : 0.436362, accuarcy : 0.823700\n",
            "total step : 747 \n",
            "error : 0.436090, accuarcy : 0.823700\n",
            "total step : 748 \n",
            "error : 0.435818, accuarcy : 0.823600\n",
            "total step : 749 \n",
            "error : 0.435546, accuarcy : 0.823600\n",
            "total step : 750 \n",
            "error : 0.435275, accuarcy : 0.823500\n",
            "total step : 751 \n",
            "error : 0.435004, accuarcy : 0.823500\n",
            "total step : 752 \n",
            "error : 0.434734, accuarcy : 0.823500\n",
            "total step : 753 \n",
            "error : 0.434464, accuarcy : 0.823600\n",
            "total step : 754 \n",
            "error : 0.434194, accuarcy : 0.823700\n",
            "total step : 755 \n",
            "error : 0.433925, accuarcy : 0.823600\n",
            "total step : 756 \n",
            "error : 0.433657, accuarcy : 0.823600\n",
            "total step : 757 \n",
            "error : 0.433389, accuarcy : 0.823600\n",
            "total step : 758 \n",
            "error : 0.433121, accuarcy : 0.823800\n",
            "total step : 759 \n",
            "error : 0.432854, accuarcy : 0.823800\n",
            "total step : 760 \n",
            "error : 0.432587, accuarcy : 0.823900\n",
            "total step : 761 \n",
            "error : 0.432321, accuarcy : 0.824000\n",
            "total step : 762 \n",
            "error : 0.432055, accuarcy : 0.824100\n",
            "total step : 763 \n",
            "error : 0.431790, accuarcy : 0.824100\n",
            "total step : 764 \n",
            "error : 0.431525, accuarcy : 0.824000\n",
            "total step : 765 \n",
            "error : 0.431260, accuarcy : 0.824300\n",
            "total step : 766 \n",
            "error : 0.430996, accuarcy : 0.824500\n",
            "total step : 767 \n",
            "error : 0.430733, accuarcy : 0.824300\n",
            "total step : 768 \n",
            "error : 0.430469, accuarcy : 0.824300\n",
            "total step : 769 \n",
            "error : 0.430206, accuarcy : 0.824300\n",
            "total step : 770 \n",
            "error : 0.429944, accuarcy : 0.824300\n",
            "total step : 771 \n",
            "error : 0.429682, accuarcy : 0.824200\n",
            "total step : 772 \n",
            "error : 0.429421, accuarcy : 0.824300\n",
            "total step : 773 \n",
            "error : 0.429160, accuarcy : 0.824300\n",
            "total step : 774 \n",
            "error : 0.428899, accuarcy : 0.824500\n",
            "total step : 775 \n",
            "error : 0.428639, accuarcy : 0.824600\n",
            "total step : 776 \n",
            "error : 0.428379, accuarcy : 0.824600\n",
            "total step : 777 \n",
            "error : 0.428120, accuarcy : 0.824700\n",
            "total step : 778 \n",
            "error : 0.427861, accuarcy : 0.824600\n",
            "total step : 779 \n",
            "error : 0.427602, accuarcy : 0.824600\n",
            "total step : 780 \n",
            "error : 0.427344, accuarcy : 0.824500\n",
            "total step : 781 \n",
            "error : 0.427086, accuarcy : 0.824500\n",
            "total step : 782 \n",
            "error : 0.426829, accuarcy : 0.824600\n",
            "total step : 783 \n",
            "error : 0.426572, accuarcy : 0.824700\n",
            "total step : 784 \n",
            "error : 0.426316, accuarcy : 0.824700\n",
            "total step : 785 \n",
            "error : 0.426060, accuarcy : 0.824700\n",
            "total step : 786 \n",
            "error : 0.425804, accuarcy : 0.824700\n",
            "total step : 787 \n",
            "error : 0.425549, accuarcy : 0.824700\n",
            "total step : 788 \n",
            "error : 0.425294, accuarcy : 0.824700\n",
            "total step : 789 \n",
            "error : 0.425040, accuarcy : 0.824600\n",
            "total step : 790 \n",
            "error : 0.424786, accuarcy : 0.824600\n",
            "total step : 791 \n",
            "error : 0.424532, accuarcy : 0.824500\n",
            "total step : 792 \n",
            "error : 0.424279, accuarcy : 0.824400\n",
            "total step : 793 \n",
            "error : 0.424026, accuarcy : 0.824500\n",
            "total step : 794 \n",
            "error : 0.423774, accuarcy : 0.824600\n",
            "total step : 795 \n",
            "error : 0.423522, accuarcy : 0.824600\n",
            "total step : 796 \n",
            "error : 0.423270, accuarcy : 0.824600\n",
            "total step : 797 \n",
            "error : 0.423019, accuarcy : 0.824500\n",
            "total step : 798 \n",
            "error : 0.422768, accuarcy : 0.824600\n",
            "total step : 799 \n",
            "error : 0.422518, accuarcy : 0.824600\n",
            "total step : 800 \n",
            "error : 0.422268, accuarcy : 0.824800\n",
            "total step : 801 \n",
            "error : 0.422019, accuarcy : 0.824800\n",
            "total step : 802 \n",
            "error : 0.421769, accuarcy : 0.824900\n",
            "total step : 803 \n",
            "error : 0.421521, accuarcy : 0.825100\n",
            "total step : 804 \n",
            "error : 0.421272, accuarcy : 0.825200\n",
            "total step : 805 \n",
            "error : 0.421024, accuarcy : 0.825200\n",
            "total step : 806 \n",
            "error : 0.420777, accuarcy : 0.825300\n",
            "total step : 807 \n",
            "error : 0.420530, accuarcy : 0.825200\n",
            "total step : 808 \n",
            "error : 0.420283, accuarcy : 0.825100\n",
            "total step : 809 \n",
            "error : 0.420036, accuarcy : 0.825100\n",
            "total step : 810 \n",
            "error : 0.419790, accuarcy : 0.825200\n",
            "total step : 811 \n",
            "error : 0.419545, accuarcy : 0.825200\n",
            "total step : 812 \n",
            "error : 0.419300, accuarcy : 0.825100\n",
            "total step : 813 \n",
            "error : 0.419055, accuarcy : 0.825100\n",
            "total step : 814 \n",
            "error : 0.418810, accuarcy : 0.825100\n",
            "total step : 815 \n",
            "error : 0.418566, accuarcy : 0.825200\n",
            "total step : 816 \n",
            "error : 0.418323, accuarcy : 0.825300\n",
            "total step : 817 \n",
            "error : 0.418079, accuarcy : 0.825300\n",
            "total step : 818 \n",
            "error : 0.417836, accuarcy : 0.825500\n",
            "total step : 819 \n",
            "error : 0.417594, accuarcy : 0.825400\n",
            "total step : 820 \n",
            "error : 0.417352, accuarcy : 0.825500\n",
            "total step : 821 \n",
            "error : 0.417110, accuarcy : 0.825800\n",
            "total step : 822 \n",
            "error : 0.416868, accuarcy : 0.825900\n",
            "total step : 823 \n",
            "error : 0.416627, accuarcy : 0.825900\n",
            "total step : 824 \n",
            "error : 0.416387, accuarcy : 0.825900\n",
            "total step : 825 \n",
            "error : 0.416146, accuarcy : 0.825900\n",
            "total step : 826 \n",
            "error : 0.415907, accuarcy : 0.826200\n",
            "total step : 827 \n",
            "error : 0.415667, accuarcy : 0.826100\n",
            "total step : 828 \n",
            "error : 0.415428, accuarcy : 0.826100\n",
            "total step : 829 \n",
            "error : 0.415189, accuarcy : 0.826200\n",
            "total step : 830 \n",
            "error : 0.414951, accuarcy : 0.826200\n",
            "total step : 831 \n",
            "error : 0.414713, accuarcy : 0.826100\n",
            "total step : 832 \n",
            "error : 0.414475, accuarcy : 0.826100\n",
            "total step : 833 \n",
            "error : 0.414238, accuarcy : 0.826200\n",
            "total step : 834 \n",
            "error : 0.414001, accuarcy : 0.826300\n",
            "total step : 835 \n",
            "error : 0.413764, accuarcy : 0.826400\n",
            "total step : 836 \n",
            "error : 0.413528, accuarcy : 0.826400\n",
            "total step : 837 \n",
            "error : 0.413292, accuarcy : 0.826400\n",
            "total step : 838 \n",
            "error : 0.413057, accuarcy : 0.826400\n",
            "total step : 839 \n",
            "error : 0.412821, accuarcy : 0.826400\n",
            "total step : 840 \n",
            "error : 0.412587, accuarcy : 0.826500\n",
            "total step : 841 \n",
            "error : 0.412352, accuarcy : 0.826600\n",
            "total step : 842 \n",
            "error : 0.412118, accuarcy : 0.826600\n",
            "total step : 843 \n",
            "error : 0.411884, accuarcy : 0.826500\n",
            "total step : 844 \n",
            "error : 0.411651, accuarcy : 0.826600\n",
            "total step : 845 \n",
            "error : 0.411418, accuarcy : 0.826600\n",
            "total step : 846 \n",
            "error : 0.411186, accuarcy : 0.826600\n",
            "total step : 847 \n",
            "error : 0.410953, accuarcy : 0.826600\n",
            "total step : 848 \n",
            "error : 0.410721, accuarcy : 0.826600\n",
            "total step : 849 \n",
            "error : 0.410490, accuarcy : 0.826700\n",
            "total step : 850 \n",
            "error : 0.410259, accuarcy : 0.826700\n",
            "total step : 851 \n",
            "error : 0.410028, accuarcy : 0.826700\n",
            "total step : 852 \n",
            "error : 0.409797, accuarcy : 0.826700\n",
            "total step : 853 \n",
            "error : 0.409567, accuarcy : 0.826700\n",
            "total step : 854 \n",
            "error : 0.409337, accuarcy : 0.826700\n",
            "total step : 855 \n",
            "error : 0.409108, accuarcy : 0.826700\n",
            "total step : 856 \n",
            "error : 0.408879, accuarcy : 0.826900\n",
            "total step : 857 \n",
            "error : 0.408650, accuarcy : 0.827000\n",
            "total step : 858 \n",
            "error : 0.408422, accuarcy : 0.827000\n",
            "total step : 859 \n",
            "error : 0.408194, accuarcy : 0.826800\n",
            "total step : 860 \n",
            "error : 0.407966, accuarcy : 0.826900\n",
            "total step : 861 \n",
            "error : 0.407739, accuarcy : 0.827000\n",
            "total step : 862 \n",
            "error : 0.407512, accuarcy : 0.827000\n",
            "total step : 863 \n",
            "error : 0.407285, accuarcy : 0.827100\n",
            "total step : 864 \n",
            "error : 0.407059, accuarcy : 0.827200\n",
            "total step : 865 \n",
            "error : 0.406833, accuarcy : 0.827200\n",
            "total step : 866 \n",
            "error : 0.406607, accuarcy : 0.827200\n",
            "total step : 867 \n",
            "error : 0.406382, accuarcy : 0.827200\n",
            "total step : 868 \n",
            "error : 0.406157, accuarcy : 0.827100\n",
            "total step : 869 \n",
            "error : 0.405932, accuarcy : 0.827100\n",
            "total step : 870 \n",
            "error : 0.405708, accuarcy : 0.827100\n",
            "total step : 871 \n",
            "error : 0.405484, accuarcy : 0.827100\n",
            "total step : 872 \n",
            "error : 0.405260, accuarcy : 0.827200\n",
            "total step : 873 \n",
            "error : 0.405037, accuarcy : 0.827200\n",
            "total step : 874 \n",
            "error : 0.404814, accuarcy : 0.827200\n",
            "total step : 875 \n",
            "error : 0.404591, accuarcy : 0.827400\n",
            "total step : 876 \n",
            "error : 0.404369, accuarcy : 0.827400\n",
            "total step : 877 \n",
            "error : 0.404147, accuarcy : 0.827400\n",
            "total step : 878 \n",
            "error : 0.403925, accuarcy : 0.827500\n",
            "total step : 879 \n",
            "error : 0.403704, accuarcy : 0.827500\n",
            "total step : 880 \n",
            "error : 0.403483, accuarcy : 0.827600\n",
            "total step : 881 \n",
            "error : 0.403262, accuarcy : 0.827600\n",
            "total step : 882 \n",
            "error : 0.403042, accuarcy : 0.827600\n",
            "total step : 883 \n",
            "error : 0.402822, accuarcy : 0.827600\n",
            "total step : 884 \n",
            "error : 0.402602, accuarcy : 0.827800\n",
            "total step : 885 \n",
            "error : 0.402383, accuarcy : 0.827700\n",
            "total step : 886 \n",
            "error : 0.402164, accuarcy : 0.827700\n",
            "total step : 887 \n",
            "error : 0.401945, accuarcy : 0.827700\n",
            "total step : 888 \n",
            "error : 0.401727, accuarcy : 0.827700\n",
            "total step : 889 \n",
            "error : 0.401509, accuarcy : 0.827700\n",
            "total step : 890 \n",
            "error : 0.401291, accuarcy : 0.827700\n",
            "total step : 891 \n",
            "error : 0.401074, accuarcy : 0.827700\n",
            "total step : 892 \n",
            "error : 0.400857, accuarcy : 0.827500\n",
            "total step : 893 \n",
            "error : 0.400640, accuarcy : 0.827400\n",
            "total step : 894 \n",
            "error : 0.400423, accuarcy : 0.827400\n",
            "total step : 895 \n",
            "error : 0.400207, accuarcy : 0.827500\n",
            "total step : 896 \n",
            "error : 0.399991, accuarcy : 0.827500\n",
            "total step : 897 \n",
            "error : 0.399776, accuarcy : 0.827400\n",
            "total step : 898 \n",
            "error : 0.399561, accuarcy : 0.827600\n",
            "total step : 899 \n",
            "error : 0.399346, accuarcy : 0.827600\n",
            "total step : 900 \n",
            "error : 0.399131, accuarcy : 0.827600\n",
            "total step : 901 \n",
            "error : 0.398917, accuarcy : 0.827600\n",
            "total step : 902 \n",
            "error : 0.398703, accuarcy : 0.827800\n",
            "total step : 903 \n",
            "error : 0.398490, accuarcy : 0.827800\n",
            "total step : 904 \n",
            "error : 0.398276, accuarcy : 0.827800\n",
            "total step : 905 \n",
            "error : 0.398063, accuarcy : 0.827800\n",
            "total step : 906 \n",
            "error : 0.397851, accuarcy : 0.827800\n",
            "total step : 907 \n",
            "error : 0.397638, accuarcy : 0.827900\n",
            "total step : 908 \n",
            "error : 0.397426, accuarcy : 0.827900\n",
            "total step : 909 \n",
            "error : 0.397214, accuarcy : 0.827900\n",
            "total step : 910 \n",
            "error : 0.397003, accuarcy : 0.827900\n",
            "total step : 911 \n",
            "error : 0.396792, accuarcy : 0.827900\n",
            "total step : 912 \n",
            "error : 0.396581, accuarcy : 0.828100\n",
            "total step : 913 \n",
            "error : 0.396370, accuarcy : 0.828200\n",
            "total step : 914 \n",
            "error : 0.396160, accuarcy : 0.828300\n",
            "total step : 915 \n",
            "error : 0.395950, accuarcy : 0.828400\n",
            "total step : 916 \n",
            "error : 0.395741, accuarcy : 0.828300\n",
            "total step : 917 \n",
            "error : 0.395531, accuarcy : 0.828300\n",
            "total step : 918 \n",
            "error : 0.395322, accuarcy : 0.828400\n",
            "total step : 919 \n",
            "error : 0.395114, accuarcy : 0.828500\n",
            "total step : 920 \n",
            "error : 0.394905, accuarcy : 0.828600\n",
            "total step : 921 \n",
            "error : 0.394697, accuarcy : 0.828500\n",
            "total step : 922 \n",
            "error : 0.394489, accuarcy : 0.828400\n",
            "total step : 923 \n",
            "error : 0.394282, accuarcy : 0.828400\n",
            "total step : 924 \n",
            "error : 0.394075, accuarcy : 0.828500\n",
            "total step : 925 \n",
            "error : 0.393868, accuarcy : 0.828600\n",
            "total step : 926 \n",
            "error : 0.393661, accuarcy : 0.828600\n",
            "total step : 927 \n",
            "error : 0.393455, accuarcy : 0.828600\n",
            "total step : 928 \n",
            "error : 0.393249, accuarcy : 0.828800\n",
            "total step : 929 \n",
            "error : 0.393043, accuarcy : 0.828700\n",
            "total step : 930 \n",
            "error : 0.392838, accuarcy : 0.828600\n",
            "total step : 931 \n",
            "error : 0.392633, accuarcy : 0.828700\n",
            "total step : 932 \n",
            "error : 0.392428, accuarcy : 0.828600\n",
            "total step : 933 \n",
            "error : 0.392223, accuarcy : 0.828600\n",
            "total step : 934 \n",
            "error : 0.392019, accuarcy : 0.828800\n",
            "total step : 935 \n",
            "error : 0.391815, accuarcy : 0.828700\n",
            "total step : 936 \n",
            "error : 0.391611, accuarcy : 0.828800\n",
            "total step : 937 \n",
            "error : 0.391408, accuarcy : 0.829100\n",
            "total step : 938 \n",
            "error : 0.391205, accuarcy : 0.829200\n",
            "total step : 939 \n",
            "error : 0.391002, accuarcy : 0.829200\n",
            "total step : 940 \n",
            "error : 0.390799, accuarcy : 0.829300\n",
            "total step : 941 \n",
            "error : 0.390597, accuarcy : 0.829400\n",
            "total step : 942 \n",
            "error : 0.390395, accuarcy : 0.829500\n",
            "total step : 943 \n",
            "error : 0.390194, accuarcy : 0.829500\n",
            "total step : 944 \n",
            "error : 0.389992, accuarcy : 0.829500\n",
            "total step : 945 \n",
            "error : 0.389791, accuarcy : 0.829500\n",
            "total step : 946 \n",
            "error : 0.389590, accuarcy : 0.829600\n",
            "total step : 947 \n",
            "error : 0.389390, accuarcy : 0.829600\n",
            "total step : 948 \n",
            "error : 0.389190, accuarcy : 0.829600\n",
            "total step : 949 \n",
            "error : 0.388990, accuarcy : 0.829700\n",
            "total step : 950 \n",
            "error : 0.388790, accuarcy : 0.829700\n",
            "total step : 951 \n",
            "error : 0.388591, accuarcy : 0.829900\n",
            "total step : 952 \n",
            "error : 0.388391, accuarcy : 0.829900\n",
            "total step : 953 \n",
            "error : 0.388193, accuarcy : 0.830000\n",
            "total step : 954 \n",
            "error : 0.387994, accuarcy : 0.830000\n",
            "total step : 955 \n",
            "error : 0.387796, accuarcy : 0.830000\n",
            "total step : 956 \n",
            "error : 0.387598, accuarcy : 0.830100\n",
            "total step : 957 \n",
            "error : 0.387400, accuarcy : 0.830100\n",
            "total step : 958 \n",
            "error : 0.387203, accuarcy : 0.830100\n",
            "total step : 959 \n",
            "error : 0.387006, accuarcy : 0.830100\n",
            "total step : 960 \n",
            "error : 0.386809, accuarcy : 0.830100\n",
            "total step : 961 \n",
            "error : 0.386612, accuarcy : 0.830100\n",
            "total step : 962 \n",
            "error : 0.386416, accuarcy : 0.830100\n",
            "total step : 963 \n",
            "error : 0.386220, accuarcy : 0.830100\n",
            "total step : 964 \n",
            "error : 0.386024, accuarcy : 0.830200\n",
            "total step : 965 \n",
            "error : 0.385828, accuarcy : 0.830200\n",
            "total step : 966 \n",
            "error : 0.385633, accuarcy : 0.830100\n",
            "total step : 967 \n",
            "error : 0.385438, accuarcy : 0.830100\n",
            "total step : 968 \n",
            "error : 0.385244, accuarcy : 0.830100\n",
            "total step : 969 \n",
            "error : 0.385049, accuarcy : 0.830000\n",
            "total step : 970 \n",
            "error : 0.384855, accuarcy : 0.830000\n",
            "total step : 971 \n",
            "error : 0.384661, accuarcy : 0.830100\n",
            "total step : 972 \n",
            "error : 0.384467, accuarcy : 0.830000\n",
            "total step : 973 \n",
            "error : 0.384274, accuarcy : 0.830000\n",
            "total step : 974 \n",
            "error : 0.384081, accuarcy : 0.830100\n",
            "total step : 975 \n",
            "error : 0.383888, accuarcy : 0.829900\n",
            "total step : 976 \n",
            "error : 0.383696, accuarcy : 0.829900\n",
            "total step : 977 \n",
            "error : 0.383503, accuarcy : 0.829900\n",
            "total step : 978 \n",
            "error : 0.383311, accuarcy : 0.829800\n",
            "total step : 979 \n",
            "error : 0.383120, accuarcy : 0.829800\n",
            "total step : 980 \n",
            "error : 0.382928, accuarcy : 0.829800\n",
            "total step : 981 \n",
            "error : 0.382737, accuarcy : 0.829800\n",
            "total step : 982 \n",
            "error : 0.382546, accuarcy : 0.829800\n",
            "total step : 983 \n",
            "error : 0.382355, accuarcy : 0.829700\n",
            "total step : 984 \n",
            "error : 0.382165, accuarcy : 0.829700\n",
            "total step : 985 \n",
            "error : 0.381975, accuarcy : 0.829800\n",
            "total step : 986 \n",
            "error : 0.381785, accuarcy : 0.829900\n",
            "total step : 987 \n",
            "error : 0.381595, accuarcy : 0.829900\n",
            "total step : 988 \n",
            "error : 0.381406, accuarcy : 0.830100\n",
            "total step : 989 \n",
            "error : 0.381217, accuarcy : 0.830400\n",
            "total step : 990 \n",
            "error : 0.381028, accuarcy : 0.830400\n",
            "total step : 991 \n",
            "error : 0.380839, accuarcy : 0.830400\n",
            "total step : 992 \n",
            "error : 0.380651, accuarcy : 0.830200\n",
            "total step : 993 \n",
            "error : 0.380463, accuarcy : 0.830200\n",
            "total step : 994 \n",
            "error : 0.380275, accuarcy : 0.830200\n",
            "total step : 995 \n",
            "error : 0.380088, accuarcy : 0.830200\n",
            "total step : 996 \n",
            "error : 0.379900, accuarcy : 0.830200\n",
            "total step : 997 \n",
            "error : 0.379713, accuarcy : 0.830200\n",
            "total step : 998 \n",
            "error : 0.379526, accuarcy : 0.830200\n",
            "total step : 999 \n",
            "error : 0.379340, accuarcy : 0.830300\n",
            "total step : 1000 \n",
            "error : 0.379154, accuarcy : 0.830400\n",
            "total step : 1001 \n",
            "error : 0.378968, accuarcy : 0.830300\n",
            "total step : 1002 \n",
            "error : 0.378782, accuarcy : 0.830300\n",
            "total step : 1003 \n",
            "error : 0.378596, accuarcy : 0.830200\n",
            "total step : 1004 \n",
            "error : 0.378411, accuarcy : 0.830300\n",
            "total step : 1005 \n",
            "error : 0.378226, accuarcy : 0.830500\n",
            "total step : 1006 \n",
            "error : 0.378041, accuarcy : 0.830500\n",
            "total step : 1007 \n",
            "error : 0.377857, accuarcy : 0.830600\n",
            "total step : 1008 \n",
            "error : 0.377673, accuarcy : 0.830600\n",
            "total step : 1009 \n",
            "error : 0.377489, accuarcy : 0.830500\n",
            "total step : 1010 \n",
            "error : 0.377305, accuarcy : 0.830600\n",
            "total step : 1011 \n",
            "error : 0.377121, accuarcy : 0.830600\n",
            "total step : 1012 \n",
            "error : 0.376938, accuarcy : 0.830700\n",
            "total step : 1013 \n",
            "error : 0.376755, accuarcy : 0.830600\n",
            "total step : 1014 \n",
            "error : 0.376572, accuarcy : 0.830600\n",
            "total step : 1015 \n",
            "error : 0.376390, accuarcy : 0.830600\n",
            "total step : 1016 \n",
            "error : 0.376207, accuarcy : 0.830600\n",
            "total step : 1017 \n",
            "error : 0.376025, accuarcy : 0.830500\n",
            "total step : 1018 \n",
            "error : 0.375844, accuarcy : 0.830400\n",
            "total step : 1019 \n",
            "error : 0.375662, accuarcy : 0.830500\n",
            "total step : 1020 \n",
            "error : 0.375481, accuarcy : 0.830400\n",
            "total step : 1021 \n",
            "error : 0.375300, accuarcy : 0.830400\n",
            "total step : 1022 \n",
            "error : 0.375119, accuarcy : 0.830400\n",
            "total step : 1023 \n",
            "error : 0.374938, accuarcy : 0.830500\n",
            "total step : 1024 \n",
            "error : 0.374758, accuarcy : 0.830500\n",
            "total step : 1025 \n",
            "error : 0.374578, accuarcy : 0.830600\n",
            "total step : 1026 \n",
            "error : 0.374398, accuarcy : 0.830800\n",
            "total step : 1027 \n",
            "error : 0.374219, accuarcy : 0.830700\n",
            "total step : 1028 \n",
            "error : 0.374039, accuarcy : 0.830700\n",
            "total step : 1029 \n",
            "error : 0.373860, accuarcy : 0.830700\n",
            "total step : 1030 \n",
            "error : 0.373681, accuarcy : 0.830700\n",
            "total step : 1031 \n",
            "error : 0.373503, accuarcy : 0.830700\n",
            "total step : 1032 \n",
            "error : 0.373324, accuarcy : 0.830500\n",
            "total step : 1033 \n",
            "error : 0.373146, accuarcy : 0.830500\n",
            "total step : 1034 \n",
            "error : 0.372968, accuarcy : 0.830500\n",
            "total step : 1035 \n",
            "error : 0.372790, accuarcy : 0.830500\n",
            "total step : 1036 \n",
            "error : 0.372613, accuarcy : 0.830500\n",
            "total step : 1037 \n",
            "error : 0.372436, accuarcy : 0.830500\n",
            "total step : 1038 \n",
            "error : 0.372259, accuarcy : 0.830500\n",
            "total step : 1039 \n",
            "error : 0.372082, accuarcy : 0.830600\n",
            "total step : 1040 \n",
            "error : 0.371906, accuarcy : 0.830600\n",
            "total step : 1041 \n",
            "error : 0.371729, accuarcy : 0.830600\n",
            "total step : 1042 \n",
            "error : 0.371553, accuarcy : 0.830600\n",
            "total step : 1043 \n",
            "error : 0.371378, accuarcy : 0.830600\n",
            "total step : 1044 \n",
            "error : 0.371202, accuarcy : 0.830700\n",
            "total step : 1045 \n",
            "error : 0.371027, accuarcy : 0.830700\n",
            "total step : 1046 \n",
            "error : 0.370852, accuarcy : 0.830700\n",
            "total step : 1047 \n",
            "error : 0.370677, accuarcy : 0.830700\n",
            "total step : 1048 \n",
            "error : 0.370502, accuarcy : 0.830700\n",
            "total step : 1049 \n",
            "error : 0.370328, accuarcy : 0.830700\n",
            "total step : 1050 \n",
            "error : 0.370153, accuarcy : 0.830700\n",
            "total step : 1051 \n",
            "error : 0.369980, accuarcy : 0.830700\n",
            "total step : 1052 \n",
            "error : 0.369806, accuarcy : 0.830800\n",
            "total step : 1053 \n",
            "error : 0.369632, accuarcy : 0.830800\n",
            "total step : 1054 \n",
            "error : 0.369459, accuarcy : 0.830900\n",
            "total step : 1055 \n",
            "error : 0.369286, accuarcy : 0.830900\n",
            "total step : 1056 \n",
            "error : 0.369113, accuarcy : 0.830800\n",
            "total step : 1057 \n",
            "error : 0.368941, accuarcy : 0.830700\n",
            "total step : 1058 \n",
            "error : 0.368768, accuarcy : 0.830600\n",
            "total step : 1059 \n",
            "error : 0.368596, accuarcy : 0.830500\n",
            "total step : 1060 \n",
            "error : 0.368424, accuarcy : 0.830600\n",
            "total step : 1061 \n",
            "error : 0.368253, accuarcy : 0.830500\n",
            "total step : 1062 \n",
            "error : 0.368081, accuarcy : 0.830400\n",
            "total step : 1063 \n",
            "error : 0.367910, accuarcy : 0.830500\n",
            "total step : 1064 \n",
            "error : 0.367739, accuarcy : 0.830600\n",
            "total step : 1065 \n",
            "error : 0.367568, accuarcy : 0.830600\n",
            "total step : 1066 \n",
            "error : 0.367398, accuarcy : 0.830600\n",
            "total step : 1067 \n",
            "error : 0.367228, accuarcy : 0.830600\n",
            "total step : 1068 \n",
            "error : 0.367057, accuarcy : 0.830600\n",
            "total step : 1069 \n",
            "error : 0.366888, accuarcy : 0.830500\n",
            "total step : 1070 \n",
            "error : 0.366718, accuarcy : 0.830500\n",
            "total step : 1071 \n",
            "error : 0.366549, accuarcy : 0.830500\n",
            "total step : 1072 \n",
            "error : 0.366379, accuarcy : 0.830700\n",
            "total step : 1073 \n",
            "error : 0.366210, accuarcy : 0.830700\n",
            "total step : 1074 \n",
            "error : 0.366042, accuarcy : 0.830800\n",
            "total step : 1075 \n",
            "error : 0.365873, accuarcy : 0.830800\n",
            "total step : 1076 \n",
            "error : 0.365705, accuarcy : 0.830800\n",
            "total step : 1077 \n",
            "error : 0.365537, accuarcy : 0.830900\n",
            "total step : 1078 \n",
            "error : 0.365369, accuarcy : 0.830900\n",
            "total step : 1079 \n",
            "error : 0.365201, accuarcy : 0.830900\n",
            "total step : 1080 \n",
            "error : 0.365034, accuarcy : 0.830900\n",
            "total step : 1081 \n",
            "error : 0.364866, accuarcy : 0.830900\n",
            "total step : 1082 \n",
            "error : 0.364699, accuarcy : 0.831000\n",
            "total step : 1083 \n",
            "error : 0.364533, accuarcy : 0.831000\n",
            "total step : 1084 \n",
            "error : 0.364366, accuarcy : 0.831000\n",
            "total step : 1085 \n",
            "error : 0.364200, accuarcy : 0.831000\n",
            "total step : 1086 \n",
            "error : 0.364034, accuarcy : 0.831100\n",
            "total step : 1087 \n",
            "error : 0.363868, accuarcy : 0.830900\n",
            "total step : 1088 \n",
            "error : 0.363702, accuarcy : 0.830900\n",
            "total step : 1089 \n",
            "error : 0.363536, accuarcy : 0.830900\n",
            "total step : 1090 \n",
            "error : 0.363371, accuarcy : 0.830900\n",
            "total step : 1091 \n",
            "error : 0.363206, accuarcy : 0.830900\n",
            "total step : 1092 \n",
            "error : 0.363041, accuarcy : 0.831000\n",
            "total step : 1093 \n",
            "error : 0.362877, accuarcy : 0.831000\n",
            "total step : 1094 \n",
            "error : 0.362712, accuarcy : 0.831000\n",
            "total step : 1095 \n",
            "error : 0.362548, accuarcy : 0.831000\n",
            "total step : 1096 \n",
            "error : 0.362384, accuarcy : 0.831100\n",
            "total step : 1097 \n",
            "error : 0.362220, accuarcy : 0.831300\n",
            "total step : 1098 \n",
            "error : 0.362057, accuarcy : 0.831300\n",
            "total step : 1099 \n",
            "error : 0.361893, accuarcy : 0.831300\n",
            "total step : 1100 \n",
            "error : 0.361730, accuarcy : 0.831300\n",
            "total step : 1101 \n",
            "error : 0.361567, accuarcy : 0.831200\n",
            "total step : 1102 \n",
            "error : 0.361404, accuarcy : 0.831200\n",
            "total step : 1103 \n",
            "error : 0.361242, accuarcy : 0.831300\n",
            "total step : 1104 \n",
            "error : 0.361080, accuarcy : 0.831300\n",
            "total step : 1105 \n",
            "error : 0.360917, accuarcy : 0.831300\n",
            "total step : 1106 \n",
            "error : 0.360756, accuarcy : 0.831300\n",
            "total step : 1107 \n",
            "error : 0.360594, accuarcy : 0.831300\n",
            "total step : 1108 \n",
            "error : 0.360432, accuarcy : 0.831300\n",
            "total step : 1109 \n",
            "error : 0.360271, accuarcy : 0.831400\n",
            "total step : 1110 \n",
            "error : 0.360110, accuarcy : 0.831400\n",
            "total step : 1111 \n",
            "error : 0.359949, accuarcy : 0.831500\n",
            "total step : 1112 \n",
            "error : 0.359788, accuarcy : 0.831400\n",
            "total step : 1113 \n",
            "error : 0.359628, accuarcy : 0.831400\n",
            "total step : 1114 \n",
            "error : 0.359468, accuarcy : 0.831500\n",
            "total step : 1115 \n",
            "error : 0.359308, accuarcy : 0.831400\n",
            "total step : 1116 \n",
            "error : 0.359148, accuarcy : 0.831400\n",
            "total step : 1117 \n",
            "error : 0.358988, accuarcy : 0.831400\n",
            "total step : 1118 \n",
            "error : 0.358829, accuarcy : 0.831400\n",
            "total step : 1119 \n",
            "error : 0.358670, accuarcy : 0.831400\n",
            "total step : 1120 \n",
            "error : 0.358510, accuarcy : 0.831500\n",
            "total step : 1121 \n",
            "error : 0.358352, accuarcy : 0.831600\n",
            "total step : 1122 \n",
            "error : 0.358193, accuarcy : 0.831700\n",
            "total step : 1123 \n",
            "error : 0.358035, accuarcy : 0.831700\n",
            "total step : 1124 \n",
            "error : 0.357876, accuarcy : 0.831800\n",
            "total step : 1125 \n",
            "error : 0.357718, accuarcy : 0.831900\n",
            "total step : 1126 \n",
            "error : 0.357561, accuarcy : 0.831900\n",
            "total step : 1127 \n",
            "error : 0.357403, accuarcy : 0.831900\n",
            "total step : 1128 \n",
            "error : 0.357245, accuarcy : 0.831900\n",
            "total step : 1129 \n",
            "error : 0.357088, accuarcy : 0.831900\n",
            "total step : 1130 \n",
            "error : 0.356931, accuarcy : 0.832000\n",
            "total step : 1131 \n",
            "error : 0.356774, accuarcy : 0.832000\n",
            "total step : 1132 \n",
            "error : 0.356618, accuarcy : 0.832000\n",
            "total step : 1133 \n",
            "error : 0.356461, accuarcy : 0.832100\n",
            "total step : 1134 \n",
            "error : 0.356305, accuarcy : 0.832100\n",
            "total step : 1135 \n",
            "error : 0.356149, accuarcy : 0.832100\n",
            "total step : 1136 \n",
            "error : 0.355993, accuarcy : 0.832000\n",
            "total step : 1137 \n",
            "error : 0.355838, accuarcy : 0.832000\n",
            "total step : 1138 \n",
            "error : 0.355682, accuarcy : 0.832000\n",
            "total step : 1139 \n",
            "error : 0.355527, accuarcy : 0.832000\n",
            "total step : 1140 \n",
            "error : 0.355372, accuarcy : 0.832100\n",
            "total step : 1141 \n",
            "error : 0.355217, accuarcy : 0.832200\n",
            "total step : 1142 \n",
            "error : 0.355062, accuarcy : 0.832200\n",
            "total step : 1143 \n",
            "error : 0.354908, accuarcy : 0.832200\n",
            "total step : 1144 \n",
            "error : 0.354754, accuarcy : 0.832300\n",
            "total step : 1145 \n",
            "error : 0.354599, accuarcy : 0.832300\n",
            "total step : 1146 \n",
            "error : 0.354446, accuarcy : 0.832400\n",
            "total step : 1147 \n",
            "error : 0.354292, accuarcy : 0.832500\n",
            "total step : 1148 \n",
            "error : 0.354138, accuarcy : 0.832500\n",
            "total step : 1149 \n",
            "error : 0.353985, accuarcy : 0.832600\n",
            "total step : 1150 \n",
            "error : 0.353832, accuarcy : 0.832600\n",
            "total step : 1151 \n",
            "error : 0.353679, accuarcy : 0.832600\n",
            "total step : 1152 \n",
            "error : 0.353526, accuarcy : 0.832700\n",
            "total step : 1153 \n",
            "error : 0.353374, accuarcy : 0.832700\n",
            "total step : 1154 \n",
            "error : 0.353221, accuarcy : 0.832700\n",
            "total step : 1155 \n",
            "error : 0.353069, accuarcy : 0.832700\n",
            "total step : 1156 \n",
            "error : 0.352917, accuarcy : 0.832800\n",
            "total step : 1157 \n",
            "error : 0.352766, accuarcy : 0.832800\n",
            "total step : 1158 \n",
            "error : 0.352614, accuarcy : 0.832800\n",
            "total step : 1159 \n",
            "error : 0.352463, accuarcy : 0.832800\n",
            "total step : 1160 \n",
            "error : 0.352311, accuarcy : 0.832800\n",
            "total step : 1161 \n",
            "error : 0.352160, accuarcy : 0.832800\n",
            "total step : 1162 \n",
            "error : 0.352009, accuarcy : 0.833000\n",
            "total step : 1163 \n",
            "error : 0.351859, accuarcy : 0.833100\n",
            "total step : 1164 \n",
            "error : 0.351708, accuarcy : 0.833200\n",
            "total step : 1165 \n",
            "error : 0.351558, accuarcy : 0.833200\n",
            "total step : 1166 \n",
            "error : 0.351408, accuarcy : 0.833200\n",
            "total step : 1167 \n",
            "error : 0.351258, accuarcy : 0.833200\n",
            "total step : 1168 \n",
            "error : 0.351108, accuarcy : 0.833200\n",
            "total step : 1169 \n",
            "error : 0.350959, accuarcy : 0.833300\n",
            "total step : 1170 \n",
            "error : 0.350809, accuarcy : 0.833300\n",
            "total step : 1171 \n",
            "error : 0.350660, accuarcy : 0.833300\n",
            "total step : 1172 \n",
            "error : 0.350511, accuarcy : 0.833300\n",
            "total step : 1173 \n",
            "error : 0.350363, accuarcy : 0.833300\n",
            "total step : 1174 \n",
            "error : 0.350214, accuarcy : 0.833300\n",
            "total step : 1175 \n",
            "error : 0.350066, accuarcy : 0.833300\n",
            "total step : 1176 \n",
            "error : 0.349917, accuarcy : 0.833500\n",
            "total step : 1177 \n",
            "error : 0.349769, accuarcy : 0.833600\n",
            "total step : 1178 \n",
            "error : 0.349621, accuarcy : 0.833600\n",
            "total step : 1179 \n",
            "error : 0.349474, accuarcy : 0.833600\n",
            "total step : 1180 \n",
            "error : 0.349326, accuarcy : 0.833600\n",
            "total step : 1181 \n",
            "error : 0.349179, accuarcy : 0.833600\n",
            "total step : 1182 \n",
            "error : 0.349032, accuarcy : 0.833700\n",
            "total step : 1183 \n",
            "error : 0.348885, accuarcy : 0.833900\n",
            "total step : 1184 \n",
            "error : 0.348738, accuarcy : 0.833900\n",
            "total step : 1185 \n",
            "error : 0.348591, accuarcy : 0.834100\n",
            "total step : 1186 \n",
            "error : 0.348445, accuarcy : 0.834100\n",
            "total step : 1187 \n",
            "error : 0.348299, accuarcy : 0.834100\n",
            "total step : 1188 \n",
            "error : 0.348153, accuarcy : 0.834100\n",
            "total step : 1189 \n",
            "error : 0.348007, accuarcy : 0.834300\n",
            "total step : 1190 \n",
            "error : 0.347861, accuarcy : 0.834300\n",
            "total step : 1191 \n",
            "error : 0.347716, accuarcy : 0.834300\n",
            "total step : 1192 \n",
            "error : 0.347570, accuarcy : 0.834300\n",
            "total step : 1193 \n",
            "error : 0.347425, accuarcy : 0.834300\n",
            "total step : 1194 \n",
            "error : 0.347280, accuarcy : 0.834200\n",
            "total step : 1195 \n",
            "error : 0.347135, accuarcy : 0.834200\n",
            "total step : 1196 \n",
            "error : 0.346991, accuarcy : 0.834200\n",
            "total step : 1197 \n",
            "error : 0.346846, accuarcy : 0.834400\n",
            "total step : 1198 \n",
            "error : 0.346702, accuarcy : 0.834400\n",
            "total step : 1199 \n",
            "error : 0.346558, accuarcy : 0.834500\n",
            "total step : 1200 \n",
            "error : 0.346414, accuarcy : 0.834600\n",
            "total step : 1201 \n",
            "error : 0.346270, accuarcy : 0.834500\n",
            "total step : 1202 \n",
            "error : 0.346126, accuarcy : 0.834500\n",
            "total step : 1203 \n",
            "error : 0.345983, accuarcy : 0.834500\n",
            "total step : 1204 \n",
            "error : 0.345840, accuarcy : 0.834700\n",
            "total step : 1205 \n",
            "error : 0.345697, accuarcy : 0.834700\n",
            "total step : 1206 \n",
            "error : 0.345554, accuarcy : 0.834700\n",
            "total step : 1207 \n",
            "error : 0.345411, accuarcy : 0.834700\n",
            "total step : 1208 \n",
            "error : 0.345269, accuarcy : 0.834700\n",
            "total step : 1209 \n",
            "error : 0.345126, accuarcy : 0.834700\n",
            "total step : 1210 \n",
            "error : 0.344984, accuarcy : 0.834700\n",
            "total step : 1211 \n",
            "error : 0.344842, accuarcy : 0.834800\n",
            "total step : 1212 \n",
            "error : 0.344700, accuarcy : 0.834800\n",
            "total step : 1213 \n",
            "error : 0.344559, accuarcy : 0.834900\n",
            "total step : 1214 \n",
            "error : 0.344417, accuarcy : 0.834900\n",
            "total step : 1215 \n",
            "error : 0.344276, accuarcy : 0.834900\n",
            "total step : 1216 \n",
            "error : 0.344135, accuarcy : 0.835000\n",
            "total step : 1217 \n",
            "error : 0.343994, accuarcy : 0.835000\n",
            "total step : 1218 \n",
            "error : 0.343853, accuarcy : 0.835100\n",
            "total step : 1219 \n",
            "error : 0.343712, accuarcy : 0.835000\n",
            "total step : 1220 \n",
            "error : 0.343572, accuarcy : 0.835100\n",
            "total step : 1221 \n",
            "error : 0.343431, accuarcy : 0.835100\n",
            "total step : 1222 \n",
            "error : 0.343291, accuarcy : 0.835100\n",
            "total step : 1223 \n",
            "error : 0.343151, accuarcy : 0.835100\n",
            "total step : 1224 \n",
            "error : 0.343012, accuarcy : 0.835100\n",
            "total step : 1225 \n",
            "error : 0.342872, accuarcy : 0.835200\n",
            "total step : 1226 \n",
            "error : 0.342732, accuarcy : 0.835200\n",
            "total step : 1227 \n",
            "error : 0.342593, accuarcy : 0.835200\n",
            "total step : 1228 \n",
            "error : 0.342454, accuarcy : 0.835400\n",
            "total step : 1229 \n",
            "error : 0.342315, accuarcy : 0.835300\n",
            "total step : 1230 \n",
            "error : 0.342176, accuarcy : 0.835300\n",
            "total step : 1231 \n",
            "error : 0.342038, accuarcy : 0.835300\n",
            "total step : 1232 \n",
            "error : 0.341899, accuarcy : 0.835200\n",
            "total step : 1233 \n",
            "error : 0.341761, accuarcy : 0.835300\n",
            "total step : 1234 \n",
            "error : 0.341623, accuarcy : 0.835300\n",
            "total step : 1235 \n",
            "error : 0.341485, accuarcy : 0.835300\n",
            "total step : 1236 \n",
            "error : 0.341347, accuarcy : 0.835300\n",
            "total step : 1237 \n",
            "error : 0.341209, accuarcy : 0.835300\n",
            "total step : 1238 \n",
            "error : 0.341072, accuarcy : 0.835300\n",
            "total step : 1239 \n",
            "error : 0.340935, accuarcy : 0.835300\n",
            "total step : 1240 \n",
            "error : 0.340798, accuarcy : 0.835400\n",
            "total step : 1241 \n",
            "error : 0.340661, accuarcy : 0.835600\n",
            "total step : 1242 \n",
            "error : 0.340524, accuarcy : 0.835500\n",
            "total step : 1243 \n",
            "error : 0.340387, accuarcy : 0.835500\n",
            "total step : 1244 \n",
            "error : 0.340251, accuarcy : 0.835600\n",
            "total step : 1245 \n",
            "error : 0.340114, accuarcy : 0.835500\n",
            "total step : 1246 \n",
            "error : 0.339978, accuarcy : 0.835500\n",
            "total step : 1247 \n",
            "error : 0.339842, accuarcy : 0.835600\n",
            "total step : 1248 \n",
            "error : 0.339706, accuarcy : 0.835600\n",
            "total step : 1249 \n",
            "error : 0.339571, accuarcy : 0.835800\n",
            "total step : 1250 \n",
            "error : 0.339435, accuarcy : 0.835800\n",
            "total step : 1251 \n",
            "error : 0.339300, accuarcy : 0.835800\n",
            "total step : 1252 \n",
            "error : 0.339165, accuarcy : 0.835800\n",
            "total step : 1253 \n",
            "error : 0.339030, accuarcy : 0.835800\n",
            "total step : 1254 \n",
            "error : 0.338895, accuarcy : 0.835800\n",
            "total step : 1255 \n",
            "error : 0.338760, accuarcy : 0.835900\n",
            "total step : 1256 \n",
            "error : 0.338626, accuarcy : 0.836100\n",
            "total step : 1257 \n",
            "error : 0.338491, accuarcy : 0.836100\n",
            "total step : 1258 \n",
            "error : 0.338357, accuarcy : 0.836100\n",
            "total step : 1259 \n",
            "error : 0.338223, accuarcy : 0.836100\n",
            "total step : 1260 \n",
            "error : 0.338089, accuarcy : 0.836000\n",
            "total step : 1261 \n",
            "error : 0.337955, accuarcy : 0.836000\n",
            "total step : 1262 \n",
            "error : 0.337822, accuarcy : 0.836100\n",
            "total step : 1263 \n",
            "error : 0.337688, accuarcy : 0.836100\n",
            "total step : 1264 \n",
            "error : 0.337555, accuarcy : 0.836100\n",
            "total step : 1265 \n",
            "error : 0.337422, accuarcy : 0.836300\n",
            "total step : 1266 \n",
            "error : 0.337289, accuarcy : 0.836500\n",
            "total step : 1267 \n",
            "error : 0.337156, accuarcy : 0.836700\n",
            "total step : 1268 \n",
            "error : 0.337023, accuarcy : 0.836700\n",
            "total step : 1269 \n",
            "error : 0.336891, accuarcy : 0.836700\n",
            "total step : 1270 \n",
            "error : 0.336759, accuarcy : 0.836800\n",
            "total step : 1271 \n",
            "error : 0.336626, accuarcy : 0.836900\n",
            "total step : 1272 \n",
            "error : 0.336494, accuarcy : 0.836900\n",
            "total step : 1273 \n",
            "error : 0.336363, accuarcy : 0.837000\n",
            "total step : 1274 \n",
            "error : 0.336231, accuarcy : 0.837000\n",
            "total step : 1275 \n",
            "error : 0.336099, accuarcy : 0.837000\n",
            "total step : 1276 \n",
            "error : 0.335968, accuarcy : 0.837000\n",
            "total step : 1277 \n",
            "error : 0.335837, accuarcy : 0.837000\n",
            "total step : 1278 \n",
            "error : 0.335706, accuarcy : 0.837000\n",
            "total step : 1279 \n",
            "error : 0.335575, accuarcy : 0.837200\n",
            "total step : 1280 \n",
            "error : 0.335444, accuarcy : 0.837200\n",
            "total step : 1281 \n",
            "error : 0.335313, accuarcy : 0.837200\n",
            "total step : 1282 \n",
            "error : 0.335183, accuarcy : 0.837200\n",
            "total step : 1283 \n",
            "error : 0.335052, accuarcy : 0.837400\n",
            "total step : 1284 \n",
            "error : 0.334922, accuarcy : 0.837400\n",
            "total step : 1285 \n",
            "error : 0.334792, accuarcy : 0.837400\n",
            "total step : 1286 \n",
            "error : 0.334662, accuarcy : 0.837500\n",
            "total step : 1287 \n",
            "error : 0.334533, accuarcy : 0.837500\n",
            "total step : 1288 \n",
            "error : 0.334403, accuarcy : 0.837500\n",
            "total step : 1289 \n",
            "error : 0.334274, accuarcy : 0.837500\n",
            "total step : 1290 \n",
            "error : 0.334144, accuarcy : 0.837500\n",
            "total step : 1291 \n",
            "error : 0.334015, accuarcy : 0.837500\n",
            "total step : 1292 \n",
            "error : 0.333886, accuarcy : 0.837600\n",
            "total step : 1293 \n",
            "error : 0.333757, accuarcy : 0.837600\n",
            "total step : 1294 \n",
            "error : 0.333629, accuarcy : 0.837600\n",
            "total step : 1295 \n",
            "error : 0.333500, accuarcy : 0.837600\n",
            "total step : 1296 \n",
            "error : 0.333372, accuarcy : 0.837600\n",
            "total step : 1297 \n",
            "error : 0.333244, accuarcy : 0.837600\n",
            "total step : 1298 \n",
            "error : 0.333116, accuarcy : 0.837600\n",
            "total step : 1299 \n",
            "error : 0.332988, accuarcy : 0.837600\n",
            "total step : 1300 \n",
            "error : 0.332860, accuarcy : 0.837700\n",
            "total step : 1301 \n",
            "error : 0.332732, accuarcy : 0.837600\n",
            "total step : 1302 \n",
            "error : 0.332605, accuarcy : 0.837600\n",
            "total step : 1303 \n",
            "error : 0.332478, accuarcy : 0.837600\n",
            "total step : 1304 \n",
            "error : 0.332350, accuarcy : 0.837700\n",
            "total step : 1305 \n",
            "error : 0.332223, accuarcy : 0.837700\n",
            "total step : 1306 \n",
            "error : 0.332096, accuarcy : 0.837700\n",
            "total step : 1307 \n",
            "error : 0.331970, accuarcy : 0.837800\n",
            "total step : 1308 \n",
            "error : 0.331843, accuarcy : 0.837800\n",
            "total step : 1309 \n",
            "error : 0.331717, accuarcy : 0.837500\n",
            "total step : 1310 \n",
            "error : 0.331590, accuarcy : 0.837600\n",
            "total step : 1311 \n",
            "error : 0.331464, accuarcy : 0.837600\n",
            "total step : 1312 \n",
            "error : 0.331338, accuarcy : 0.837600\n",
            "total step : 1313 \n",
            "error : 0.331212, accuarcy : 0.837600\n",
            "total step : 1314 \n",
            "error : 0.331087, accuarcy : 0.837600\n",
            "total step : 1315 \n",
            "error : 0.330961, accuarcy : 0.837500\n",
            "total step : 1316 \n",
            "error : 0.330836, accuarcy : 0.837700\n",
            "total step : 1317 \n",
            "error : 0.330710, accuarcy : 0.837700\n",
            "total step : 1318 \n",
            "error : 0.330585, accuarcy : 0.837600\n",
            "total step : 1319 \n",
            "error : 0.330460, accuarcy : 0.837600\n",
            "total step : 1320 \n",
            "error : 0.330335, accuarcy : 0.837600\n",
            "total step : 1321 \n",
            "error : 0.330211, accuarcy : 0.837700\n",
            "total step : 1322 \n",
            "error : 0.330086, accuarcy : 0.837800\n",
            "total step : 1323 \n",
            "error : 0.329962, accuarcy : 0.837800\n",
            "total step : 1324 \n",
            "error : 0.329838, accuarcy : 0.837800\n",
            "total step : 1325 \n",
            "error : 0.329713, accuarcy : 0.837800\n",
            "total step : 1326 \n",
            "error : 0.329589, accuarcy : 0.837800\n",
            "total step : 1327 \n",
            "error : 0.329466, accuarcy : 0.837900\n",
            "total step : 1328 \n",
            "error : 0.329342, accuarcy : 0.837900\n",
            "total step : 1329 \n",
            "error : 0.329218, accuarcy : 0.837900\n",
            "total step : 1330 \n",
            "error : 0.329095, accuarcy : 0.837900\n",
            "total step : 1331 \n",
            "error : 0.328972, accuarcy : 0.837900\n",
            "total step : 1332 \n",
            "error : 0.328848, accuarcy : 0.837900\n",
            "total step : 1333 \n",
            "error : 0.328725, accuarcy : 0.837900\n",
            "total step : 1334 \n",
            "error : 0.328603, accuarcy : 0.837900\n",
            "total step : 1335 \n",
            "error : 0.328480, accuarcy : 0.838000\n",
            "total step : 1336 \n",
            "error : 0.328357, accuarcy : 0.838000\n",
            "total step : 1337 \n",
            "error : 0.328235, accuarcy : 0.837900\n",
            "total step : 1338 \n",
            "error : 0.328113, accuarcy : 0.837800\n",
            "total step : 1339 \n",
            "error : 0.327990, accuarcy : 0.837900\n",
            "total step : 1340 \n",
            "error : 0.327868, accuarcy : 0.837900\n",
            "total step : 1341 \n",
            "error : 0.327746, accuarcy : 0.838000\n",
            "total step : 1342 \n",
            "error : 0.327625, accuarcy : 0.838100\n",
            "total step : 1343 \n",
            "error : 0.327503, accuarcy : 0.838100\n",
            "total step : 1344 \n",
            "error : 0.327382, accuarcy : 0.838100\n",
            "total step : 1345 \n",
            "error : 0.327260, accuarcy : 0.838100\n",
            "total step : 1346 \n",
            "error : 0.327139, accuarcy : 0.838100\n",
            "total step : 1347 \n",
            "error : 0.327018, accuarcy : 0.838200\n",
            "total step : 1348 \n",
            "error : 0.326897, accuarcy : 0.838400\n",
            "total step : 1349 \n",
            "error : 0.326776, accuarcy : 0.838400\n",
            "total step : 1350 \n",
            "error : 0.326656, accuarcy : 0.838400\n",
            "total step : 1351 \n",
            "error : 0.326535, accuarcy : 0.838400\n",
            "total step : 1352 \n",
            "error : 0.326415, accuarcy : 0.838400\n",
            "total step : 1353 \n",
            "error : 0.326295, accuarcy : 0.838400\n",
            "total step : 1354 \n",
            "error : 0.326175, accuarcy : 0.838400\n",
            "total step : 1355 \n",
            "error : 0.326055, accuarcy : 0.838400\n",
            "total step : 1356 \n",
            "error : 0.325935, accuarcy : 0.838400\n",
            "total step : 1357 \n",
            "error : 0.325815, accuarcy : 0.838300\n",
            "total step : 1358 \n",
            "error : 0.325696, accuarcy : 0.838300\n",
            "total step : 1359 \n",
            "error : 0.325576, accuarcy : 0.838300\n",
            "total step : 1360 \n",
            "error : 0.325457, accuarcy : 0.838300\n",
            "total step : 1361 \n",
            "error : 0.325338, accuarcy : 0.838200\n",
            "total step : 1362 \n",
            "error : 0.325219, accuarcy : 0.838200\n",
            "total step : 1363 \n",
            "error : 0.325100, accuarcy : 0.838200\n",
            "total step : 1364 \n",
            "error : 0.324981, accuarcy : 0.838200\n",
            "total step : 1365 \n",
            "error : 0.324862, accuarcy : 0.838200\n",
            "total step : 1366 \n",
            "error : 0.324744, accuarcy : 0.838100\n",
            "total step : 1367 \n",
            "error : 0.324626, accuarcy : 0.838100\n",
            "total step : 1368 \n",
            "error : 0.324507, accuarcy : 0.838100\n",
            "total step : 1369 \n",
            "error : 0.324389, accuarcy : 0.838100\n",
            "total step : 1370 \n",
            "error : 0.324271, accuarcy : 0.838100\n",
            "total step : 1371 \n",
            "error : 0.324153, accuarcy : 0.838200\n",
            "total step : 1372 \n",
            "error : 0.324036, accuarcy : 0.838300\n",
            "total step : 1373 \n",
            "error : 0.323918, accuarcy : 0.838300\n",
            "total step : 1374 \n",
            "error : 0.323801, accuarcy : 0.838300\n",
            "total step : 1375 \n",
            "error : 0.323683, accuarcy : 0.838400\n",
            "total step : 1376 \n",
            "error : 0.323566, accuarcy : 0.838300\n",
            "total step : 1377 \n",
            "error : 0.323449, accuarcy : 0.838300\n",
            "total step : 1378 \n",
            "error : 0.323332, accuarcy : 0.838300\n",
            "total step : 1379 \n",
            "error : 0.323216, accuarcy : 0.838400\n",
            "total step : 1380 \n",
            "error : 0.323099, accuarcy : 0.838200\n",
            "total step : 1381 \n",
            "error : 0.322982, accuarcy : 0.838100\n",
            "total step : 1382 \n",
            "error : 0.322866, accuarcy : 0.838100\n",
            "total step : 1383 \n",
            "error : 0.322750, accuarcy : 0.838000\n",
            "total step : 1384 \n",
            "error : 0.322634, accuarcy : 0.838100\n",
            "total step : 1385 \n",
            "error : 0.322518, accuarcy : 0.838100\n",
            "total step : 1386 \n",
            "error : 0.322402, accuarcy : 0.838200\n",
            "total step : 1387 \n",
            "error : 0.322286, accuarcy : 0.838100\n",
            "total step : 1388 \n",
            "error : 0.322171, accuarcy : 0.838200\n",
            "total step : 1389 \n",
            "error : 0.322055, accuarcy : 0.838300\n",
            "total step : 1390 \n",
            "error : 0.321940, accuarcy : 0.838400\n",
            "total step : 1391 \n",
            "error : 0.321824, accuarcy : 0.838400\n",
            "total step : 1392 \n",
            "error : 0.321709, accuarcy : 0.838400\n",
            "total step : 1393 \n",
            "error : 0.321594, accuarcy : 0.838500\n",
            "total step : 1394 \n",
            "error : 0.321480, accuarcy : 0.838600\n",
            "total step : 1395 \n",
            "error : 0.321365, accuarcy : 0.838600\n",
            "total step : 1396 \n",
            "error : 0.321250, accuarcy : 0.838600\n",
            "total step : 1397 \n",
            "error : 0.321136, accuarcy : 0.838600\n",
            "total step : 1398 \n",
            "error : 0.321022, accuarcy : 0.838600\n",
            "total step : 1399 \n",
            "error : 0.320907, accuarcy : 0.838600\n",
            "total step : 1400 \n",
            "error : 0.320793, accuarcy : 0.838600\n",
            "total step : 1401 \n",
            "error : 0.320679, accuarcy : 0.838600\n",
            "total step : 1402 \n",
            "error : 0.320565, accuarcy : 0.838600\n",
            "total step : 1403 \n",
            "error : 0.320452, accuarcy : 0.838600\n",
            "total step : 1404 \n",
            "error : 0.320338, accuarcy : 0.838600\n",
            "total step : 1405 \n",
            "error : 0.320225, accuarcy : 0.838700\n",
            "total step : 1406 \n",
            "error : 0.320111, accuarcy : 0.838600\n",
            "total step : 1407 \n",
            "error : 0.319998, accuarcy : 0.838600\n",
            "total step : 1408 \n",
            "error : 0.319885, accuarcy : 0.838500\n",
            "total step : 1409 \n",
            "error : 0.319772, accuarcy : 0.838400\n",
            "total step : 1410 \n",
            "error : 0.319659, accuarcy : 0.838400\n",
            "total step : 1411 \n",
            "error : 0.319547, accuarcy : 0.838500\n",
            "total step : 1412 \n",
            "error : 0.319434, accuarcy : 0.838500\n",
            "total step : 1413 \n",
            "error : 0.319322, accuarcy : 0.838500\n",
            "total step : 1414 \n",
            "error : 0.319209, accuarcy : 0.838600\n",
            "total step : 1415 \n",
            "error : 0.319097, accuarcy : 0.838500\n",
            "total step : 1416 \n",
            "error : 0.318985, accuarcy : 0.838500\n",
            "total step : 1417 \n",
            "error : 0.318873, accuarcy : 0.838500\n",
            "total step : 1418 \n",
            "error : 0.318761, accuarcy : 0.838600\n",
            "total step : 1419 \n",
            "error : 0.318650, accuarcy : 0.838600\n",
            "total step : 1420 \n",
            "error : 0.318538, accuarcy : 0.838500\n",
            "total step : 1421 \n",
            "error : 0.318426, accuarcy : 0.838500\n",
            "total step : 1422 \n",
            "error : 0.318315, accuarcy : 0.838600\n",
            "total step : 1423 \n",
            "error : 0.318204, accuarcy : 0.838600\n",
            "total step : 1424 \n",
            "error : 0.318093, accuarcy : 0.838700\n",
            "total step : 1425 \n",
            "error : 0.317982, accuarcy : 0.838700\n",
            "total step : 1426 \n",
            "error : 0.317871, accuarcy : 0.838700\n",
            "total step : 1427 \n",
            "error : 0.317760, accuarcy : 0.838700\n",
            "total step : 1428 \n",
            "error : 0.317650, accuarcy : 0.838700\n",
            "total step : 1429 \n",
            "error : 0.317539, accuarcy : 0.838700\n",
            "total step : 1430 \n",
            "error : 0.317429, accuarcy : 0.838700\n",
            "total step : 1431 \n",
            "error : 0.317318, accuarcy : 0.838600\n",
            "total step : 1432 \n",
            "error : 0.317208, accuarcy : 0.838700\n",
            "total step : 1433 \n",
            "error : 0.317098, accuarcy : 0.838600\n",
            "total step : 1434 \n",
            "error : 0.316988, accuarcy : 0.838700\n",
            "total step : 1435 \n",
            "error : 0.316879, accuarcy : 0.838700\n",
            "total step : 1436 \n",
            "error : 0.316769, accuarcy : 0.838700\n",
            "total step : 1437 \n",
            "error : 0.316659, accuarcy : 0.838800\n",
            "total step : 1438 \n",
            "error : 0.316550, accuarcy : 0.838800\n",
            "total step : 1439 \n",
            "error : 0.316441, accuarcy : 0.838900\n",
            "total step : 1440 \n",
            "error : 0.316331, accuarcy : 0.839000\n",
            "total step : 1441 \n",
            "error : 0.316222, accuarcy : 0.839000\n",
            "total step : 1442 \n",
            "error : 0.316113, accuarcy : 0.839000\n",
            "total step : 1443 \n",
            "error : 0.316005, accuarcy : 0.839100\n",
            "total step : 1444 \n",
            "error : 0.315896, accuarcy : 0.839200\n",
            "total step : 1445 \n",
            "error : 0.315787, accuarcy : 0.839200\n",
            "total step : 1446 \n",
            "error : 0.315679, accuarcy : 0.839400\n",
            "total step : 1447 \n",
            "error : 0.315570, accuarcy : 0.839500\n",
            "total step : 1448 \n",
            "error : 0.315462, accuarcy : 0.839800\n",
            "total step : 1449 \n",
            "error : 0.315354, accuarcy : 0.839900\n",
            "total step : 1450 \n",
            "error : 0.315246, accuarcy : 0.839900\n",
            "total step : 1451 \n",
            "error : 0.315138, accuarcy : 0.839800\n",
            "total step : 1452 \n",
            "error : 0.315030, accuarcy : 0.839700\n",
            "total step : 1453 \n",
            "error : 0.314923, accuarcy : 0.839800\n",
            "total step : 1454 \n",
            "error : 0.314815, accuarcy : 0.839900\n",
            "total step : 1455 \n",
            "error : 0.314708, accuarcy : 0.840100\n",
            "total step : 1456 \n",
            "error : 0.314600, accuarcy : 0.840200\n",
            "total step : 1457 \n",
            "error : 0.314493, accuarcy : 0.840200\n",
            "total step : 1458 \n",
            "error : 0.314386, accuarcy : 0.840100\n",
            "total step : 1459 \n",
            "error : 0.314279, accuarcy : 0.840300\n",
            "total step : 1460 \n",
            "error : 0.314172, accuarcy : 0.840300\n",
            "total step : 1461 \n",
            "error : 0.314066, accuarcy : 0.840300\n",
            "total step : 1462 \n",
            "error : 0.313959, accuarcy : 0.840300\n",
            "total step : 1463 \n",
            "error : 0.313852, accuarcy : 0.840300\n",
            "total step : 1464 \n",
            "error : 0.313746, accuarcy : 0.840300\n",
            "total step : 1465 \n",
            "error : 0.313640, accuarcy : 0.840400\n",
            "total step : 1466 \n",
            "error : 0.313534, accuarcy : 0.840500\n",
            "total step : 1467 \n",
            "error : 0.313428, accuarcy : 0.840500\n",
            "total step : 1468 \n",
            "error : 0.313322, accuarcy : 0.840400\n",
            "total step : 1469 \n",
            "error : 0.313216, accuarcy : 0.840400\n",
            "total step : 1470 \n",
            "error : 0.313110, accuarcy : 0.840400\n",
            "total step : 1471 \n",
            "error : 0.313004, accuarcy : 0.840300\n",
            "total step : 1472 \n",
            "error : 0.312899, accuarcy : 0.840400\n",
            "total step : 1473 \n",
            "error : 0.312794, accuarcy : 0.840400\n",
            "total step : 1474 \n",
            "error : 0.312688, accuarcy : 0.840500\n",
            "total step : 1475 \n",
            "error : 0.312583, accuarcy : 0.840500\n",
            "total step : 1476 \n",
            "error : 0.312478, accuarcy : 0.840500\n",
            "total step : 1477 \n",
            "error : 0.312373, accuarcy : 0.840500\n",
            "total step : 1478 \n",
            "error : 0.312268, accuarcy : 0.840500\n",
            "total step : 1479 \n",
            "error : 0.312164, accuarcy : 0.840600\n",
            "total step : 1480 \n",
            "error : 0.312059, accuarcy : 0.840600\n",
            "total step : 1481 \n",
            "error : 0.311955, accuarcy : 0.840600\n",
            "total step : 1482 \n",
            "error : 0.311850, accuarcy : 0.840600\n",
            "total step : 1483 \n",
            "error : 0.311746, accuarcy : 0.840600\n",
            "total step : 1484 \n",
            "error : 0.311642, accuarcy : 0.840600\n",
            "total step : 1485 \n",
            "error : 0.311538, accuarcy : 0.840600\n",
            "total step : 1486 \n",
            "error : 0.311434, accuarcy : 0.840700\n",
            "total step : 1487 \n",
            "error : 0.311330, accuarcy : 0.840800\n",
            "total step : 1488 \n",
            "error : 0.311226, accuarcy : 0.840800\n",
            "total step : 1489 \n",
            "error : 0.311123, accuarcy : 0.840900\n",
            "total step : 1490 \n",
            "error : 0.311019, accuarcy : 0.841000\n",
            "total step : 1491 \n",
            "error : 0.310916, accuarcy : 0.841000\n",
            "total step : 1492 \n",
            "error : 0.310813, accuarcy : 0.840900\n",
            "total step : 1493 \n",
            "error : 0.310709, accuarcy : 0.841100\n",
            "total step : 1494 \n",
            "error : 0.310606, accuarcy : 0.841100\n",
            "total step : 1495 \n",
            "error : 0.310503, accuarcy : 0.841200\n",
            "total step : 1496 \n",
            "error : 0.310401, accuarcy : 0.841100\n",
            "total step : 1497 \n",
            "error : 0.310298, accuarcy : 0.841100\n",
            "total step : 1498 \n",
            "error : 0.310195, accuarcy : 0.841200\n",
            "total step : 1499 \n",
            "error : 0.310093, accuarcy : 0.841100\n",
            "total step : 1500 \n",
            "error : 0.309990, accuarcy : 0.841000\n",
            "total step : 1501 \n",
            "error : 0.309888, accuarcy : 0.841000\n",
            "total step : 1502 \n",
            "error : 0.309786, accuarcy : 0.841100\n",
            "total step : 1503 \n",
            "error : 0.309684, accuarcy : 0.841000\n",
            "total step : 1504 \n",
            "error : 0.309582, accuarcy : 0.841000\n",
            "total step : 1505 \n",
            "error : 0.309480, accuarcy : 0.841100\n",
            "total step : 1506 \n",
            "error : 0.309378, accuarcy : 0.841100\n",
            "total step : 1507 \n",
            "error : 0.309277, accuarcy : 0.841200\n",
            "total step : 1508 \n",
            "error : 0.309175, accuarcy : 0.841000\n",
            "total step : 1509 \n",
            "error : 0.309074, accuarcy : 0.841000\n",
            "total step : 1510 \n",
            "error : 0.308972, accuarcy : 0.841000\n",
            "total step : 1511 \n",
            "error : 0.308871, accuarcy : 0.841100\n",
            "total step : 1512 \n",
            "error : 0.308770, accuarcy : 0.841100\n",
            "total step : 1513 \n",
            "error : 0.308669, accuarcy : 0.841100\n",
            "total step : 1514 \n",
            "error : 0.308568, accuarcy : 0.841000\n",
            "total step : 1515 \n",
            "error : 0.308467, accuarcy : 0.841000\n",
            "total step : 1516 \n",
            "error : 0.308367, accuarcy : 0.841000\n",
            "total step : 1517 \n",
            "error : 0.308266, accuarcy : 0.841000\n",
            "total step : 1518 \n",
            "error : 0.308165, accuarcy : 0.841000\n",
            "total step : 1519 \n",
            "error : 0.308065, accuarcy : 0.841000\n",
            "total step : 1520 \n",
            "error : 0.307965, accuarcy : 0.841000\n",
            "total step : 1521 \n",
            "error : 0.307865, accuarcy : 0.841100\n",
            "total step : 1522 \n",
            "error : 0.307765, accuarcy : 0.841100\n"
          ]
        }
      ],
      "source": [
        "train_8X, train_8y = make_sample(idx = 8) # idx = target number\n",
        "train_8X = np.insert(train_8X, 0, 1, axis=1) # bias 추가\n",
        "w8,J8_history, ACC8_history = train(train_8X, train_8y,8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total step : 1 \n",
            "error : 3.671103, accuarcy : 0.678600\n",
            "total step : 2 \n",
            "error : 3.290250, accuarcy : 0.637100\n",
            "total step : 3 \n",
            "error : 3.021783, accuarcy : 0.603400\n",
            "total step : 4 \n",
            "error : 2.833153, accuarcy : 0.576200\n",
            "total step : 5 \n",
            "error : 2.694916, accuarcy : 0.557600\n",
            "total step : 6 \n",
            "error : 2.587514, accuarcy : 0.543700\n",
            "total step : 7 \n",
            "error : 2.499392, accuarcy : 0.537100\n",
            "total step : 8 \n",
            "error : 2.423664, accuarcy : 0.534700\n",
            "total step : 9 \n",
            "error : 2.356177, accuarcy : 0.533600\n",
            "total step : 10 \n",
            "error : 2.294466, accuarcy : 0.534100\n",
            "total step : 11 \n",
            "error : 2.237094, accuarcy : 0.536300\n",
            "total step : 12 \n",
            "error : 2.183220, accuarcy : 0.541600\n",
            "total step : 13 \n",
            "error : 2.132338, accuarcy : 0.546000\n",
            "total step : 14 \n",
            "error : 2.084127, accuarcy : 0.550200\n",
            "total step : 15 \n",
            "error : 2.038368, accuarcy : 0.556000\n",
            "total step : 16 \n",
            "error : 1.994892, accuarcy : 0.560100\n",
            "total step : 17 \n",
            "error : 1.953560, accuarcy : 0.564900\n",
            "total step : 18 \n",
            "error : 1.914246, accuarcy : 0.570100\n",
            "total step : 19 \n",
            "error : 1.876834, accuarcy : 0.574400\n",
            "total step : 20 \n",
            "error : 1.841213, accuarcy : 0.579000\n",
            "total step : 21 \n",
            "error : 1.807276, accuarcy : 0.585000\n",
            "total step : 22 \n",
            "error : 1.774919, accuarcy : 0.588800\n",
            "total step : 23 \n",
            "error : 1.744045, accuarcy : 0.593300\n",
            "total step : 24 \n",
            "error : 1.714563, accuarcy : 0.599800\n",
            "total step : 25 \n",
            "error : 1.686386, accuarcy : 0.604300\n",
            "total step : 26 \n",
            "error : 1.659433, accuarcy : 0.608500\n",
            "total step : 27 \n",
            "error : 1.633631, accuarcy : 0.612500\n",
            "total step : 28 \n",
            "error : 1.608910, accuarcy : 0.616700\n",
            "total step : 29 \n",
            "error : 1.585205, accuarcy : 0.620300\n",
            "total step : 30 \n",
            "error : 1.562458, accuarcy : 0.623400\n",
            "total step : 31 \n",
            "error : 1.540612, accuarcy : 0.626100\n",
            "total step : 32 \n",
            "error : 1.519615, accuarcy : 0.629700\n",
            "total step : 33 \n",
            "error : 1.499420, accuarcy : 0.633500\n",
            "total step : 34 \n",
            "error : 1.479979, accuarcy : 0.636700\n",
            "total step : 35 \n",
            "error : 1.461251, accuarcy : 0.640100\n",
            "total step : 36 \n",
            "error : 1.443196, accuarcy : 0.643600\n",
            "total step : 37 \n",
            "error : 1.425777, accuarcy : 0.646700\n",
            "total step : 38 \n",
            "error : 1.408958, accuarcy : 0.650100\n",
            "total step : 39 \n",
            "error : 1.392709, accuarcy : 0.652100\n",
            "total step : 40 \n",
            "error : 1.376998, accuarcy : 0.655400\n",
            "total step : 41 \n",
            "error : 1.361797, accuarcy : 0.657700\n",
            "total step : 42 \n",
            "error : 1.347081, accuarcy : 0.659900\n",
            "total step : 43 \n",
            "error : 1.332825, accuarcy : 0.662000\n",
            "total step : 44 \n",
            "error : 1.319006, accuarcy : 0.664700\n",
            "total step : 45 \n",
            "error : 1.305602, accuarcy : 0.667100\n",
            "total step : 46 \n",
            "error : 1.292594, accuarcy : 0.670000\n",
            "total step : 47 \n",
            "error : 1.279963, accuarcy : 0.671500\n",
            "total step : 48 \n",
            "error : 1.267691, accuarcy : 0.674300\n",
            "total step : 49 \n",
            "error : 1.255761, accuarcy : 0.676200\n",
            "total step : 50 \n",
            "error : 1.244157, accuarcy : 0.678000\n",
            "total step : 51 \n",
            "error : 1.232865, accuarcy : 0.680100\n",
            "total step : 52 \n",
            "error : 1.221872, accuarcy : 0.681700\n",
            "total step : 53 \n",
            "error : 1.211163, accuarcy : 0.683500\n",
            "total step : 54 \n",
            "error : 1.200726, accuarcy : 0.685300\n",
            "total step : 55 \n",
            "error : 1.190551, accuarcy : 0.687000\n",
            "total step : 56 \n",
            "error : 1.180626, accuarcy : 0.689000\n",
            "total step : 57 \n",
            "error : 1.170940, accuarcy : 0.690800\n",
            "total step : 58 \n",
            "error : 1.161485, accuarcy : 0.693600\n",
            "total step : 59 \n",
            "error : 1.152250, accuarcy : 0.695300\n",
            "total step : 60 \n",
            "error : 1.143228, accuarcy : 0.697300\n",
            "total step : 61 \n",
            "error : 1.134410, accuarcy : 0.699300\n",
            "total step : 62 \n",
            "error : 1.125789, accuarcy : 0.701000\n",
            "total step : 63 \n",
            "error : 1.117357, accuarcy : 0.702800\n",
            "total step : 64 \n",
            "error : 1.109107, accuarcy : 0.703400\n",
            "total step : 65 \n",
            "error : 1.101032, accuarcy : 0.704900\n",
            "total step : 66 \n",
            "error : 1.093128, accuarcy : 0.707200\n",
            "total step : 67 \n",
            "error : 1.085386, accuarcy : 0.709500\n",
            "total step : 68 \n",
            "error : 1.077803, accuarcy : 0.712200\n",
            "total step : 69 \n",
            "error : 1.070372, accuarcy : 0.714200\n",
            "total step : 70 \n",
            "error : 1.063089, accuarcy : 0.715400\n",
            "total step : 71 \n",
            "error : 1.055949, accuarcy : 0.717300\n",
            "total step : 72 \n",
            "error : 1.048947, accuarcy : 0.719200\n",
            "total step : 73 \n",
            "error : 1.042078, accuarcy : 0.719800\n",
            "total step : 74 \n",
            "error : 1.035340, accuarcy : 0.720600\n",
            "total step : 75 \n",
            "error : 1.028727, accuarcy : 0.721700\n",
            "total step : 76 \n",
            "error : 1.022236, accuarcy : 0.723300\n",
            "total step : 77 \n",
            "error : 1.015863, accuarcy : 0.724300\n",
            "total step : 78 \n",
            "error : 1.009605, accuarcy : 0.725400\n",
            "total step : 79 \n",
            "error : 1.003459, accuarcy : 0.726900\n",
            "total step : 80 \n",
            "error : 0.997421, accuarcy : 0.728100\n",
            "total step : 81 \n",
            "error : 0.991488, accuarcy : 0.728500\n",
            "total step : 82 \n",
            "error : 0.985658, accuarcy : 0.730000\n",
            "total step : 83 \n",
            "error : 0.979927, accuarcy : 0.730900\n",
            "total step : 84 \n",
            "error : 0.974293, accuarcy : 0.732600\n",
            "total step : 85 \n",
            "error : 0.968753, accuarcy : 0.733600\n",
            "total step : 86 \n",
            "error : 0.963305, accuarcy : 0.734900\n",
            "total step : 87 \n",
            "error : 0.957946, accuarcy : 0.735900\n",
            "total step : 88 \n",
            "error : 0.952674, accuarcy : 0.736800\n",
            "total step : 89 \n",
            "error : 0.947487, accuarcy : 0.737600\n",
            "total step : 90 \n",
            "error : 0.942383, accuarcy : 0.737900\n",
            "total step : 91 \n",
            "error : 0.937358, accuarcy : 0.739500\n",
            "total step : 92 \n",
            "error : 0.932413, accuarcy : 0.741000\n",
            "total step : 93 \n",
            "error : 0.927543, accuarcy : 0.742500\n",
            "total step : 94 \n",
            "error : 0.922748, accuarcy : 0.743400\n",
            "total step : 95 \n",
            "error : 0.918026, accuarcy : 0.744500\n",
            "total step : 96 \n",
            "error : 0.913375, accuarcy : 0.745700\n",
            "total step : 97 \n",
            "error : 0.908793, accuarcy : 0.746400\n",
            "total step : 98 \n",
            "error : 0.904278, accuarcy : 0.747300\n",
            "total step : 99 \n",
            "error : 0.899830, accuarcy : 0.748200\n",
            "total step : 100 \n",
            "error : 0.895445, accuarcy : 0.748900\n",
            "total step : 101 \n",
            "error : 0.891124, accuarcy : 0.749500\n",
            "total step : 102 \n",
            "error : 0.886863, accuarcy : 0.750200\n",
            "total step : 103 \n",
            "error : 0.882663, accuarcy : 0.750300\n",
            "total step : 104 \n",
            "error : 0.878521, accuarcy : 0.751200\n",
            "total step : 105 \n",
            "error : 0.874437, accuarcy : 0.751700\n",
            "total step : 106 \n",
            "error : 0.870408, accuarcy : 0.752900\n",
            "total step : 107 \n",
            "error : 0.866434, accuarcy : 0.753900\n",
            "total step : 108 \n",
            "error : 0.862513, accuarcy : 0.754900\n",
            "total step : 109 \n",
            "error : 0.858645, accuarcy : 0.755800\n",
            "total step : 110 \n",
            "error : 0.854827, accuarcy : 0.756500\n",
            "total step : 111 \n",
            "error : 0.851060, accuarcy : 0.756900\n",
            "total step : 112 \n",
            "error : 0.847341, accuarcy : 0.757800\n",
            "total step : 113 \n",
            "error : 0.843670, accuarcy : 0.759000\n",
            "total step : 114 \n",
            "error : 0.840046, accuarcy : 0.759600\n",
            "total step : 115 \n",
            "error : 0.836468, accuarcy : 0.760600\n",
            "total step : 116 \n",
            "error : 0.832935, accuarcy : 0.761000\n",
            "total step : 117 \n",
            "error : 0.829446, accuarcy : 0.761500\n",
            "total step : 118 \n",
            "error : 0.825999, accuarcy : 0.762400\n",
            "total step : 119 \n",
            "error : 0.822595, accuarcy : 0.763300\n",
            "total step : 120 \n",
            "error : 0.819232, accuarcy : 0.764000\n",
            "total step : 121 \n",
            "error : 0.815909, accuarcy : 0.764200\n",
            "total step : 122 \n",
            "error : 0.812627, accuarcy : 0.764500\n",
            "total step : 123 \n",
            "error : 0.809383, accuarcy : 0.764800\n",
            "total step : 124 \n",
            "error : 0.806177, accuarcy : 0.765700\n",
            "total step : 125 \n",
            "error : 0.803008, accuarcy : 0.766200\n",
            "total step : 126 \n",
            "error : 0.799876, accuarcy : 0.766700\n",
            "total step : 127 \n",
            "error : 0.796780, accuarcy : 0.767200\n",
            "total step : 128 \n",
            "error : 0.793720, accuarcy : 0.768400\n",
            "total step : 129 \n",
            "error : 0.790693, accuarcy : 0.769100\n",
            "total step : 130 \n",
            "error : 0.787701, accuarcy : 0.769700\n",
            "total step : 131 \n",
            "error : 0.784742, accuarcy : 0.770200\n",
            "total step : 132 \n",
            "error : 0.781816, accuarcy : 0.770600\n",
            "total step : 133 \n",
            "error : 0.778921, accuarcy : 0.771100\n",
            "total step : 134 \n",
            "error : 0.776058, accuarcy : 0.771700\n",
            "total step : 135 \n",
            "error : 0.773226, accuarcy : 0.772200\n",
            "total step : 136 \n",
            "error : 0.770425, accuarcy : 0.773100\n",
            "total step : 137 \n",
            "error : 0.767653, accuarcy : 0.773400\n",
            "total step : 138 \n",
            "error : 0.764910, accuarcy : 0.773700\n",
            "total step : 139 \n",
            "error : 0.762196, accuarcy : 0.774900\n",
            "total step : 140 \n",
            "error : 0.759510, accuarcy : 0.775200\n",
            "total step : 141 \n",
            "error : 0.756852, accuarcy : 0.775400\n",
            "total step : 142 \n",
            "error : 0.754221, accuarcy : 0.775900\n",
            "total step : 143 \n",
            "error : 0.751617, accuarcy : 0.776000\n",
            "total step : 144 \n",
            "error : 0.749039, accuarcy : 0.776800\n",
            "total step : 145 \n",
            "error : 0.746487, accuarcy : 0.777200\n",
            "total step : 146 \n",
            "error : 0.743960, accuarcy : 0.777800\n",
            "total step : 147 \n",
            "error : 0.741458, accuarcy : 0.778100\n",
            "total step : 148 \n",
            "error : 0.738981, accuarcy : 0.778600\n",
            "total step : 149 \n",
            "error : 0.736528, accuarcy : 0.779200\n",
            "total step : 150 \n",
            "error : 0.734098, accuarcy : 0.779600\n",
            "total step : 151 \n",
            "error : 0.731692, accuarcy : 0.780000\n",
            "total step : 152 \n",
            "error : 0.729309, accuarcy : 0.780500\n",
            "total step : 153 \n",
            "error : 0.726948, accuarcy : 0.780700\n",
            "total step : 154 \n",
            "error : 0.724610, accuarcy : 0.780700\n",
            "total step : 155 \n",
            "error : 0.722293, accuarcy : 0.781300\n",
            "total step : 156 \n",
            "error : 0.719998, accuarcy : 0.782100\n",
            "total step : 157 \n",
            "error : 0.717724, accuarcy : 0.782700\n",
            "total step : 158 \n",
            "error : 0.715470, accuarcy : 0.783200\n",
            "total step : 159 \n",
            "error : 0.713237, accuarcy : 0.783500\n",
            "total step : 160 \n",
            "error : 0.711025, accuarcy : 0.784100\n",
            "total step : 161 \n",
            "error : 0.708832, accuarcy : 0.784500\n",
            "total step : 162 \n",
            "error : 0.706658, accuarcy : 0.784800\n",
            "total step : 163 \n",
            "error : 0.704504, accuarcy : 0.785600\n",
            "total step : 164 \n",
            "error : 0.702369, accuarcy : 0.786000\n",
            "total step : 165 \n",
            "error : 0.700252, accuarcy : 0.786900\n",
            "total step : 166 \n",
            "error : 0.698154, accuarcy : 0.787200\n",
            "total step : 167 \n",
            "error : 0.696074, accuarcy : 0.787400\n",
            "total step : 168 \n",
            "error : 0.694012, accuarcy : 0.787800\n",
            "total step : 169 \n",
            "error : 0.691967, accuarcy : 0.787800\n",
            "total step : 170 \n",
            "error : 0.689940, accuarcy : 0.788000\n",
            "total step : 171 \n",
            "error : 0.687929, accuarcy : 0.788000\n",
            "total step : 172 \n",
            "error : 0.685936, accuarcy : 0.788200\n",
            "total step : 173 \n",
            "error : 0.683959, accuarcy : 0.789100\n",
            "total step : 174 \n",
            "error : 0.681998, accuarcy : 0.789400\n",
            "total step : 175 \n",
            "error : 0.680054, accuarcy : 0.789900\n",
            "total step : 176 \n",
            "error : 0.678125, accuarcy : 0.790000\n",
            "total step : 177 \n",
            "error : 0.676212, accuarcy : 0.790900\n",
            "total step : 178 \n",
            "error : 0.674315, accuarcy : 0.791400\n",
            "total step : 179 \n",
            "error : 0.672432, accuarcy : 0.791500\n",
            "total step : 180 \n",
            "error : 0.670565, accuarcy : 0.791700\n",
            "total step : 181 \n",
            "error : 0.668713, accuarcy : 0.792200\n",
            "total step : 182 \n",
            "error : 0.666875, accuarcy : 0.792500\n",
            "total step : 183 \n",
            "error : 0.665052, accuarcy : 0.792900\n",
            "total step : 184 \n",
            "error : 0.663243, accuarcy : 0.793200\n",
            "total step : 185 \n",
            "error : 0.661448, accuarcy : 0.793300\n",
            "total step : 186 \n",
            "error : 0.659666, accuarcy : 0.793700\n",
            "total step : 187 \n",
            "error : 0.657899, accuarcy : 0.794000\n",
            "total step : 188 \n",
            "error : 0.656145, accuarcy : 0.794600\n",
            "total step : 189 \n",
            "error : 0.654404, accuarcy : 0.794900\n",
            "total step : 190 \n",
            "error : 0.652677, accuarcy : 0.795300\n",
            "total step : 191 \n",
            "error : 0.650963, accuarcy : 0.795700\n",
            "total step : 192 \n",
            "error : 0.649261, accuarcy : 0.796600\n",
            "total step : 193 \n",
            "error : 0.647572, accuarcy : 0.797200\n",
            "total step : 194 \n",
            "error : 0.645896, accuarcy : 0.797700\n",
            "total step : 195 \n",
            "error : 0.644232, accuarcy : 0.798200\n",
            "total step : 196 \n",
            "error : 0.642581, accuarcy : 0.798400\n",
            "total step : 197 \n",
            "error : 0.640941, accuarcy : 0.798500\n",
            "total step : 198 \n",
            "error : 0.639314, accuarcy : 0.798800\n",
            "total step : 199 \n",
            "error : 0.637698, accuarcy : 0.799100\n",
            "total step : 200 \n",
            "error : 0.636094, accuarcy : 0.799500\n",
            "total step : 201 \n",
            "error : 0.634501, accuarcy : 0.799800\n",
            "total step : 202 \n",
            "error : 0.632920, accuarcy : 0.800100\n",
            "total step : 203 \n",
            "error : 0.631350, accuarcy : 0.800300\n",
            "total step : 204 \n",
            "error : 0.629791, accuarcy : 0.800600\n",
            "total step : 205 \n",
            "error : 0.628243, accuarcy : 0.801000\n",
            "total step : 206 \n",
            "error : 0.626706, accuarcy : 0.801300\n",
            "total step : 207 \n",
            "error : 0.625180, accuarcy : 0.801300\n",
            "total step : 208 \n",
            "error : 0.623664, accuarcy : 0.801600\n",
            "total step : 209 \n",
            "error : 0.622159, accuarcy : 0.802100\n",
            "total step : 210 \n",
            "error : 0.620664, accuarcy : 0.802300\n",
            "total step : 211 \n",
            "error : 0.619180, accuarcy : 0.802500\n",
            "total step : 212 \n",
            "error : 0.617705, accuarcy : 0.802900\n",
            "total step : 213 \n",
            "error : 0.616241, accuarcy : 0.803200\n",
            "total step : 214 \n",
            "error : 0.614787, accuarcy : 0.803300\n",
            "total step : 215 \n",
            "error : 0.613342, accuarcy : 0.803800\n",
            "total step : 216 \n",
            "error : 0.611908, accuarcy : 0.803800\n",
            "total step : 217 \n",
            "error : 0.610483, accuarcy : 0.804000\n",
            "total step : 218 \n",
            "error : 0.609067, accuarcy : 0.804400\n",
            "total step : 219 \n",
            "error : 0.607661, accuarcy : 0.805000\n",
            "total step : 220 \n",
            "error : 0.606264, accuarcy : 0.804800\n",
            "total step : 221 \n",
            "error : 0.604876, accuarcy : 0.805200\n",
            "total step : 222 \n",
            "error : 0.603498, accuarcy : 0.805400\n",
            "total step : 223 \n",
            "error : 0.602128, accuarcy : 0.805700\n",
            "total step : 224 \n",
            "error : 0.600767, accuarcy : 0.806100\n",
            "total step : 225 \n",
            "error : 0.599416, accuarcy : 0.806100\n",
            "total step : 226 \n",
            "error : 0.598073, accuarcy : 0.806400\n",
            "total step : 227 \n",
            "error : 0.596738, accuarcy : 0.806600\n",
            "total step : 228 \n",
            "error : 0.595413, accuarcy : 0.806900\n",
            "total step : 229 \n",
            "error : 0.594095, accuarcy : 0.807100\n",
            "total step : 230 \n",
            "error : 0.592786, accuarcy : 0.807200\n",
            "total step : 231 \n",
            "error : 0.591486, accuarcy : 0.807800\n",
            "total step : 232 \n",
            "error : 0.590194, accuarcy : 0.807900\n",
            "total step : 233 \n",
            "error : 0.588909, accuarcy : 0.808100\n",
            "total step : 234 \n",
            "error : 0.587633, accuarcy : 0.808500\n",
            "total step : 235 \n",
            "error : 0.586365, accuarcy : 0.808500\n",
            "total step : 236 \n",
            "error : 0.585105, accuarcy : 0.808700\n",
            "total step : 237 \n",
            "error : 0.583853, accuarcy : 0.808600\n",
            "total step : 238 \n",
            "error : 0.582608, accuarcy : 0.808800\n",
            "total step : 239 \n",
            "error : 0.581371, accuarcy : 0.809100\n",
            "total step : 240 \n",
            "error : 0.580142, accuarcy : 0.809400\n",
            "total step : 241 \n",
            "error : 0.578920, accuarcy : 0.809700\n",
            "total step : 242 \n",
            "error : 0.577706, accuarcy : 0.809800\n",
            "total step : 243 \n",
            "error : 0.576499, accuarcy : 0.809800\n",
            "total step : 244 \n",
            "error : 0.575300, accuarcy : 0.810000\n",
            "total step : 245 \n",
            "error : 0.574108, accuarcy : 0.810400\n",
            "total step : 246 \n",
            "error : 0.572923, accuarcy : 0.810700\n",
            "total step : 247 \n",
            "error : 0.571745, accuarcy : 0.811000\n",
            "total step : 248 \n",
            "error : 0.570574, accuarcy : 0.811300\n",
            "total step : 249 \n",
            "error : 0.569410, accuarcy : 0.811300\n",
            "total step : 250 \n",
            "error : 0.568253, accuarcy : 0.811600\n",
            "total step : 251 \n",
            "error : 0.567103, accuarcy : 0.811700\n",
            "total step : 252 \n",
            "error : 0.565960, accuarcy : 0.811800\n",
            "total step : 253 \n",
            "error : 0.564824, accuarcy : 0.812200\n",
            "total step : 254 \n",
            "error : 0.563694, accuarcy : 0.812400\n",
            "total step : 255 \n",
            "error : 0.562571, accuarcy : 0.812500\n",
            "total step : 256 \n",
            "error : 0.561454, accuarcy : 0.812700\n",
            "total step : 257 \n",
            "error : 0.560344, accuarcy : 0.812900\n",
            "total step : 258 \n",
            "error : 0.559241, accuarcy : 0.813200\n",
            "total step : 259 \n",
            "error : 0.558144, accuarcy : 0.813600\n",
            "total step : 260 \n",
            "error : 0.557053, accuarcy : 0.813900\n",
            "total step : 261 \n",
            "error : 0.555968, accuarcy : 0.814200\n",
            "total step : 262 \n",
            "error : 0.554890, accuarcy : 0.814200\n",
            "total step : 263 \n",
            "error : 0.553818, accuarcy : 0.814300\n",
            "total step : 264 \n",
            "error : 0.552752, accuarcy : 0.814900\n",
            "total step : 265 \n",
            "error : 0.551692, accuarcy : 0.814800\n",
            "total step : 266 \n",
            "error : 0.550638, accuarcy : 0.815000\n",
            "total step : 267 \n",
            "error : 0.549590, accuarcy : 0.815000\n",
            "total step : 268 \n",
            "error : 0.548549, accuarcy : 0.815600\n",
            "total step : 269 \n",
            "error : 0.547512, accuarcy : 0.815800\n",
            "total step : 270 \n",
            "error : 0.546482, accuarcy : 0.816100\n",
            "total step : 271 \n",
            "error : 0.545458, accuarcy : 0.816100\n",
            "total step : 272 \n",
            "error : 0.544439, accuarcy : 0.816200\n",
            "total step : 273 \n",
            "error : 0.543426, accuarcy : 0.816600\n",
            "total step : 274 \n",
            "error : 0.542419, accuarcy : 0.816800\n",
            "total step : 275 \n",
            "error : 0.541417, accuarcy : 0.817000\n",
            "total step : 276 \n",
            "error : 0.540420, accuarcy : 0.817200\n",
            "total step : 277 \n",
            "error : 0.539430, accuarcy : 0.817300\n",
            "total step : 278 \n",
            "error : 0.538444, accuarcy : 0.817400\n",
            "total step : 279 \n",
            "error : 0.537464, accuarcy : 0.817600\n",
            "total step : 280 \n",
            "error : 0.536490, accuarcy : 0.817600\n",
            "total step : 281 \n",
            "error : 0.535521, accuarcy : 0.817600\n",
            "total step : 282 \n",
            "error : 0.534557, accuarcy : 0.817800\n",
            "total step : 283 \n",
            "error : 0.533598, accuarcy : 0.818000\n",
            "total step : 284 \n",
            "error : 0.532644, accuarcy : 0.818100\n",
            "total step : 285 \n",
            "error : 0.531696, accuarcy : 0.818300\n",
            "total step : 286 \n",
            "error : 0.530752, accuarcy : 0.818600\n",
            "total step : 287 \n",
            "error : 0.529814, accuarcy : 0.818600\n",
            "total step : 288 \n",
            "error : 0.528881, accuarcy : 0.818800\n",
            "total step : 289 \n",
            "error : 0.527953, accuarcy : 0.819000\n",
            "total step : 290 \n",
            "error : 0.527029, accuarcy : 0.819200\n",
            "total step : 291 \n",
            "error : 0.526111, accuarcy : 0.819500\n",
            "total step : 292 \n",
            "error : 0.525198, accuarcy : 0.819900\n",
            "total step : 293 \n",
            "error : 0.524289, accuarcy : 0.820200\n",
            "total step : 294 \n",
            "error : 0.523385, accuarcy : 0.820200\n",
            "total step : 295 \n",
            "error : 0.522486, accuarcy : 0.820600\n",
            "total step : 296 \n",
            "error : 0.521592, accuarcy : 0.821000\n",
            "total step : 297 \n",
            "error : 0.520702, accuarcy : 0.821300\n",
            "total step : 298 \n",
            "error : 0.519817, accuarcy : 0.821700\n",
            "total step : 299 \n",
            "error : 0.518936, accuarcy : 0.821900\n",
            "total step : 300 \n",
            "error : 0.518060, accuarcy : 0.822000\n",
            "total step : 301 \n",
            "error : 0.517189, accuarcy : 0.822000\n",
            "total step : 302 \n",
            "error : 0.516322, accuarcy : 0.822000\n",
            "total step : 303 \n",
            "error : 0.515460, accuarcy : 0.822300\n",
            "total step : 304 \n",
            "error : 0.514602, accuarcy : 0.822400\n",
            "total step : 305 \n",
            "error : 0.513748, accuarcy : 0.822500\n",
            "total step : 306 \n",
            "error : 0.512899, accuarcy : 0.822500\n",
            "total step : 307 \n",
            "error : 0.512054, accuarcy : 0.822800\n",
            "total step : 308 \n",
            "error : 0.511214, accuarcy : 0.823400\n",
            "total step : 309 \n",
            "error : 0.510378, accuarcy : 0.823700\n",
            "total step : 310 \n",
            "error : 0.509546, accuarcy : 0.823700\n",
            "total step : 311 \n",
            "error : 0.508718, accuarcy : 0.823600\n",
            "total step : 312 \n",
            "error : 0.507894, accuarcy : 0.823900\n",
            "total step : 313 \n",
            "error : 0.507075, accuarcy : 0.824100\n",
            "total step : 314 \n",
            "error : 0.506259, accuarcy : 0.824200\n",
            "total step : 315 \n",
            "error : 0.505448, accuarcy : 0.824700\n",
            "total step : 316 \n",
            "error : 0.504641, accuarcy : 0.825000\n",
            "total step : 317 \n",
            "error : 0.503838, accuarcy : 0.825200\n",
            "total step : 318 \n",
            "error : 0.503038, accuarcy : 0.825400\n",
            "total step : 319 \n",
            "error : 0.502243, accuarcy : 0.825700\n",
            "total step : 320 \n",
            "error : 0.501452, accuarcy : 0.825600\n",
            "total step : 321 \n",
            "error : 0.500664, accuarcy : 0.825900\n",
            "total step : 322 \n",
            "error : 0.499881, accuarcy : 0.826100\n",
            "total step : 323 \n",
            "error : 0.499101, accuarcy : 0.826400\n",
            "total step : 324 \n",
            "error : 0.498325, accuarcy : 0.826600\n",
            "total step : 325 \n",
            "error : 0.497553, accuarcy : 0.826700\n",
            "total step : 326 \n",
            "error : 0.496785, accuarcy : 0.826700\n",
            "total step : 327 \n",
            "error : 0.496021, accuarcy : 0.826900\n",
            "total step : 328 \n",
            "error : 0.495260, accuarcy : 0.827000\n",
            "total step : 329 \n",
            "error : 0.494503, accuarcy : 0.827300\n",
            "total step : 330 \n",
            "error : 0.493749, accuarcy : 0.827200\n",
            "total step : 331 \n",
            "error : 0.492999, accuarcy : 0.827400\n",
            "total step : 332 \n",
            "error : 0.492253, accuarcy : 0.827700\n",
            "total step : 333 \n",
            "error : 0.491510, accuarcy : 0.827700\n",
            "total step : 334 \n",
            "error : 0.490771, accuarcy : 0.827900\n",
            "total step : 335 \n",
            "error : 0.490036, accuarcy : 0.828000\n",
            "total step : 336 \n",
            "error : 0.489304, accuarcy : 0.828200\n",
            "total step : 337 \n",
            "error : 0.488575, accuarcy : 0.828400\n",
            "total step : 338 \n",
            "error : 0.487850, accuarcy : 0.828800\n",
            "total step : 339 \n",
            "error : 0.487128, accuarcy : 0.828900\n",
            "total step : 340 \n",
            "error : 0.486410, accuarcy : 0.829400\n",
            "total step : 341 \n",
            "error : 0.485695, accuarcy : 0.829500\n",
            "total step : 342 \n",
            "error : 0.484984, accuarcy : 0.829700\n",
            "total step : 343 \n",
            "error : 0.484276, accuarcy : 0.829700\n",
            "total step : 344 \n",
            "error : 0.483571, accuarcy : 0.829800\n",
            "total step : 345 \n",
            "error : 0.482869, accuarcy : 0.830100\n",
            "total step : 346 \n",
            "error : 0.482171, accuarcy : 0.830100\n",
            "total step : 347 \n",
            "error : 0.481476, accuarcy : 0.830400\n",
            "total step : 348 \n",
            "error : 0.480784, accuarcy : 0.830400\n",
            "total step : 349 \n",
            "error : 0.480096, accuarcy : 0.830700\n",
            "total step : 350 \n",
            "error : 0.479410, accuarcy : 0.831100\n",
            "total step : 351 \n",
            "error : 0.478728, accuarcy : 0.831100\n",
            "total step : 352 \n",
            "error : 0.478049, accuarcy : 0.831100\n",
            "total step : 353 \n",
            "error : 0.477373, accuarcy : 0.831600\n",
            "total step : 354 \n",
            "error : 0.476700, accuarcy : 0.831800\n",
            "total step : 355 \n",
            "error : 0.476030, accuarcy : 0.832200\n",
            "total step : 356 \n",
            "error : 0.475363, accuarcy : 0.832300\n",
            "total step : 357 \n",
            "error : 0.474700, accuarcy : 0.832700\n",
            "total step : 358 \n",
            "error : 0.474039, accuarcy : 0.832900\n",
            "total step : 359 \n",
            "error : 0.473381, accuarcy : 0.833300\n",
            "total step : 360 \n",
            "error : 0.472727, accuarcy : 0.833600\n",
            "total step : 361 \n",
            "error : 0.472075, accuarcy : 0.833500\n",
            "total step : 362 \n",
            "error : 0.471426, accuarcy : 0.834000\n",
            "total step : 363 \n",
            "error : 0.470780, accuarcy : 0.834100\n",
            "total step : 364 \n",
            "error : 0.470137, accuarcy : 0.834300\n",
            "total step : 365 \n",
            "error : 0.469497, accuarcy : 0.834500\n",
            "total step : 366 \n",
            "error : 0.468860, accuarcy : 0.834800\n",
            "total step : 367 \n",
            "error : 0.468225, accuarcy : 0.835100\n",
            "total step : 368 \n",
            "error : 0.467593, accuarcy : 0.835500\n",
            "total step : 369 \n",
            "error : 0.466965, accuarcy : 0.835600\n",
            "total step : 370 \n",
            "error : 0.466339, accuarcy : 0.835700\n",
            "total step : 371 \n",
            "error : 0.465715, accuarcy : 0.835800\n",
            "total step : 372 \n",
            "error : 0.465095, accuarcy : 0.836000\n",
            "total step : 373 \n",
            "error : 0.464477, accuarcy : 0.836100\n",
            "total step : 374 \n",
            "error : 0.463862, accuarcy : 0.836400\n",
            "total step : 375 \n",
            "error : 0.463250, accuarcy : 0.836300\n",
            "total step : 376 \n",
            "error : 0.462640, accuarcy : 0.836400\n",
            "total step : 377 \n",
            "error : 0.462033, accuarcy : 0.836400\n",
            "total step : 378 \n",
            "error : 0.461428, accuarcy : 0.836300\n",
            "total step : 379 \n",
            "error : 0.460827, accuarcy : 0.836400\n",
            "total step : 380 \n",
            "error : 0.460228, accuarcy : 0.836500\n",
            "total step : 381 \n",
            "error : 0.459631, accuarcy : 0.836700\n",
            "total step : 382 \n",
            "error : 0.459037, accuarcy : 0.836800\n",
            "total step : 383 \n",
            "error : 0.458446, accuarcy : 0.837100\n",
            "total step : 384 \n",
            "error : 0.457857, accuarcy : 0.837300\n",
            "total step : 385 \n",
            "error : 0.457270, accuarcy : 0.837500\n",
            "total step : 386 \n",
            "error : 0.456686, accuarcy : 0.837800\n",
            "total step : 387 \n",
            "error : 0.456105, accuarcy : 0.838000\n",
            "total step : 388 \n",
            "error : 0.455526, accuarcy : 0.838000\n",
            "total step : 389 \n",
            "error : 0.454950, accuarcy : 0.838000\n",
            "total step : 390 \n",
            "error : 0.454376, accuarcy : 0.838000\n",
            "total step : 391 \n",
            "error : 0.453804, accuarcy : 0.837900\n",
            "total step : 392 \n",
            "error : 0.453235, accuarcy : 0.838100\n",
            "total step : 393 \n",
            "error : 0.452669, accuarcy : 0.838000\n",
            "total step : 394 \n",
            "error : 0.452104, accuarcy : 0.838100\n",
            "total step : 395 \n",
            "error : 0.451543, accuarcy : 0.838200\n",
            "total step : 396 \n",
            "error : 0.450983, accuarcy : 0.838400\n",
            "total step : 397 \n",
            "error : 0.450426, accuarcy : 0.838600\n",
            "total step : 398 \n",
            "error : 0.449871, accuarcy : 0.838800\n",
            "total step : 399 \n",
            "error : 0.449318, accuarcy : 0.839000\n",
            "total step : 400 \n",
            "error : 0.448768, accuarcy : 0.839300\n",
            "total step : 401 \n",
            "error : 0.448220, accuarcy : 0.839200\n",
            "total step : 402 \n",
            "error : 0.447674, accuarcy : 0.839300\n",
            "total step : 403 \n",
            "error : 0.447131, accuarcy : 0.839300\n",
            "total step : 404 \n",
            "error : 0.446590, accuarcy : 0.839400\n",
            "total step : 405 \n",
            "error : 0.446051, accuarcy : 0.839500\n",
            "total step : 406 \n",
            "error : 0.445514, accuarcy : 0.839500\n",
            "total step : 407 \n",
            "error : 0.444980, accuarcy : 0.839500\n",
            "total step : 408 \n",
            "error : 0.444447, accuarcy : 0.839600\n",
            "total step : 409 \n",
            "error : 0.443917, accuarcy : 0.839900\n",
            "total step : 410 \n",
            "error : 0.443389, accuarcy : 0.839900\n",
            "total step : 411 \n",
            "error : 0.442863, accuarcy : 0.840000\n",
            "total step : 412 \n",
            "error : 0.442340, accuarcy : 0.840100\n",
            "total step : 413 \n",
            "error : 0.441818, accuarcy : 0.840500\n",
            "total step : 414 \n",
            "error : 0.441299, accuarcy : 0.840700\n",
            "total step : 415 \n",
            "error : 0.440781, accuarcy : 0.840900\n",
            "total step : 416 \n",
            "error : 0.440266, accuarcy : 0.840800\n",
            "total step : 417 \n",
            "error : 0.439753, accuarcy : 0.841100\n",
            "total step : 418 \n",
            "error : 0.439242, accuarcy : 0.841000\n",
            "total step : 419 \n",
            "error : 0.438733, accuarcy : 0.841300\n",
            "total step : 420 \n",
            "error : 0.438226, accuarcy : 0.841600\n",
            "total step : 421 \n",
            "error : 0.437721, accuarcy : 0.842000\n",
            "total step : 422 \n",
            "error : 0.437218, accuarcy : 0.842000\n",
            "total step : 423 \n",
            "error : 0.436717, accuarcy : 0.842100\n",
            "total step : 424 \n",
            "error : 0.436218, accuarcy : 0.842400\n",
            "total step : 425 \n",
            "error : 0.435721, accuarcy : 0.842400\n",
            "total step : 426 \n",
            "error : 0.435226, accuarcy : 0.842500\n",
            "total step : 427 \n",
            "error : 0.434732, accuarcy : 0.842900\n",
            "total step : 428 \n",
            "error : 0.434241, accuarcy : 0.842800\n",
            "total step : 429 \n",
            "error : 0.433752, accuarcy : 0.842900\n",
            "total step : 430 \n",
            "error : 0.433265, accuarcy : 0.843000\n",
            "total step : 431 \n",
            "error : 0.432779, accuarcy : 0.843100\n",
            "total step : 432 \n",
            "error : 0.432296, accuarcy : 0.843100\n",
            "total step : 433 \n",
            "error : 0.431814, accuarcy : 0.843100\n",
            "total step : 434 \n",
            "error : 0.431335, accuarcy : 0.843300\n",
            "total step : 435 \n",
            "error : 0.430857, accuarcy : 0.843400\n",
            "total step : 436 \n",
            "error : 0.430381, accuarcy : 0.843600\n",
            "total step : 437 \n",
            "error : 0.429907, accuarcy : 0.843700\n",
            "total step : 438 \n",
            "error : 0.429434, accuarcy : 0.844000\n",
            "total step : 439 \n",
            "error : 0.428964, accuarcy : 0.844300\n",
            "total step : 440 \n",
            "error : 0.428495, accuarcy : 0.844400\n",
            "total step : 441 \n",
            "error : 0.428028, accuarcy : 0.844300\n",
            "total step : 442 \n",
            "error : 0.427563, accuarcy : 0.844300\n",
            "total step : 443 \n",
            "error : 0.427100, accuarcy : 0.844500\n",
            "total step : 444 \n",
            "error : 0.426638, accuarcy : 0.844600\n",
            "total step : 445 \n",
            "error : 0.426178, accuarcy : 0.844700\n",
            "total step : 446 \n",
            "error : 0.425720, accuarcy : 0.844900\n",
            "total step : 447 \n",
            "error : 0.425264, accuarcy : 0.845100\n",
            "total step : 448 \n",
            "error : 0.424809, accuarcy : 0.845200\n",
            "total step : 449 \n",
            "error : 0.424357, accuarcy : 0.845300\n",
            "total step : 450 \n",
            "error : 0.423905, accuarcy : 0.845700\n",
            "total step : 451 \n",
            "error : 0.423456, accuarcy : 0.845800\n",
            "total step : 452 \n",
            "error : 0.423008, accuarcy : 0.845700\n",
            "total step : 453 \n",
            "error : 0.422562, accuarcy : 0.845800\n",
            "total step : 454 \n",
            "error : 0.422118, accuarcy : 0.846000\n",
            "total step : 455 \n",
            "error : 0.421675, accuarcy : 0.846200\n",
            "total step : 456 \n",
            "error : 0.421234, accuarcy : 0.846400\n",
            "total step : 457 \n",
            "error : 0.420794, accuarcy : 0.846600\n",
            "total step : 458 \n",
            "error : 0.420357, accuarcy : 0.846700\n",
            "total step : 459 \n",
            "error : 0.419920, accuarcy : 0.846800\n",
            "total step : 460 \n",
            "error : 0.419486, accuarcy : 0.846900\n",
            "total step : 461 \n",
            "error : 0.419053, accuarcy : 0.846900\n",
            "total step : 462 \n",
            "error : 0.418621, accuarcy : 0.847000\n",
            "total step : 463 \n",
            "error : 0.418192, accuarcy : 0.847100\n",
            "total step : 464 \n",
            "error : 0.417763, accuarcy : 0.847200\n",
            "total step : 465 \n",
            "error : 0.417337, accuarcy : 0.847400\n",
            "total step : 466 \n",
            "error : 0.416912, accuarcy : 0.847400\n",
            "total step : 467 \n",
            "error : 0.416488, accuarcy : 0.847500\n",
            "total step : 468 \n",
            "error : 0.416066, accuarcy : 0.847500\n",
            "total step : 469 \n",
            "error : 0.415645, accuarcy : 0.847500\n",
            "total step : 470 \n",
            "error : 0.415227, accuarcy : 0.847500\n",
            "total step : 471 \n",
            "error : 0.414809, accuarcy : 0.847600\n",
            "total step : 472 \n",
            "error : 0.414393, accuarcy : 0.847500\n",
            "total step : 473 \n",
            "error : 0.413979, accuarcy : 0.847400\n",
            "total step : 474 \n",
            "error : 0.413566, accuarcy : 0.847500\n",
            "total step : 475 \n",
            "error : 0.413154, accuarcy : 0.847600\n",
            "total step : 476 \n",
            "error : 0.412744, accuarcy : 0.847800\n",
            "total step : 477 \n",
            "error : 0.412336, accuarcy : 0.847900\n",
            "total step : 478 \n",
            "error : 0.411928, accuarcy : 0.848100\n",
            "total step : 479 \n",
            "error : 0.411523, accuarcy : 0.847900\n",
            "total step : 480 \n",
            "error : 0.411119, accuarcy : 0.848000\n",
            "total step : 481 \n",
            "error : 0.410716, accuarcy : 0.848200\n",
            "total step : 482 \n",
            "error : 0.410314, accuarcy : 0.848400\n",
            "total step : 483 \n",
            "error : 0.409914, accuarcy : 0.848800\n",
            "total step : 484 \n",
            "error : 0.409516, accuarcy : 0.848800\n",
            "total step : 485 \n",
            "error : 0.409119, accuarcy : 0.849100\n",
            "total step : 486 \n",
            "error : 0.408723, accuarcy : 0.849400\n",
            "total step : 487 \n",
            "error : 0.408329, accuarcy : 0.849700\n",
            "total step : 488 \n",
            "error : 0.407936, accuarcy : 0.849800\n",
            "total step : 489 \n",
            "error : 0.407544, accuarcy : 0.849900\n",
            "total step : 490 \n",
            "error : 0.407154, accuarcy : 0.850000\n",
            "total step : 491 \n",
            "error : 0.406765, accuarcy : 0.850100\n"
          ]
        }
      ],
      "source": [
        "train_9X, train_9y = make_sample(idx = 9) # idx = target number\n",
        "train_9X = np.insert(train_9X, 0, 1, axis=1) # bias 추가\n",
        "w9,J9_history, ACC9_history = train(train_9X, train_9y,9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mult_cls_classifier(X,w0,w1,w2,w3,w4,w5,w6,w7,w8,w9):\n",
        "    \n",
        "    res_0 = 1 / (1+np.exp(-X.dot(w0)))\n",
        "    res_1 = 1 / (1+np.exp(-X.dot(w1)))\n",
        "    res_2 = 1 / (1+np.exp(-X.dot(w2)))\n",
        "    res_3 = 1 / (1+np.exp(-X.dot(w3)))\n",
        "    res_4 = 1 / (1+np.exp(-X.dot(w4)))\n",
        "    res_5 = 1 / (1+np.exp(-X.dot(w5)))\n",
        "    res_6 = 1 / (1+np.exp(-X.dot(w6)))\n",
        "    res_7 = 1 / (1+np.exp(-X.dot(w7)))\n",
        "    res_8 = 1 / (1+np.exp(-X.dot(w8)))\n",
        "    res_9 = 1 / (1+np.exp(-X.dot(w9)))\n",
        "    \n",
        "    res = np.concatenate((res_0,\n",
        "                        res_1,\n",
        "                        res_2,\n",
        "                        res_3,\n",
        "                        res_4,\n",
        "                        res_5,\n",
        "                        res_6,\n",
        "                        res_7,\n",
        "                        res_8,\n",
        "                        res_9), axis = 1)\n",
        "    \n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mOkzi2x5FGRd",
        "outputId": "99f37103-ff9d-408a-cd92-b8ae51765f24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of test_X :  (10000, 785)\n",
            "shape of test_label :  (10000,)\n",
            "accuracy :  0.6844\n"
          ]
        }
      ],
      "source": [
        "# eval\n",
        "import pandas as pd\n",
        "\n",
        "test_X = test_raw_img.astype('float')/255    \n",
        "test_X = test_X.reshape(len(test_X.squeeze()), -1)\n",
        "test_X = np.insert(test_X, 0, 1, axis=1) # + bias\n",
        "print('shape of test_X : ', test_X.shape)\n",
        "print('shape of test_label : ', test_label.shape)\n",
        "\n",
        "probs = mult_cls_classifier(test_X,w0,w1,w2,w3,w4,w5,w6,w7,w8,w9)\n",
        "# make prediction using argmax\n",
        "max_pred = np.argmax(probs, axis = 1)\n",
        "\n",
        "acc = np.sum(np.where(test_label==max_pred, True, False))/len(test_X)\n",
        "print('accuracy : ', acc)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### L2 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cross entropy loss\n",
        "def CrossEntropyLoss(w, X, y, alpha) :\n",
        "    delta = 1e-7\n",
        "    \n",
        "    preds = 1 / (1+np.exp(-X.dot(w)))\n",
        "    reg_term = 0.5 * alpha * np.sum(w**2)\n",
        "    loss = np.sum(-y*np.log(preds+delta)-(1-y)*np.log(1-preds+delta))/len(preds) + reg_term\n",
        "        \n",
        "    return loss , preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train\n",
        "\n",
        "def train(X, y, index,alpha) :\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        X : train_X\n",
        "        y : train_y\n",
        "\n",
        "    Returns:\n",
        "        w : weight\n",
        "    \"\"\"\n",
        "    w = np.random.randn(len(X[0]), 1)\n",
        "    lr = 0.1 # learning rate(수정 가능)\n",
        "    step = 0\n",
        "    acc = 0\n",
        "    prev_loss = float('inf')    \n",
        "    weight_capacity = list()\n",
        "    \n",
        "    while (acc <= 0.85) :\n",
        "        step += 1\n",
        "        correct = 0\n",
        "        \n",
        "        acc = eval(index, w)\n",
        "        \n",
        "        loss, preds = CrossEntropyLoss(w, X, y, alpha)\n",
        "        \n",
        "        diff = preds - y\n",
        "        gradient = X.T.dot(diff) / X.shape[0]\n",
        "        weight_capacity.append(np.linalg.norm(w[0][0]))\n",
        "        w -= lr * gradient\n",
        "        \n",
        "        if abs(loss - prev_loss) < 1e-4:\n",
        "            break\n",
        "        \n",
        "        prev_loss = loss\n",
        "        \n",
        "        # print(\"total step : %d \" % step)\n",
        "        # print(\"error : %f, accuarcy : %f\" % (loss, acc))\n",
        "        \n",
        "    return w, weight_capacity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "lambdas = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "weights = np.zeros((5, len(lambdas)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, l in enumerate(lambdas):\n",
        "    w,_ = train(train_7X, train_7y, 7, l)\n",
        "    weights[:, i] = np.sum(np.array(w[:1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "for i in range(5):\n",
        "    ax.plot(lambdas, weights[i], label=f'Feature {i+1}')\n",
        "ax.set_xscale('log')\n",
        "ax.set_xlabel('lambda')\n",
        "ax.set_ylabel('weight')\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ac989702ee82cf85834fc904c407af640e06b0902e988e9cd8a067d885b3b790"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
