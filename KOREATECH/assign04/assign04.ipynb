{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# install\n",
    "## numpy\n",
    "## matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data load & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(train_raw_img, train_label), (test_raw_img, test_label) = load_mnist(flatten=False, normalize=False)\n",
    "print(train_raw_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 1)\n",
      "(10000, 784)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing (train & inference)\n",
    "\n",
    "train_img = train_raw_img.reshape(len(train_raw_img.squeeze()), -1)\n",
    "train_label = train_label.reshape(len(train_label), -1)\n",
    "\n",
    "test_img = test_raw_img.reshape(len(test_raw_img.squeeze()), -1)\n",
    "test_label = test_label.reshape(len(test_label), -1)\n",
    "\n",
    "print(train_img.shape)\n",
    "print(train_label.shape)\n",
    "print(test_img.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization (set value 0 ~ 1)\n",
    "\n",
    "train_img = train_img.astype('float')\n",
    "train_img = train_img/255\n",
    "\n",
    "test_img = test_img.astype('float')\n",
    "test_img = test_img/255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear :\n",
    "    def __init__(self, input_size=1, hidden_size=1) :\n",
    "        self.W = \n",
    "        self.b = \n",
    "        self.x = \n",
    "        self.dW = \n",
    "        self.db = \n",
    "    \n",
    "    def forward(self, x) :\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dout, lr) :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu :\n",
    "    def __init__(self) :\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        self.mask = (x < 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout) :\n",
    "        dout[self.mask] = 0\n",
    "        return dout                                                                                                                                                                                                                                                                                                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax_with_crossEntropy :\n",
    "    def __init__(self) :\n",
    "        self.delta = 1e-7\n",
    "        # softmax\n",
    "        self.softmax_x = \n",
    "        self.softmax_out = \n",
    "        # crossEntropy\n",
    "        self.pred = \n",
    "        self.target = \n",
    "        \n",
    "    def softmax_forward(self, x) :\n",
    "        pass\n",
    "    \n",
    "    def crossEntropy_forward(self, pred, target) :\n",
    "        pass\n",
    "    \n",
    "    def backward(self) :\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot label 만드는 함수\n",
    "\n",
    "def make_one_hot(labels) :\n",
    "    a = []\n",
    "    for label in labels :\n",
    "        one_hot = np.zeros(10)\n",
    "        one_hot[label] = 1\n",
    "        a.append(one_hot)\n",
    "    a = np.array(a)\n",
    "    return a\n",
    "\n",
    "# one_hot_labels = make_one_hot(train_label)\n",
    "# print(train_label[0])\n",
    "# print(one_hot_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train version 1\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def train_MLP(config) :\n",
    "    lr, num_epoch = config['learning_rate'], config['num_epoch']\n",
    "    print_loss_interval = 1\n",
    "    \n",
    "    layer1 = Linear(784, 100)\n",
    "    relu = Relu()\n",
    "    layer2 = Linear(100, 10)\n",
    "    softmax_with_CE = softmax_with_crossEntropy()\n",
    "    \n",
    "    for iter in range(num_epoch) :\n",
    "        # forward\n",
    "        x = layer1.forward(train_img)\n",
    "        x = relu.forward(x)\n",
    "        x = layer2.forward(x)\n",
    "        preds = softmax_with_CE.softmax_forward(x)\n",
    "        \n",
    "        # loss\n",
    "        one_hot_labels = make_one_hot(train_label)\n",
    "        losses = softmax_with_CE.crossEntropy_forward(preds, one_hot_labels)\n",
    "        loss = losses.sum()/len(preds)\n",
    "        \n",
    "        if iter % print_loss_interval == 0:\n",
    "            print(\"[epoch %d / %d] average loss : %f\" % (iter, num_epoch, loss))\n",
    "        \n",
    "        # backward\n",
    "        dL = softmax_with_CE.backward()\n",
    "        dL = layer2.backward(dL, lr)\n",
    "        dL = relu.backward(dL)\n",
    "        dL = layer1.backward(dL, lr)\n",
    "            \n",
    "    model = OrderedDict()\n",
    "    model['layer1'] = layer1\n",
    "    model['relu'] = relu\n",
    "    model['layer2'] = layer2\n",
    "    model['softmax_with_CE'] = softmax_with_CE\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100\n",
    "          }\n",
    "\n",
    "model = train_MLP(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, train_version = True) :\n",
    "    if train_version :\n",
    "        x = train_img\n",
    "        labels = train_label.squeeze()\n",
    "        print('In train dataset ... ')\n",
    "    else : \n",
    "        x = test_img\n",
    "        labels = test_label.squeeze()\n",
    "        print('\\nIn test dataset ... ')\n",
    "    \n",
    "    for layer in model.values() :\n",
    "        if isinstance(layer, softmax_with_crossEntropy) :\n",
    "            x = layer.softmax_forward(x)\n",
    "        else :\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "    preds = x.argmax(axis=1)\n",
    "    acc = np.sum(np.where(preds==labels, True, False))/len(labels)\n",
    "    return acc\n",
    "\n",
    "print('\\t Accuracy :', eval(model, train_version=True))\n",
    "print('\\t Accuracy :', eval(model, train_version=False))\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2_regularization :\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config) :\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { 'model' : MLP_Classifier(),\n",
    "            'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100\n",
    "          }\n",
    "\n",
    "model = train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t Accuracy :', eval(model, train_version=True))\n",
    "print('\\t Accuracy :', eval(model, train_version=False))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac989702ee82cf85834fc904c407af640e06b0902e988e9cd8a067d885b3b790"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
