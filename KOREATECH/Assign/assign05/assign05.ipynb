{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# install\n",
    "## numpy\n",
    "## matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data load & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(train_raw_img, train_label), (test_raw_img, test_label) = load_mnist(flatten=False, normalize=False)\n",
    "print(train_raw_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 1)\n",
      "(10000, 784)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing (train & inference)\n",
    "\n",
    "train_img = train_raw_img.reshape(len(train_raw_img.squeeze()), -1)\n",
    "train_label = train_label.reshape(len(train_label), -1)\n",
    "\n",
    "test_img = test_raw_img.reshape(len(test_raw_img.squeeze()), -1)\n",
    "test_label = test_label.reshape(len(test_label), -1)\n",
    "\n",
    "print(train_img.shape)\n",
    "print(train_label.shape)\n",
    "print(test_img.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization (set value 0 ~ 1)\n",
    "\n",
    "train_img = train_img.astype('float')\n",
    "train_img = train_img/255\n",
    "\n",
    "test_img = test_img.astype('float')\n",
    "test_img = test_img/255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Linear:\n",
    "    def __init__(self, input_size=1, hidden_size=1):\n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.randn(input_size, hidden_size)\n",
    "        self.b = np.zeros(hidden_size)\n",
    "        \n",
    "        # Store input size and hidden size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Calculate linear transformation\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout, lr = 0.01, lamb=0):\n",
    "        # Calculate gradients for weights and biases\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.W -= lr * (dW + lamb*self.W)\n",
    "        self.b -= lr * db\n",
    "        \n",
    "        # Calculate gradients for input\n",
    "        # print(dout.shape, self.W.T.shape)\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        \n",
    "        return dx\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습 1] activation function 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dx = dout\n",
    "        dx[self.mask] = 0\n",
    "        return dx\n",
    "    \n",
    "\n",
    "class LeakyReLU:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] *= self.alpha\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] *= self.alpha\n",
    "        dx = dout\n",
    "        return dx\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        return dx\n",
    "\n",
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = np.tanh(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out ** 2)\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_with_CrossEntropy :\n",
    "    def __init__(self) :\n",
    "        # softmax\n",
    "        self.softmax_x = None \n",
    "        self.softmax_out = None\n",
    "        # crossEntropy\n",
    "        self.pred = None\n",
    "        self.target = None\n",
    "        self.delta = 1e-7\n",
    "        \n",
    "    def softmax_forward(self, x) :\n",
    "        self.softmax_x = x\n",
    "        tmp = np.max(self.softmax_x, axis=1).reshape(-1, 1)\n",
    "        self.softmax_out = np.exp(self.softmax_x-tmp)/np.sum(np.exp(self.softmax_x-tmp), axis=1).reshape(-1,1)\n",
    "        return self.softmax_out\n",
    "    \n",
    "    def crossEntropy_forward(self, pred, target) :\n",
    "        self.pred = pred\n",
    "        self.target = target\n",
    "        loss = -np.sum(self.target*np.log(self.pred+self.delta), axis=1)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self) :\n",
    "        dout = (self.pred-self.target)/len(self.pred)\n",
    "        return dout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot label 만드는 함수\n",
    "\n",
    "def make_one_hot(labels) :\n",
    "    a = []\n",
    "    for label in labels :\n",
    "        one_hot = np.zeros(10)\n",
    "        one_hot[label] = 1\n",
    "        a.append(one_hot)\n",
    "    a = np.array(a)\n",
    "    return a\n",
    "\n",
    "# one_hot_labels = make_one_hot(train_label)\n",
    "# print(train_label[0])\n",
    "# print(one_hot_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, train_version = True) :\n",
    "    if train_version :\n",
    "        x = train_img\n",
    "        labels = train_label.squeeze()\n",
    "        print('In train dataset ... ')\n",
    "    else : \n",
    "        x = test_img\n",
    "        labels = test_label.squeeze()\n",
    "        print('\\nIn test dataset ... ')\n",
    "    \n",
    "    for layer in model.values() :\n",
    "        if isinstance(layer, Softmax_with_CrossEntropy) :\n",
    "            x = layer.softmax_forward(x)\n",
    "        else :\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "    preds = x.argmax(axis=1)\n",
    "    acc = np.sum(np.where(preds==labels, True, False))/len(labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def train_MLP(config) :\n",
    "    lr, num_epoch = config['learning_rate'], config['num_epoch']\n",
    "    print_loss_interval = 20\n",
    "    \n",
    "    layer1 = Linear(784, 100)\n",
    "    activation_function = config['activation_function']\n",
    "    layer2 = Linear(100, 10)\n",
    "    softmax_with_CE = Softmax_with_CrossEntropy()\n",
    "    \n",
    "    for iter in range(num_epoch) :\n",
    "        # forward\n",
    "        x = layer1.forward(train_img)\n",
    "        x = activation_function.forward(x)\n",
    "        x = layer2.forward(x)\n",
    "        preds = softmax_with_CE.softmax_forward(x)\n",
    "        \n",
    "        # loss\n",
    "        one_hot_labels = make_one_hot(train_label)\n",
    "        losses = softmax_with_CE.crossEntropy_forward(preds, one_hot_labels)\n",
    "        loss = losses.sum()/len(preds)\n",
    "        \n",
    "        # backward\n",
    "        dL = softmax_with_CE.backward()\n",
    "        dL = layer2.backward(dL, lr)\n",
    "        dL = activation_function.backward(dL)\n",
    "        dL = layer1.backward(dL, lr)\n",
    "        \n",
    "        if iter == 0 or (iter+1) % print_loss_interval == 0:\n",
    "            print(\"[epoch %d / %d] average loss : %f\" % (iter+1, num_epoch, loss))\n",
    "            \n",
    "    model = OrderedDict()\n",
    "    model['layer1'] = layer1\n",
    "    model['activation_function'] = activation_function\n",
    "    model['layer2'] = layer2\n",
    "    model['softmax_with_CE'] = softmax_with_CE\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ReLU---\n",
      "[epoch 1 / 100] average loss : 14.772929\n",
      "[epoch 20 / 100] average loss : 5.344340\n",
      "[epoch 40 / 100] average loss : 3.798438\n",
      "[epoch 60 / 100] average loss : 3.132562\n",
      "[epoch 80 / 100] average loss : 2.738742\n",
      "[epoch 100 / 100] average loss : 2.470601\n",
      "\n",
      "---Leaky_Relu---\n",
      "[epoch 1 / 100] average loss : 14.190107\n",
      "[epoch 20 / 100] average loss : 5.663444\n",
      "[epoch 40 / 100] average loss : 3.880566\n",
      "[epoch 60 / 100] average loss : 3.192718\n",
      "[epoch 80 / 100] average loss : 2.786946\n",
      "[epoch 100 / 100] average loss : 2.514844\n",
      "\n",
      "---Sigmoid---\n",
      "[epoch 1 / 100] average loss : 10.381612\n",
      "[epoch 20 / 100] average loss : 5.460914\n",
      "[epoch 40 / 100] average loss : 4.290746\n",
      "[epoch 60 / 100] average loss : 3.677266\n",
      "[epoch 80 / 100] average loss : 3.205758\n",
      "[epoch 100 / 100] average loss : 2.839443\n",
      "\n",
      "---TanH---\n",
      "[epoch 1 / 100] average loss : 11.602126\n",
      "[epoch 20 / 100] average loss : 7.167317\n",
      "[epoch 40 / 100] average loss : 5.264566\n",
      "[epoch 60 / 100] average loss : 4.178959\n",
      "[epoch 80 / 100] average loss : 3.500621\n",
      "[epoch 100 / 100] average loss : 3.045402\n"
     ]
    }
   ],
   "source": [
    "config1 = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100,\n",
    "            'activation_function' : ReLU()\n",
    "          }\n",
    "\n",
    "config2 = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100,\n",
    "            'activation_function' : LeakyReLU()\n",
    "          }\n",
    "\n",
    "config3 = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100,\n",
    "            'activation_function' : Sigmoid()\n",
    "          }\n",
    "\n",
    "config4 = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100,\n",
    "            'activation_function' : Tanh()\n",
    "          }\n",
    "\n",
    "print('---ReLU---')\n",
    "model1 = train_MLP(config1)\n",
    "\n",
    "print('\\n---Leaky_Relu---')\n",
    "model2 = train_MLP(config2)\n",
    "\n",
    "print('\\n---Sigmoid---')\n",
    "model3 = train_MLP(config3)\n",
    "\n",
    "print('\\n---TanH---')\n",
    "model4 = train_MLP(config4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In test dataset ... \n",
      "\t Accuracy : 0.786\n",
      "\n",
      "In test dataset ... \n",
      "\t Accuracy : 0.7803\n",
      "\n",
      "In test dataset ... \n",
      "\t Accuracy : 0.3685\n",
      "\n",
      "In test dataset ... \n",
      "\t Accuracy : 0.544\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "\n",
    "print('\\t Accuracy :', eval(model1, train_version=False))\n",
    "print('\\t Accuracy :', eval(model2, train_version=False))\n",
    "print('\\t Accuracy :', eval(model3, train_version=False))\n",
    "print('\\t Accuracy :', eval(model4, train_version=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습 2] type of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_with_CrossEntropy :\n",
    "    def __init__(self) :\n",
    "        # softmax\n",
    "        self.softmax_x = None \n",
    "        self.softmax_out = None\n",
    "        # crossEntropy\n",
    "        self.pred = None\n",
    "        self.target = None\n",
    "        self.delta = 1e-7\n",
    "        \n",
    "    def softmax_forward(self, x) :\n",
    "        self.softmax_x = x\n",
    "        tmp = np.max(self.softmax_x, axis=1).reshape(-1, 1)\n",
    "        self.softmax_out = np.exp(self.softmax_x-tmp)/np.sum(np.exp(self.softmax_x-tmp), axis=1).reshape(-1,1)\n",
    "        return self.softmax_out\n",
    "    \n",
    "    def crossEntropy_forward(self, pred, target) :\n",
    "        self.pred = pred\n",
    "        self.target = target\n",
    "        loss = -np.sum(self.target*np.log(self.pred+self.delta), axis=1)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self) :\n",
    "        dout = (self.pred-self.target)/len(self.pred)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def train_MLP_v2(config,train_img = train_img, train_label = train_label) :\n",
    "    lr, num_epoch, batch_size = config['learning_rate'], config['num_epoch'], config['batch_size']\n",
    "    print_loss_interval = 20\n",
    "    \n",
    "    layer1 = Linear(784, 100)\n",
    "    activation_function = config['activation_function']\n",
    "    layer2 = Linear(100, 10)\n",
    "    softmax_with_CE = Softmax_with_CrossEntropy()\n",
    "    \n",
    "    for iter in range(num_epoch):\n",
    "        # forward\n",
    "        num_data = len(train_img)\n",
    "        num_batch = num_data // batch_size\n",
    "        if num_data % batch_size != 0:\n",
    "            num_batch += 1\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch_idx in range(num_batch):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(num_data, (batch_idx + 1) * batch_size)\n",
    "\n",
    "            batch_img = train_img[start_idx:end_idx]\n",
    "            batch_label = train_label[start_idx:end_idx]\n",
    "\n",
    "            x = layer1.forward(batch_img)\n",
    "            x = activation_function.forward(x)\n",
    "            x = layer2.forward(x)\n",
    "            preds = softmax_with_CE.softmax_forward(x)\n",
    "\n",
    "            # loss\n",
    "            one_hot_labels = make_one_hot(batch_label)\n",
    "            losses = softmax_with_CE.crossEntropy_forward(preds, one_hot_labels)\n",
    "            batch_loss = losses.sum() / len(preds)\n",
    "            epoch_loss += batch_loss\n",
    "\n",
    "            # backward\n",
    "            dL = softmax_with_CE.backward()\n",
    "            dL = layer2.backward(dL, lr)\n",
    "            dL = activation_function.backward(dL)\n",
    "            dL = layer1.backward(dL, lr)\n",
    "\n",
    "        avg_loss = epoch_loss / num_batch\n",
    "\n",
    "        if iter == 0 or (iter + 1) % print_loss_interval == 0:\n",
    "            print(\"[epoch %d / %d] average loss : %f\" % (iter + 1, num_epoch, avg_loss))\n",
    "\n",
    "            \n",
    "    model = OrderedDict()\n",
    "    model['layer1'] = layer1\n",
    "    model['activation_function'] = activation_function\n",
    "    model['layer2'] = layer2\n",
    "    model['softmax_with_CE'] = softmax_with_CE\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---batch---\n",
      "[epoch 1 / 20] average loss : 14.510660\n",
      "[epoch 20 / 20] average loss : 5.113783\n",
      "\n",
      "---mini_batch---\n",
      "[epoch 1 / 20] average loss : 7.847106\n",
      "[epoch 20 / 20] average loss : 1.199033\n",
      "\n",
      "---stochastic---\n",
      "[epoch 1 / 20] average loss : 2.029318\n",
      "[epoch 20 / 20] average loss : 1.690258\n"
     ]
    }
   ],
   "source": [
    "print('---batch---')\n",
    "config_batch = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 20,\n",
    "            'activation_function' : ReLU(),\n",
    "            'batch_size' : len(train_img) \n",
    "          }\n",
    "model_batch = train_MLP_v2(config_batch)\n",
    "\n",
    "\n",
    "print('\\n---mini_batch---')\n",
    "config_mini_batch = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 20,\n",
    "            'activation_function' : ReLU(),\n",
    "            'batch_size' : 2500\n",
    "          }\n",
    "model_mini_batch = train_MLP_v2(config_mini_batch)\n",
    "\n",
    "print('\\n---stochastic---')\n",
    "config_stochastic = { 'learning_rate' : 0.001,\n",
    "            'num_epoch' : 20,\n",
    "            'activation_function' : ReLU(),\n",
    "            'batch_size' : 1\n",
    "          }\n",
    "model_stochastic = train_MLP_v2(config_stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In test dataset ... \n",
      "\t Accuracy : 0.6273\n",
      "\n",
      "In test dataset ... \n",
      "\t Accuracy : 0.8636\n",
      "\n",
      "In test dataset ... \n",
      "\t Accuracy : 0.9186\n"
     ]
    }
   ],
   "source": [
    "print('\\t Accuracy :', eval(model_batch, train_version=False))\n",
    "print('\\t Accuracy :', eval(model_mini_batch, train_version=False))\n",
    "print('\\t Accuracy :', eval(model_stochastic, train_version=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra problem 1 (Multi Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Linear_v2:\n",
    "    def __init__(self, input_size=1, hidden_size=1):\n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.randn(input_size, hidden_size)\n",
    "        self.b = np.zeros(hidden_size)\n",
    "        \n",
    "        # Store input size and hidden size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Calculate linear transformation\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout, lr, lamb=0):\n",
    "        # Calculate gradients for weights and biases    \n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dW += lamb * self.W\n",
    "        # Update weights and biases\n",
    "        self.W -= lr * dW\n",
    "        self.b -= lr * db\n",
    "        \n",
    "        # Calculate gradients for input\n",
    "        # print(dout.shape, self.W.T.shape)\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "    \n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_with_MSVM:\n",
    "    def __init__(self):\n",
    "        # softmax\n",
    "        self.softmax_out = None\n",
    "        self.softmax_x = None\n",
    "        # MSVM\n",
    "        self.pred = None\n",
    "        self.target = None\n",
    "        self.reg = None\n",
    "\n",
    "    def softmax_forward(self, x):\n",
    "        self.softmax_x = x\n",
    "        # subtracting the maximum value for numerical stability\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        self.softmax_out = exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "        return self.softmax_out\n",
    "    \n",
    "    def sigmoid_forward(self, x):\n",
    "        self.sigmoid_x = x\n",
    "        self.sigmoid_out = 1 / (1 + np.exp(-x))\n",
    "        return self.sigmoid_out\n",
    "        \n",
    "\n",
    "    def forward(self, x, target, reg=0.1):\n",
    "        self.target = target\n",
    "        self.MSVM_out = self.softmax_forward(x)\n",
    "        scores = np.dot(x, self.MSVM_out.T)\n",
    "        correct_scores = scores[np.arange(x.shape[0]), target.flatten()]\n",
    "        margins = np.maximum(0, scores - correct_scores.reshape(-1, 1) + 1)\n",
    "        margins[np.arange(x.shape[0]), target.flatten()] = 0\n",
    "        loss = np.sum(margins)\n",
    "        loss /= x.shape[0]\n",
    "        loss += 0.5 * reg * np.sum(self.MSVM_out * self.MSVM_out)\n",
    "        self.pred = (scores >= np.max(scores, axis=1).reshape(-1, 1)).astype(float)\n",
    "        self.margin = margins\n",
    "        return loss\n",
    "\n",
    "    def backward(self, lr, reg):\n",
    "        self.reg = reg\n",
    "        batch_size = self.pred.shape[0]\n",
    "        dL_dsoftmax = self.MSVM_out - self.target\n",
    "\n",
    "        dsoftmax_dMSVMx = (self.softmax_out * (1 - self.softmax_out))\n",
    "        dL_dMSVMx = dL_dsoftmax * dsoftmax_dMSVMx\n",
    "\n",
    "        zero_mask = (dL_dsoftmax == 0) | (self.MSVM_out == 0)\n",
    "        dL_dMSVMx = np.where(zero_mask, 0, -dL_dMSVMx)\n",
    "        dMSVMx_dw = np.dot(dL_dMSVMx.T, self.pred) / batch_size\n",
    "\n",
    "        # L2 regularization\n",
    "        dMSVMx_dw += self.reg * self.MSVM_out\n",
    "\n",
    "        self.MSVM_out -= np.clip(lr * dMSVMx_dw.T, -1e8, 1e8)\n",
    "        return dL_dMSVMx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def clip_gradient(grad, clip_value):\n",
    "    return np.clip(grad, -clip_value, clip_value)\n",
    "\n",
    "def train_MSVM(config, train_img=train_img, train_label=train_label):\n",
    "    lr, num_epoch, batch_size, reg, grad_clip = config['learning_rate'], config['num_epoch'], config['batch_size'], config['reg'], config['grad_clip']\n",
    "    print_loss_interval = 20\n",
    "    \n",
    "    layer1 = Linear_v2(784, 100)\n",
    "    activation_function = config['activation_function']\n",
    "    layer2 = Linear_v2(100, 10)\n",
    "    softmax_with_MSVM = Softmax_with_MSVM()\n",
    "    \n",
    "    for iter in range(num_epoch):\n",
    "        # forward\n",
    "        num_data = len(train_img)\n",
    "        num_batch = num_data // batch_size\n",
    "        if num_data % batch_size != 0:\n",
    "            num_batch += 1\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch_idx in range(num_batch):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(num_data, (batch_idx + 1) * batch_size)\n",
    "\n",
    "            batch_img = train_img[start_idx:end_idx]\n",
    "            batch_label = train_label[start_idx:end_idx]\n",
    "\n",
    "            x = layer1.forward(batch_img)\n",
    "            x = activation_function.forward(x)\n",
    "            x = layer2.forward(x)\n",
    "            preds = softmax_with_MSVM.softmax_forward(x)\n",
    "            \n",
    "            loss = softmax_with_MSVM.forward(preds, batch_label, config['reg'])\n",
    "            batch_loss = loss.sum() / len(preds)\n",
    "            epoch_loss += batch_loss\n",
    "\n",
    "            # backward\n",
    "            dL = softmax_with_MSVM.backward(lr, reg)\n",
    "            dL = clip_gradient(dL, grad_clip)\n",
    "            dL = layer2.backward(dL, lr)\n",
    "            dL = clip_gradient(dL, grad_clip)\n",
    "            dL = activation_function.backward(dL)\n",
    "            dL = clip_gradient(dL, grad_clip)\n",
    "            dL = layer1.backward(dL, lr)\n",
    "            dL = clip_gradient(dL, grad_clip)\n",
    "\n",
    "        avg_loss = epoch_loss / num_batch\n",
    "        print(avg_loss)\n",
    "\n",
    "        if iter == 0 or (iter + 1) % print_loss_interval == 0:\n",
    "            print(\"[epoch %d / %d] average loss : %f\" % (iter + 1, num_epoch, avg_loss))\n",
    "\n",
    "    model = OrderedDict()\n",
    "    model['layer1'] = layer1\n",
    "    model['activation_function'] = activation_function\n",
    "    model['layer2'] = layer2\n",
    "    model['softmax_with_MSVM'] = softmax_with_MSVM\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9063082312495206\n",
      "[epoch 1 / 20] average loss : 0.906308\n",
      "0.9060401835436428\n",
      "0.9059561605020455\n",
      "0.9059525285416465\n",
      "0.9059759786286521\n",
      "0.9059538385853636\n",
      "0.9059719987326689\n",
      "0.9059806409576466\n",
      "0.9059761413968586\n",
      "0.9059633138599817\n",
      "0.9059777983005265\n",
      "0.9059621765083925\n",
      "0.9059645734726055\n",
      "0.9059692406937665\n",
      "0.9059636398261962\n",
      "0.9059675500302721\n",
      "0.9059754288751998\n",
      "0.9059699939498896\n",
      "0.9059675500302721\n",
      "0.9059729062079733\n",
      "[epoch 20 / 20] average loss : 0.905973\n"
     ]
    }
   ],
   "source": [
    "config = { 'learning_rate' : 0.0001,\n",
    "            'num_epoch' : 20,\n",
    "            'activation_function' : ReLU(),\n",
    "            'batch_size' : 10,\n",
    "            'reg' : 0.1,\n",
    "            'grad_clip' : 5\n",
    "          }\n",
    "\n",
    "\n",
    "model = train_MSVM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, train_version = True) :\n",
    "    if train_version :\n",
    "        x = train_img\n",
    "        labels = train_label.squeeze()\n",
    "        print('In train dataset ... ')\n",
    "    else : \n",
    "        x = test_img\n",
    "        labels = test_label.squeeze()\n",
    "        print('\\nIn test dataset ... ')\n",
    "    \n",
    "    for layer in model.values() :\n",
    "        if isinstance(layer, Softmax_with_MSVM) :\n",
    "            x = layer.softmax_forward(x)\n",
    "        else :\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "    preds = x.argmax(axis=1)\n",
    "    acc = np.sum(np.where(preds==labels, True, False))/len(labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In test dataset ... \n",
      "\t Accuracy : 0.1028\n"
     ]
    }
   ],
   "source": [
    "print('\\t Accuracy :', eval(model, train_version=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra problem 2 (3-layer 이상 MLP 구현)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def get_mask(self):\n",
    "        return self.mask.shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout, x):\n",
    "        if self.mask.shape != x.shape:\n",
    "            self.mask = np.zeros_like(x, dtype = np.bool)\n",
    "        dx = dout\n",
    "        dx[self.mask] = 0\n",
    "        return dx \n",
    "\n",
    "    \n",
    "class LeakyReLU:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] *= self.alpha\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout, x):\n",
    "        if self.mask is None or self.mask.shape != x.shape:\n",
    "            self.mask = np.zeros_like(x, dtype=np.bool)\n",
    "        dx = dout\n",
    "        dx[self.mask] *= self.alpha\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        return dx\n",
    "\n",
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = np.tanh(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out ** 2)\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_with_Mul_CrossEntropy:\n",
    "    def __init__(self):\n",
    "        self.softmax_x = None \n",
    "        self.softmax_out = None\n",
    "        self.pred = None\n",
    "        self.target = None\n",
    "\n",
    "    def softmax_forward(self, x):\n",
    "        self.softmax_x = x\n",
    "        tmp = np.max(self.softmax_x, axis=1).reshape(-1, 1)\n",
    "        self.softmax_out = np.exp(self.softmax_x - tmp) / np.sum(np.exp(self.softmax_x - tmp), axis=1).reshape(-1, 1)\n",
    "        return self.softmax_out\n",
    "    \n",
    "    def crossEntropy_forward(self, pred, target):\n",
    "        delta = 1e-7\n",
    "        self.pred = pred\n",
    "        self.target = target\n",
    "        loss = -np.sum(self.target * np.log(self.pred + delta), axis=1)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dL_next):\n",
    "        batch_size = len(self.pred)\n",
    "        dsoftmax = (self.pred - self.target) / batch_size\n",
    "        dL = dsoftmax * dL_next\n",
    "        return dL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def train_Multi_MLP(config):\n",
    "    lr, num_epoch, batch_size = config['learning_rate'], config['num_epoch'], config['batch_size']\n",
    "    print_loss_interval = 1\n",
    "    \n",
    "    layer1 = Linear(784, 256)\n",
    "    activation_function1 = config['activation_function']\n",
    "    layer2 = Linear(256, 128)\n",
    "    activation_function2 = config['activation_function']\n",
    "    layer3 = Linear(128, 64)\n",
    "    activation_function3 = config['activation_function']\n",
    "    layer4 = Linear(64, 32)\n",
    "    activation_function4 = config['activation_function']\n",
    "    layer5 = Linear(32, 10)\n",
    "    softmax_with_CE = Softmax_with_Mul_CrossEntropy()\n",
    "    \n",
    "    for iter in range(num_epoch):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for i in range(0, len(train_img), batch_size):\n",
    "            # forward\n",
    "            batch_img = train_img[i:i+batch_size]\n",
    "            batch_label = train_label[i:i+batch_size]\n",
    "            \n",
    "            x = layer1.forward(batch_img)\n",
    "            x_1 = activation_function1.forward(x)\n",
    "            x = layer2.forward(x_1)\n",
    "            x_2 = activation_function2.forward(x)\n",
    "            x = layer3.forward(x_2)\n",
    "            x_3 = activation_function3.forward(x)\n",
    "            x = layer4.forward(x_3)\n",
    "            x_4 = activation_function4.forward(x)\n",
    "            x = layer5.forward(x_4)\n",
    "            preds = softmax_with_CE.softmax_forward(x)\n",
    "\n",
    "            # loss\n",
    "            one_hot_labels = make_one_hot(batch_label)\n",
    "            losses = softmax_with_CE.crossEntropy_forward(preds, one_hot_labels)\n",
    "            batch_loss = losses.sum() / len(preds)\n",
    "            total_loss += batch_loss\n",
    "            num_batches += 1\n",
    "\n",
    "            # backward\n",
    "            dL = softmax_with_CE.backward(one_hot_labels)\n",
    "            dL = layer5.backward(dL)\n",
    "            dL = activation_function4.backward(dL,x_4)\n",
    "            dL = layer4.backward(dL)        \n",
    "            dL = activation_function3.backward(dL,x_3)\n",
    "            dL = layer3.backward(dL)\n",
    "            dL = activation_function2.backward(dL,x_2)\n",
    "            dL = layer2.backward(dL)\n",
    "            dL = activation_function1.backward(dL,x_1)\n",
    "            dL = layer1.backward(dL)\n",
    "\n",
    "        # average batch loss\n",
    "        avg_loss = total_loss / num_batches\n",
    "        \n",
    "        if iter == 0 or (iter+1) % print_loss_interval == 0:\n",
    "            print(\"[epoch %d / %d] average loss : %f\" % (iter+1, num_epoch, avg_loss))\n",
    "            \n",
    "    model = OrderedDict()\n",
    "    model['layer1'] = layer1\n",
    "    model['activation_function1'] = activation_function1\n",
    "    model['layer2'] = layer2\n",
    "    model['activation_function2'] = activation_function2\n",
    "    model['layer3'] = layer3\n",
    "    model['activation_function3'] = activation_function3\n",
    "    model['layer4'] = layer4\n",
    "    model['activation_function4'] = activation_function4\n",
    "    model['layer5'] = layer5\n",
    "    model['softmax_with_CE'] = softmax_with_CE\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ny/tbs_98cs36v4cc5h0tfjfszh0000gn/T/ipykernel_2828/4294253686.py:35: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.mask = np.zeros_like(x, dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1 / 3] average loss : nan\n",
      "[epoch 2 / 3] average loss : nan\n",
      "[epoch 3 / 3] average loss : nan\n"
     ]
    }
   ],
   "source": [
    "config = { 'learning_rate' : 0.00001,\n",
    "            'num_epoch' : 3,\n",
    "            'activation_function' : LeakyReLU(),\n",
    "            'batch_size' : 2500,\n",
    "            'reg' : 0.1\n",
    "          }\n",
    "\n",
    "model = train_Multi_MLP(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$L_i = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac989702ee82cf85834fc904c407af640e06b0902e988e9cd8a067d885b3b790"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
