{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# install\n",
    "## numpy\n",
    "## matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data load & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(train_raw_img, train_label), (test_raw_img, test_label) = load_mnist(flatten=False, normalize=False)\n",
    "print(train_raw_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 1)\n",
      "(10000, 784)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing (train & inference)\n",
    "\n",
    "train_img = train_raw_img.reshape(len(train_raw_img.squeeze()), -1)\n",
    "train_label = train_label.reshape(len(train_label), -1)\n",
    "\n",
    "test_img = test_raw_img.reshape(len(test_raw_img.squeeze()), -1)\n",
    "test_label = test_label.reshape(len(test_label), -1)\n",
    "\n",
    "print(train_img.shape)\n",
    "print(train_label.shape)\n",
    "print(test_img.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization (set value 0 ~ 1)\n",
    "\n",
    "train_img = train_img.astype('float')\n",
    "train_img = train_img/255\n",
    "\n",
    "test_img = test_img.astype('float')\n",
    "test_img = test_img/255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size=1, hidden_size=1):\n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.randn(input_size, hidden_size)\n",
    "        self.b = np.zeros(hidden_size)\n",
    "        \n",
    "        # Store input size and hidden size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Calculate linear transformation\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout, lr, lamb=0):\n",
    "        # Calculate gradients for weights and biases\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.W -= lr * (dW + lamb*self.W)\n",
    "        self.b -= lr * db\n",
    "        \n",
    "        # Calculate gradients for input\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        \n",
    "        return dx\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습 1] activation function 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx\n",
    "\n",
    "class LeakyReLU:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] *= self.alpha\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] *= self.alpha\n",
    "        dx = dout\n",
    "        return dx\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        return dx\n",
    "\n",
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = np.tanh(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out ** 2)\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_with_CrossEntropy :\n",
    "    def __init__(self) :\n",
    "        # softmax\n",
    "        self.softmax_x = None \n",
    "        self.softmax_out = None\n",
    "        # crossEntropy\n",
    "        self.pred = None\n",
    "        self.target = None\n",
    "        self.delta = 1e-7\n",
    "        \n",
    "    def softmax_forward(self, x) :\n",
    "        self.softmax_x = x\n",
    "        tmp = np.max(self.softmax_x, axis=1).reshape(-1, 1)\n",
    "        self.softmax_out = np.exp(self.softmax_x-tmp)/np.sum(np.exp(self.softmax_x-tmp), axis=1).reshape(-1,1)\n",
    "        return self.softmax_out\n",
    "    \n",
    "    def crossEntropy_forward(self, pred, target) :\n",
    "        self.pred = pred\n",
    "        self.target = target\n",
    "        loss = -np.sum(self.target*np.log(self.pred+self.delta), axis=1)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self) :\n",
    "        dout = (self.pred-self.target)/len(self.pred)\n",
    "        return dout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot label 만드는 함수\n",
    "\n",
    "def make_one_hot(labels) :\n",
    "    a = []\n",
    "    for label in labels :\n",
    "        one_hot = np.zeros(10)\n",
    "        one_hot[label] = 1\n",
    "        a.append(one_hot)\n",
    "    a = np.array(a)\n",
    "    return a\n",
    "\n",
    "# one_hot_labels = make_one_hot(train_label)\n",
    "# print(train_label[0])\n",
    "# print(one_hot_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, train_version = True) :\n",
    "    if train_version :\n",
    "        x = train_img\n",
    "        labels = train_label.squeeze()\n",
    "        print('In train dataset ... ')\n",
    "    else : \n",
    "        x = test_img\n",
    "        labels = test_label.squeeze()\n",
    "        print('\\nIn test dataset ... ')\n",
    "    \n",
    "    for layer in model.values() :\n",
    "        if isinstance(layer, Softmax_with_CrossEntropy) :\n",
    "            x = layer.softmax_forward(x)\n",
    "        else :\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "    preds = x.argmax(axis=1)\n",
    "    acc = np.sum(np.where(preds==labels, True, False))/len(labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def train_MLP(config) :\n",
    "    lr, num_epoch = config['learning_rate'], config['num_epoch']\n",
    "    print_loss_interval = 20\n",
    "    \n",
    "    layer1 = Linear(784, 100)\n",
    "    activation_function = config['activation_function']\n",
    "    layer2 = Linear(100, 10)\n",
    "    softmax_with_CE = Softmax_with_CrossEntropy()\n",
    "    \n",
    "    for iter in range(num_epoch) :\n",
    "        # forward\n",
    "        x = layer1.forward(train_img)\n",
    "        x = activation_function.forward(x)\n",
    "        x = layer2.forward(x)\n",
    "        preds = softmax_with_CE.softmax_forward(x)\n",
    "        \n",
    "        # loss\n",
    "        one_hot_labels = make_one_hot(train_label)\n",
    "        losses = softmax_with_CE.crossEntropy_forward(preds, one_hot_labels)\n",
    "        loss = losses.sum()/len(preds)\n",
    "        \n",
    "        # backward\n",
    "        dL = softmax_with_CE.backward()\n",
    "        dL = layer2.backward(dL, lr)\n",
    "        dL = activation_function.backward(dL)\n",
    "        dL = layer1.backward(dL, lr)\n",
    "        \n",
    "        if iter == 0 or (iter+1) % print_loss_interval == 0:\n",
    "            print(\"[epoch %d / %d] average loss : %f\" % (iter+1, num_epoch, loss))\n",
    "            \n",
    "    model = OrderedDict()\n",
    "    model['layer1'] = layer1\n",
    "    model['activation_function'] = activation_function\n",
    "    model['layer2'] = layer2\n",
    "    model['softmax_with_CE'] = softmax_with_CE\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ReLU---\n",
      "[epoch 1 / 100] average loss : 14.950970\n",
      "[epoch 20 / 100] average loss : 5.157625\n",
      "[epoch 40 / 100] average loss : 3.568637\n",
      "[epoch 60 / 100] average loss : 2.956997\n",
      "[epoch 80 / 100] average loss : 2.607218\n",
      "[epoch 100 / 100] average loss : 2.367798\n",
      "\n",
      "---Leaky_Relu---\n",
      "[epoch 1 / 100] average loss : 14.447551\n",
      "[epoch 20 / 100] average loss : 5.268470\n",
      "[epoch 40 / 100] average loss : 3.749723\n",
      "[epoch 60 / 100] average loss : 3.122307\n",
      "[epoch 80 / 100] average loss : 2.750167\n",
      "[epoch 100 / 100] average loss : 2.492386\n",
      "\n",
      "---Sigmoid---\n",
      "[epoch 1 / 100] average loss : 10.126408\n",
      "[epoch 20 / 100] average loss : 5.111660\n",
      "[epoch 40 / 100] average loss : 4.137771\n",
      "[epoch 60 / 100] average loss : 3.443539\n",
      "[epoch 80 / 100] average loss : 2.942252\n",
      "[epoch 100 / 100] average loss : 2.572567\n",
      "\n",
      "---TanH---\n",
      "[epoch 1 / 100] average loss : 11.754419\n",
      "[epoch 20 / 100] average loss : 8.225606\n",
      "[epoch 40 / 100] average loss : 5.967779\n",
      "[epoch 60 / 100] average loss : 4.640082\n",
      "[epoch 80 / 100] average loss : 3.828185\n",
      "[epoch 100 / 100] average loss : 3.294514\n"
     ]
    }
   ],
   "source": [
    "config1 = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100,\n",
    "            'activation_function' : ReLU()\n",
    "          }\n",
    "\n",
    "config2 = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100,\n",
    "            'activation_function' : LeakyReLU()\n",
    "          }\n",
    "\n",
    "config3 = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100,\n",
    "            'activation_function' : Sigmoid()\n",
    "          }\n",
    "\n",
    "config4 = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100,\n",
    "            'activation_function' : Tanh()\n",
    "          }\n",
    "\n",
    "print('---ReLU---')\n",
    "model1 = train_MLP(config1)\n",
    "\n",
    "print('\\n---Leaky_Relu---')\n",
    "model2 = train_MLP(config2)\n",
    "\n",
    "print('\\n---Sigmoid---')\n",
    "model3 = train_MLP(config3)\n",
    "\n",
    "print('\\n---TanH---')\n",
    "model4 = train_MLP(config4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "\n",
    "print('\\t Accuracy :', eval(model1, train_version=False))\n",
    "print('\\t Accuracy :', eval(model2, train_version=False))\n",
    "print('\\t Accuracy :', eval(model3, train_version=False))\n",
    "print('\\t Accuracy :', eval(model4, train_version=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습 2] type of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def train_MLP_v2(config) :\n",
    "    lr, num_epoch = config['learning_rate'], config['num_epoch']\n",
    "    print_loss_interval = 20\n",
    "\n",
    "    layer1 = Linear(784,100)\n",
    "    activation_function = config['activation_function']\n",
    "    layer2 = Linear(100,10)\n",
    "    Softmax_with_CrossEntropy = Softmax_with_CrossEntropy()\n",
    "    batch_size = config['batch_size']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---batch---')\n",
    "config_batch = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 20,\n",
    "            'activation_function' : Relu(),\n",
    "            'batch_size' : len(train_img)\n",
    "          }\n",
    "model_batch = train_MLP_v2(config_batch)\n",
    "\n",
    "\n",
    "print('\\n---mini_batch---')\n",
    "config_mini_batch = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 20,\n",
    "            'activation_function' : Relu(),\n",
    "            'batch_size' : # fill the mini_batch size\n",
    "          }\n",
    "model_mini_batch = train_MLP_v2(config_mini_batch)\n",
    "\n",
    "print('\\n---stochastic---')\n",
    "config_stochastic = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 20,\n",
    "            'activation_function' : Relu(),\n",
    "            'batch_size' : 1\n",
    "          }\n",
    "model_stochastic = train_MLP_v2(config_stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t Accuracy :', eval(model_batch, train_version=False))\n",
    "print('\\t Accuracy :', eval(model_mini_batch, train_version=False))\n",
    "print('\\t Accuracy :', eval(model_stochastic, train_version=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra problem 1 (Multi Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSVM :\n",
    "    def __init__(self) :\n",
    "        pass\n",
    "        \n",
    "    def forward(self, pred, target) :\n",
    "        pass\n",
    "    \n",
    "    def backward(self) :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def train_MSVM(config) :\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100,\n",
    "            'activation_function' : ReLU()\n",
    "          }\n",
    "\n",
    "model = train_MSVM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t Accuracy :', eval(model, train_version=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra problem 2 (3-layer 이상 MLP 구현)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac989702ee82cf85834fc904c407af640e06b0902e988e9cd8a067d885b3b790"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
