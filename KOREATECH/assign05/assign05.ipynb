{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# install\n",
    "## numpy\n",
    "## matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data load & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(train_raw_img, train_label), (test_raw_img, test_label) = load_mnist(flatten=False, normalize=False)\n",
    "print(train_raw_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 1)\n",
      "(10000, 784)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing (train & inference)\n",
    "\n",
    "train_img = train_raw_img.reshape(len(train_raw_img.squeeze()), -1)\n",
    "train_label = train_label.reshape(len(train_label), -1)\n",
    "\n",
    "test_img = test_raw_img.reshape(len(test_raw_img.squeeze()), -1)\n",
    "test_label = test_label.reshape(len(test_label), -1)\n",
    "\n",
    "print(train_img.shape)\n",
    "print(train_label.shape)\n",
    "print(test_img.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization (set value 0 ~ 1)\n",
    "\n",
    "train_img = train_img.astype('float')\n",
    "train_img = train_img/255\n",
    "\n",
    "test_img = test_img.astype('float')\n",
    "test_img = test_img/255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size=1, hidden_size=1):\n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.randn(input_size, hidden_size)\n",
    "        self.b = np.zeros(hidden_size)\n",
    "        \n",
    "        # Store input size and hidden size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Calculate linear transformation\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout, lr, lamb=0):\n",
    "        # Calculate gradients for weights and biases\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.W -= lr * (dW + lamb*self.W)\n",
    "        self.b -= lr * db\n",
    "        \n",
    "        # Calculate gradients for input\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        \n",
    "        return dx\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습 1] activation function 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx\n",
    "\n",
    "class LeakyReLU:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] *= self.alpha\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] *= self.alpha\n",
    "        dx = dout\n",
    "        return dx\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        return dx\n",
    "\n",
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = np.tanh(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out ** 2)\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_with_CrossEntropy :\n",
    "    def __init__(self) :\n",
    "        # softmax\n",
    "        self.softmax_x = None \n",
    "        self.softmax_out = None\n",
    "        # crossEntropy\n",
    "        self.pred = None\n",
    "        self.target = None\n",
    "        self.delta = 1e-7\n",
    "        \n",
    "    def softmax_forward(self, x) :\n",
    "        self.softmax_x = x\n",
    "        tmp = np.max(self.softmax_x, axis=1).reshape(-1, 1)\n",
    "        self.softmax_out = np.exp(self.softmax_x-tmp)/np.sum(np.exp(self.softmax_x-tmp), axis=1).reshape(-1,1)\n",
    "        return self.softmax_out\n",
    "    \n",
    "    def crossEntropy_forward(self, pred, target) :\n",
    "        self.pred = pred\n",
    "        self.target = target\n",
    "        loss = -np.sum(self.target*np.log(self.pred+self.delta), axis=1)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self) :\n",
    "        dout = (self.pred-self.target)/len(self.pred)\n",
    "        return dout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot label 만드는 함수\n",
    "\n",
    "def make_one_hot(labels) :\n",
    "    a = []\n",
    "    for label in labels :\n",
    "        one_hot = np.zeros(10)\n",
    "        one_hot[label] = 1\n",
    "        a.append(one_hot)\n",
    "    a = np.array(a)\n",
    "    return a\n",
    "\n",
    "# one_hot_labels = make_one_hot(train_label)\n",
    "# print(train_label[0])\n",
    "# print(one_hot_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, train_version = True) :\n",
    "    if train_version :\n",
    "        x = train_img\n",
    "        labels = train_label.squeeze()\n",
    "        print('In train dataset ... ')\n",
    "    else : \n",
    "        x = test_img\n",
    "        labels = test_label.squeeze()\n",
    "        print('\\nIn test dataset ... ')\n",
    "    \n",
    "    for layer in model.values() :\n",
    "        if isinstance(layer, Softmax_with_CrossEntropy) :\n",
    "            x = layer.softmax_forward(x)\n",
    "        else :\n",
    "            print(layer)\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "    preds = x.argmax(axis=1)\n",
    "    acc = np.sum(np.where(preds==labels, True, False))/len(labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def train_MLP(config) :\n",
    "    lr, num_epoch = config['learning_rate'], config['num_epoch']\n",
    "    print_loss_interval = 20\n",
    "    \n",
    "    layer1 = Linear(784, 100)\n",
    "    activation_function = config['activation_function']\n",
    "    layer2 = Linear(100, 10)\n",
    "    softmax_with_CE = Softmax_with_CrossEntropy()\n",
    "    \n",
    "    for iter in range(num_epoch) :\n",
    "        # forward\n",
    "        x = layer1.forward(train_img)\n",
    "        x = activation_function.forward(x)\n",
    "        x = layer2.forward(x)\n",
    "        preds = softmax_with_CE.softmax_forward(x)\n",
    "        \n",
    "        # loss\n",
    "        one_hot_labels = make_one_hot(train_label)\n",
    "        losses = softmax_with_CE.crossEntropy_forward(preds, one_hot_labels)\n",
    "        loss = losses.sum()/len(preds)\n",
    "        \n",
    "        # backward\n",
    "        dL = softmax_with_CE.backward()\n",
    "        dL = layer2.backward(dL, lr)\n",
    "        dL = activation_function.backward(dL)\n",
    "        dL = layer1.backward(dL, lr)\n",
    "        \n",
    "        if iter == 0 or (iter+1) % print_loss_interval == 0:\n",
    "            print(\"[epoch %d / %d] average loss : %f\" % (iter+1, num_epoch, loss))\n",
    "            \n",
    "    model = OrderedDict()\n",
    "    model['layer1'] = layer1\n",
    "    model['activation_function'] = activation_function\n",
    "    model['layer2'] = layer2\n",
    "    model['softmax_with_CE'] = softmax_with_CE\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ReLU---\n",
      "(60000, 100)\n",
      "[epoch 1 / 100] average loss : 13.804500\n",
      "(60000, 100)\n",
      "(60000, 100)\n",
      "(60000, 100)\n",
      "(60000, 100)\n",
      "(60000, 100)\n",
      "(60000, 100)\n",
      "(60000, 100)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 22\u001b[0m\n\u001b[1;32m     16\u001b[0m config4 \u001b[39m=\u001b[39m { \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m : \u001b[39m0.1\u001b[39m,\n\u001b[1;32m     17\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mnum_epoch\u001b[39m\u001b[39m'\u001b[39m : \u001b[39m100\u001b[39m,\n\u001b[1;32m     18\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mactivation_function\u001b[39m\u001b[39m'\u001b[39m : Tanh()\n\u001b[1;32m     19\u001b[0m           }\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m---ReLU---\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m model1 \u001b[39m=\u001b[39m train_MLP(config1)\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m---Leaky_Relu---\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m model2 \u001b[39m=\u001b[39m train_MLP(config2)\n",
      "Cell \u001b[0;32mIn[61], line 14\u001b[0m, in \u001b[0;36mtrain_MLP\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     10\u001b[0m softmax_with_CE \u001b[39m=\u001b[39m Softmax_with_CrossEntropy()\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epoch) :\n\u001b[1;32m     13\u001b[0m     \u001b[39m# forward\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     x \u001b[39m=\u001b[39m layer1\u001b[39m.\u001b[39;49mforward(train_img)\n\u001b[1;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     16\u001b[0m     x \u001b[39m=\u001b[39m activation_function\u001b[39m.\u001b[39mforward(x)\n",
      "Cell \u001b[0;32mIn[52], line 14\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     12\u001b[0m     \u001b[39m# Calculate linear transformation\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m x\n\u001b[0;32m---> 14\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb\n\u001b[1;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config1 = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100,\n",
    "            'activation_function' : ReLU()\n",
    "          }\n",
    "\n",
    "config2 = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100,\n",
    "            'activation_function' : LeakyReLU()\n",
    "          }\n",
    "\n",
    "config3 = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100,\n",
    "            'activation_function' : Sigmoid()\n",
    "          }\n",
    "\n",
    "config4 = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 100,\n",
    "            'activation_function' : Tanh()\n",
    "          }\n",
    "\n",
    "print('---ReLU---')\n",
    "model1 = train_MLP(config1)\n",
    "\n",
    "print('\\n---Leaky_Relu---')\n",
    "model2 = train_MLP(config2)\n",
    "\n",
    "print('\\n---Sigmoid---')\n",
    "model3 = train_MLP(config3)\n",
    "\n",
    "print('\\n---TanH---')\n",
    "model4 = train_MLP(config4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# evaluation\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m Accuracy :\u001b[39m\u001b[39m'\u001b[39m, \u001b[39meval\u001b[39m(model1, train_version\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m Accuracy :\u001b[39m\u001b[39m'\u001b[39m, \u001b[39meval\u001b[39m(model2, train_version\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m Accuracy :\u001b[39m\u001b[39m'\u001b[39m, \u001b[39meval\u001b[39m(model3, train_version\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "\n",
    "print('\\t Accuracy :', eval(model1, train_version=False))\n",
    "print('\\t Accuracy :', eval(model2, train_version=False))\n",
    "print('\\t Accuracy :', eval(model3, train_version=False))\n",
    "print('\\t Accuracy :', eval(model4, train_version=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습 2] type of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_with_CrossEntropy :\n",
    "    def __init__(self) :\n",
    "        # softmax\n",
    "        self.softmax_x = None \n",
    "        self.softmax_out = None\n",
    "        # crossEntropy\n",
    "        self.pred = None\n",
    "        self.target = None\n",
    "        self.delta = 1e-7\n",
    "        \n",
    "    def softmax_forward(self, x) :\n",
    "        self.softmax_x = x\n",
    "        tmp = np.max(self.softmax_x, axis=1).reshape(-1, 1)\n",
    "        self.softmax_out = np.exp(self.softmax_x-tmp)/np.sum(np.exp(self.softmax_x-tmp), axis=1).reshape(-1,1)\n",
    "        return self.softmax_out\n",
    "    \n",
    "    def crossEntropy_forward(self, pred, target) :\n",
    "        self.pred = pred\n",
    "        self.target = target\n",
    "        loss = -np.sum(self.target*np.log(self.pred+self.delta), axis=1)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self) :\n",
    "        dout = (self.pred-self.target)/len(self.pred)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def train_MLP_v2(config,train_img = train_img, train_label = train_label) :\n",
    "    lr, num_epoch, batch_size = config['learning_rate'], config['num_epoch'], config['batch_size']\n",
    "    print_loss_interval = 20\n",
    "    \n",
    "    layer1 = Linear(784, 100)\n",
    "    activation_function = config['activation_function']\n",
    "    layer2 = Linear(100, 10)\n",
    "    softmax_with_CE = Softmax_with_CrossEntropy()\n",
    "    \n",
    "    for iter in range(num_epoch):\n",
    "        # forward\n",
    "        num_data = len(train_img)\n",
    "        num_batch = num_data // batch_size\n",
    "        if num_data % batch_size != 0:\n",
    "            num_batch += 1\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch_idx in range(num_batch):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(num_data, (batch_idx + 1) * batch_size)\n",
    "\n",
    "            batch_img = train_img[start_idx:end_idx]\n",
    "            batch_label = train_label[start_idx:end_idx]\n",
    "\n",
    "            x = layer1.forward(batch_img)\n",
    "            x = activation_function.forward(x)\n",
    "            x = layer2.forward(x)\n",
    "            preds = softmax_with_CE.softmax_forward(x)\n",
    "\n",
    "            # loss\n",
    "            one_hot_labels = make_one_hot(batch_label)\n",
    "            losses = softmax_with_CE.crossEntropy_forward(preds, one_hot_labels)\n",
    "            batch_loss = losses.sum() / len(preds)\n",
    "            epoch_loss += batch_loss\n",
    "\n",
    "            # backward\n",
    "            dL = softmax_with_CE.backward()\n",
    "            dL = layer2.backward(dL, lr)\n",
    "            dL = activation_function.backward(dL)\n",
    "            dL = layer1.backward(dL, lr)\n",
    "\n",
    "        avg_loss = epoch_loss / num_batch\n",
    "\n",
    "        if iter == 0 or (iter + 1) % print_loss_interval == 0:\n",
    "            print(\"[epoch %d / %d] average loss : %f\" % (iter + 1, num_epoch, avg_loss))\n",
    "\n",
    "            \n",
    "    model = OrderedDict()\n",
    "    model['layer1'] = layer1\n",
    "    model['activation_function'] = activation_function\n",
    "    model['layer2'] = layer2\n",
    "    model['softmax_with_CE'] = softmax_with_CE\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---batch---\n",
      "[epoch 1 / 20] average loss : 14.854804\n",
      "[epoch 20 / 20] average loss : 5.714694\n",
      "\n",
      "---mini_batch---\n",
      "[epoch 1 / 20] average loss : 7.555124\n",
      "[epoch 20 / 20] average loss : 1.196243\n",
      "\n",
      "---stochastic---\n",
      "[epoch 1 / 20] average loss : 2.105571\n",
      "[epoch 20 / 20] average loss : 1.724185\n"
     ]
    }
   ],
   "source": [
    "print('---batch---')\n",
    "config_batch = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 20,\n",
    "            'activation_function' : ReLU(),\n",
    "            'batch_size' : len(train_img) \n",
    "          }\n",
    "model_batch = train_MLP_v2(config_batch)\n",
    "\n",
    "\n",
    "print('\\n---mini_batch---')\n",
    "config_mini_batch = { 'learning_rate' : 0.1,\n",
    "            'num_epoch' : 20,\n",
    "            'activation_function' : ReLU(),\n",
    "            'batch_size' : 2500\n",
    "          }\n",
    "model_mini_batch = train_MLP_v2(config_mini_batch)\n",
    "\n",
    "print('\\n---stochastic---')\n",
    "config_stochastic = { 'learning_rate' : 0.001,\n",
    "            'num_epoch' : 20,\n",
    "            'activation_function' : ReLU(),\n",
    "            'batch_size' : 1\n",
    "          }\n",
    "model_stochastic = train_MLP_v2(config_stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In test dataset ... \n",
      "\t Accuracy : 0.5875\n",
      "\n",
      "In test dataset ... \n",
      "\t Accuracy : 0.866\n",
      "\n",
      "In test dataset ... \n",
      "\t Accuracy : 0.9401\n"
     ]
    }
   ],
   "source": [
    "print('\\t Accuracy :', eval(model_batch, train_version=False))\n",
    "print('\\t Accuracy :', eval(model_mini_batch, train_version=False))\n",
    "print('\\t Accuracy :', eval(model_stochastic, train_version=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra problem 1 (Multi Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, train_version = True) :\n",
    "    if train_version :\n",
    "        x = train_img\n",
    "        labels = train_label.squeeze()\n",
    "        print('In train dataset ... ')\n",
    "    else : \n",
    "        x = test_img\n",
    "        labels = test_label.squeeze()\n",
    "        print('\\nIn test dataset ... ')\n",
    "    \n",
    "    for layer in model.values() :\n",
    "        if isinstance(layer, Softmax_with_CrossEntropy) :\n",
    "            x = layer.softmax_forward(x)\n",
    "        else :\n",
    "            x = layer.forward(x, labels)\n",
    "            \n",
    "    preds = x.argmax(axis=1)\n",
    "    acc = np.sum(np.where(preds==labels, True, False))/len(labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_v2:\n",
    "    def __init__(self, input_size=1, hidden_size=1):\n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.randn(input_size, hidden_size)\n",
    "        self.b = np.zeros(hidden_size)\n",
    "        \n",
    "        # Store input size and hidden size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Calculate linear transformation\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout, lr, lamb=0):\n",
    "        # Calculate gradients for weights and biases\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.W -= lr * (dW + lamb*self.W)\n",
    "        self.b -= lr * db\n",
    "        \n",
    "        # Calculate gradients for input\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        \n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_with_MSVM :\n",
    "    def __init__(self) :\n",
    "        # softmax\n",
    "        self.MSVM_x = None \n",
    "        self.MSVM_out = None\n",
    "        # crossEntropy\n",
    "        self.pred = None\n",
    "        self.target = None\n",
    "        \n",
    "    def softmax_forward(self, x) :\n",
    "        self.MSVM_x = x\n",
    "        tmp = np.max(self.MSVM_x, axis=1).reshape(-1, 1)\n",
    "        self.MSVM_out = np.exp(self.MSVM_x-tmp)/np.sum(np.exp(self.MSVM_x-tmp), axis=1).reshape(-1,1)\n",
    "        return self.MSVM_out\n",
    "    \n",
    "    def forward(self, x, target, reg = 1) :\n",
    "        self.target = target\n",
    "        scores = np.dot(x, self.MSVM_out.T)\n",
    "        correct_scores = scores[np.arange(x.shape[0]), target.flatten()]\n",
    "        margins = np.maximum(0, scores - correct_scores.reshape(-1, 1) + 1)\n",
    "        margins[np.arange(x.shape[0]), target.flatten()] = 0\n",
    "        loss = np.sum(margins)\n",
    "        loss /= x.shape[0]\n",
    "        loss += 0.5 * reg * np.sum(self.MSVM_out * self.MSVM_out)\n",
    "        self.pred = (scores >= np.max(scores, axis=1).reshape(-1,1)).astype(float)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        batch_size = self.pred.shape[0]\n",
    "        \n",
    "        # derivative of cross-entropy loss w.r.t. softmax output\n",
    "        dL_dsoftmax = self.MSVM_out - self.target\n",
    "        \n",
    "        # gradient of softmax layer\n",
    "        dsoftmax_dMSVMx = self.MSVM_out * (1 - self.MSVM_out)\n",
    "        \n",
    "        # gradient of MSVM\n",
    "        dMSVMx_dw = np.where(dL_dsoftmax == 0, 0, np.where(self.MSVM_out == 0, 0, -dL_dsoftmax * dsoftmax_dMSVMx))\n",
    "        dMSVMx_dw = dMSVMx_dw.T\n",
    "        \n",
    "        return dMSVMx_dw / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def train_MSVM(config, train_img=train_img, train_label=train_label):\n",
    "    lr, num_epoch, batch_size = config['learning_rate'], config['num_epoch'], config['batch_size']\n",
    "    print_loss_interval = 20\n",
    "    \n",
    "    layer1 = Linear_v2(784, 100)\n",
    "    activation_function = config['activation_function']\n",
    "    layer2 = Linear_v2(100, 10)\n",
    "    softmax_with_MSVM = Softmax_with_MSVM()\n",
    "    \n",
    "    for iter in range(num_epoch):\n",
    "        # forward\n",
    "        num_data = len(train_img)\n",
    "        num_batch = num_data // batch_size\n",
    "        if num_data % batch_size != 0:\n",
    "            num_batch += 1\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch_idx in range(num_batch):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(num_data, (batch_idx + 1) * batch_size)\n",
    "\n",
    "            batch_img = train_img[start_idx:end_idx]\n",
    "            batch_label = train_label[start_idx:end_idx]\n",
    "\n",
    "            x = layer1.forward(batch_img)\n",
    "            x = activation_function.forward(x)\n",
    "            x = layer2.forward(x)\n",
    "            preds = softmax_with_MSVM.softmax_forward(x)\n",
    "            \n",
    "            loss = softmax_with_MSVM.forward(preds, batch_label, config['reg'])\n",
    "            batch_loss = loss.sum() / len(preds)\n",
    "            epoch_loss += batch_loss\n",
    "\n",
    "            # backward\n",
    "            dL = softmax_with_MSVM.backward()\n",
    "            dL = layer2.backward(dL.T, lr)\n",
    "            dL = activation_function.backward(dL)\n",
    "            dL = layer1.backward(dL, lr)\n",
    "\n",
    "        avg_loss = epoch_loss / num_batch\n",
    "\n",
    "        if iter == 0 or (iter + 1) % print_loss_interval == 0:\n",
    "            print(\"[epoch %d / %d] average loss : %f\" % (iter + 1, num_epoch, avg_loss))\n",
    "\n",
    "    model = OrderedDict()\n",
    "    model['layer1'] = layer1\n",
    "    model['activation_function'] = activation_function\n",
    "    model['layer2'] = layer2\n",
    "    model['softmax_with_MSVM'] = softmax_with_MSVM\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1 / 100] average loss : 1.038906\n",
      "[epoch 20 / 100] average loss : 1.043133\n",
      "[epoch 40 / 100] average loss : 1.042151\n",
      "[epoch 60 / 100] average loss : 1.042150\n",
      "[epoch 80 / 100] average loss : 1.042149\n",
      "[epoch 100 / 100] average loss : 1.042149\n"
     ]
    }
   ],
   "source": [
    "config = { 'learning_rate' : 0.001,\n",
    "            'num_epoch' : 100,\n",
    "            'activation_function' : ReLU(),\n",
    "            'batch_size' : 128,\n",
    "            'reg' : 0.1\n",
    "          }\n",
    "\n",
    "model = train_MSVM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In test dataset ... \n",
      "<__main__.Linear_v2 object at 0x10f86cf40>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m Accuracy :\u001b[39m\u001b[39m'\u001b[39m, \u001b[39meval\u001b[39;49m(model, train_version\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m))\n",
      "Cell \u001b[0;32mIn[161], line 16\u001b[0m, in \u001b[0;36meval\u001b[0;34m(model, train_version)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[39melse\u001b[39;00m :\n\u001b[1;32m     15\u001b[0m         \u001b[39mprint\u001b[39m(layer)\n\u001b[0;32m---> 16\u001b[0m         x \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mforward(x, labels)\n\u001b[1;32m     18\u001b[0m preds \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39margmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m acc \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mwhere(preds\u001b[39m==\u001b[39mlabels, \u001b[39mTrue\u001b[39;00m, \u001b[39mFalse\u001b[39;00m))\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(labels)\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "print('\\t Accuracy :', eval(model, train_version=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra problem 2 (3-layer 이상 MLP 구현)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac989702ee82cf85834fc904c407af640e06b0902e988e9cd8a067d885b3b790"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
